{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Stella_Sun.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "cK0Z6BEm4Ygp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GFHdAEFQtTRG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install torch"
      ]
    },
    {
      "metadata": {
        "id": "YJl1CJvlt5gG",
        "colab_type": "code",
        "outputId": "dc02ebc7-3661-4e53-f076-c57d05627350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x595ee000 @  0x7f799c5c52a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xAQHOkBolXXY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Pacgages"
      ]
    },
    {
      "metadata": {
        "id": "gOAe8qRguGdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pickle as pkl\n",
        "import random\n",
        "import pdb\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "#specify SOS() and EOS(end of sentence)\n",
        "#specify maximum vocabulary size = 50000\n",
        "PAD_IDX = 2\n",
        "UNK_IDX = 3\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_VOCAB_SIZE = 800000\n",
        "MAX_LENGTH = 50\n",
        "\n",
        "train_en = 'data/train.tok.en'\n",
        "train_zh = 'data/train.tok.zh'\n",
        "val_en = 'data/dev.tok.en'\n",
        "val_zh = 'data/dev.tok.zh'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PsEuCzYYuMoR",
        "colab_type": "code",
        "outputId": "4f805e52-b6e6-4a67-87ac-2cb13e3fb144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#user GPU if possible\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "  print(\"Currently using GPU\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FntyL6ovSDY",
        "colab_type": "code",
        "outputId": "77ec8948-cdbb-4913-9515-1d23252c94f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5vc0D8zWxu_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Embedding"
      ]
    },
    {
      "metadata": {
        "id": "6FZwLxswviX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "folder_path = os.getcwd() + '/gdrive/My Drive/NLP_Project/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrucOX2bsLUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"<SOS>\":0, \"<EOS>\":1, \"<PAD>\":2, \"<UNK>\":3}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
        "        self.n_words = 4  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(address_lang1, address_lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines_lang1 = open(folder_path+address_lang1, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    lines_lang2 = open(folder_path+address_lang2, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    \n",
        "    assert (len(lines_lang1)==len(lines_lang2))\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[lines_lang1[i], normalizeString(lines_lang2[i])] for i in range (len(lines_lang1))]\n",
        "    #print (pairs[-1])\n",
        "    # Reverse pairs, make Lang instances\n",
        "    lang1=address_lang1[-2:]\n",
        "    lang2=address_lang2[-2:]\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def prepareData(address_lang1, address_lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(address_lang1, address_lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGFkxsahuPPT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_emb_matrix(language):\n",
        "    #load fasttext word vectors\n",
        "    words_to_load = MAX_VOCAB_SIZE\n",
        "    if language == 'english':\n",
        "      file = 'wiki-news-300d-1M-subword.vec'\n",
        "    if language == 'chinese':\n",
        "      file = 'cc.zh.300.vec'\n",
        "    \n",
        "\n",
        "    with open(folder_path + 'data/' + file) as f:\n",
        "        #remove the first line\n",
        "        firstLine = f.readline()\n",
        "        loaded_embeddings = np.zeros((words_to_load + 4, 300))\n",
        "        words2id = {}\n",
        "        idx2words = {}\n",
        "        #ordered_words = []\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= words_to_load: \n",
        "                break\n",
        "            s = line.split()\n",
        "            loaded_embeddings[i + 4 , :] = np.asarray(s[1:])\n",
        "            words2id['<SOS>'] = SOS_token\n",
        "            words2id['<EOS>'] = EOS_token\n",
        "            words2id['<pad>'] = PAD_IDX\n",
        "            words2id['<unk>'] = UNK_IDX\n",
        "            words2id[s[0]] = i + 4\n",
        "            \n",
        "            idx2words[0] = '<SOS>'\n",
        "            idx2words[1] = '<EOD>'\n",
        "            idx2words[2] = '<pad>'\n",
        "            idx2words[3] = '<unk>'\n",
        "            \n",
        "            idx2words[i + 4] = s[0]\n",
        "   \n",
        "\n",
        "    return words2id,idx2words,loaded_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxBmZe6Yucnt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_weights_matrix(index2word_lang, word2index_lang, index2word_embed, word2index_embed, loaded_embeddings):\n",
        "    emb_dim=300\n",
        "    missing_count=0\n",
        "    matrix_len = len(index2word_lang)\n",
        "    weights_matrix = np.zeros((matrix_len, 300))\n",
        "    \n",
        "    for key in index2word_lang.keys():\n",
        "        word=index2word_lang[key]\n",
        "        if (word in word2index_embed.keys()):\n",
        "          weights_matrix[key] = loaded_embeddings[word2index_embed[word]]\n",
        "        else:\n",
        "          missing_count=missing_count+1\n",
        "          weights_matrix[key] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
        "    print (missing_count)\n",
        "    return weights_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXOL7z5YD8hg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_en = 'data/train.tok.en'\n",
        "train_zh = 'data/train.tok.zh'\n",
        "val_en = 'data/dev.tok.en'\n",
        "val_zh = 'data/dev.tok.zh'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GBRaSTyItmS5",
        "colab_type": "code",
        "outputId": "9be33e7a-1354-4771-9cfb-e62fb72d0e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# train_input_lang, train_output_lang, train_pairs = prepareData(train_zh, train_en, reverse=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 213376 sentence pairs\n",
            "Trimmed to 193446 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "zh 79368\n",
            "en 45471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hfvAHu52C5v2",
        "colab_type": "code",
        "outputId": "5243584c-aa1f-474f-ce78-61acad438e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "val_input_lang, val_output_lang, val_pairs = prepareData(val_zh, val_en, reverse=False)\n",
        "print(\"print a random pair of validation pairs:\")\n",
        "print(random.choice(val_pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 1261 sentence pairs\n",
            "Trimmed to 1083 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "zh 4615\n",
            "en 2840\n",
            "print a random pair of validation pairs:\n",
            "['  现在   在 这里 有 很多 事 会 发生    所以 让 我 一件 一件 的 展示   ', 'now there apos s a lot going on in this movie so let me break this down and show you what apos s going on .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FdZ2rxHMueaH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# words2id_eng,idx2words_eng,loaded_embeddings_eng = load_emb_matrix('english')\n",
        "# words2id_zh,idx2words_zh,loaded_embeddings_zh = load_emb_matrix('chinese')\n",
        "\n",
        "# pkl.dump(words2id_eng, open(folder_path + 'data/words2id_eng_1M.pkl', 'wb'))\n",
        "# pkl.dump(idx2words_eng, open(folder_path +'data/idx2words_eng_1M.pkl', 'wb'))\n",
        "# pkl.dump(loaded_embeddings_eng, open(folder_path +'data/embedding_matrix_eng_1M.pkl', 'wb'))\n",
        "\n",
        "# pkl.dump(words2id_zh, open(folder_path + 'data/words2id_zh_1M.pkl', 'wb'))\n",
        "# pkl.dump(idx2words_zh, open(folder_path + 'data/idx2words_zh_1M.pkl', 'wb'))\n",
        "# pkl.dump(loaded_embeddings_zh, open(folder_path +'data/embedding_matrix_zh_1M.pkl', 'wb'))\n",
        "\n",
        "words2id_eng=pkl.load(open(folder_path + 'data/words2id_eng_1M.pkl', 'rb'))\n",
        "idx2words_eng=pkl.load(open(folder_path +'data/idx2words_eng_1M.pkl', 'rb'))\n",
        "loaded_embeddings_eng=pkl.load(open(folder_path +'data/embedding_matrix_eng_1M.pkl', 'rb'))\n",
        "\n",
        "words2id_zh=pkl.load(open(folder_path + 'data/words2id_zh_1M.pkl', 'rb'))\n",
        "idx2words_zh=pkl.load(open(folder_path + 'data/idx2words_zh_1M.pkl', 'rb'))\n",
        "loaded_embeddings_zh=pkl.load(open(folder_path +'data/embedding_matrix_zh_1M.pkl', 'rb'))\n",
        "\n",
        "# #load embeding matrix\n",
        "# words2id_eng = pkl.load(open(folder_path + 'data/words2id_eng.pkl', 'rb'))\n",
        "# idx2words_eng = pkl.load(open(folder_path +'data/idx2words_eng.pkl', 'rb'))\n",
        "# loaded_embeddings_eng= pkl.load(open(folder_path +'data/embedding_matrix_eng.pkl', 'rb'))\n",
        "\n",
        "\n",
        "# words2id_zh = pkl.load(open(folder_path + 'data/words2id_zh.pkl', 'rb'))\n",
        "# idx2words_zh = pkl.load(open(folder_path +'data/idx2words_zh.pkl', 'rb'))\n",
        "# loaded_embeddings_zh= pkl.load(open(folder_path +'data/embedding_matrix_zh.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFouTPE8xLzg",
        "colab_type": "code",
        "outputId": "04811704-bef6-4143-d859-279f4ac23baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# weights_matrix_eng=generate_weights_matrix(train_input_lang.index2word, train_output_lang.word2index, idx2words_eng, words2id_eng, loaded_embeddings_eng)\n",
        "# weights_matrix_eng = torch.from_numpy(weights_matrix_eng).to(device)\n",
        "\n",
        "\n",
        "# weights_matrix_zh=generate_weights_matrix(train_input_lang.index2word, train_output_lang.word2index, idx2words_zh, words2id_zh, loaded_embeddings_zh) \n",
        "# weights_matrix_zh = torch.from_numpy(weights_matrix_zh).to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72824\n",
            "19084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "naJ7pB5eGa6l",
        "colab_type": "code",
        "outputId": "467d695d-0faf-4207-856a-805b349f21e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "weights_matrix_eng_val = generate_weights_matrix(val_input_lang.index2word, val_input_lang.word2index, idx2words_eng, words2id_eng, loaded_embeddings_eng)\n",
        "weights_matrix_eng_val= torch.from_numpy(weights_matrix_eng_val).to(device)\n",
        "\n",
        "\n",
        "weights_matrix_zh_val = generate_weights_matrix(val_input_lang.index2word, val_output_lang.word2index, idx2words_zh, words2id_zh, loaded_embeddings_zh) \n",
        "weights_matrix_zh_val = torch.from_numpy(weights_matrix_zh_val).to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4416\n",
            "380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2UL5zzkP307M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pkl.dump(weights_matrix_eng, open(folder_path +'data/weights_matrix_eng_1M.pkl', 'wb'))\n",
        "# pkl.dump(weights_matrix_zh, open(folder_path +'data/weights_matrix_zh_1M.pkl', 'wb'))\n",
        "\n",
        "weights_matrix_eng=pkl.load(open(folder_path +'data/weights_matrix_eng_1M.pkl', 'rb'))\n",
        "weights_matrix_zh=pkl.load(open(folder_path +'data/weights_matrix_zh_1M.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Jancv1aTBc5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ]
    },
    {
      "metadata": {
        "id": "-IA_9_MdJNGX",
        "colab_type": "code",
        "outputId": "5d152434-7c53-4993-a179-5d98acea1d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "val_input_lang.index2word[6]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'岁'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "rjOzDI9w4B5f",
        "colab_type": "code",
        "outputId": "2962ae7b-14ee-45ad-a8fb-6c77d4510543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "words2id_zh['岁']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "EtQLeRS54pQo",
        "colab_type": "code",
        "outputId": "88e8835c-f8d3-48b7-8d56-53683a61a744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "weights_matrix_zh_val[6]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0742,  0.1610,  0.5834, -0.2587, -0.0924,  0.3677, -0.0068,  0.1639,\n",
              "        -0.2562, -0.0389, -0.0893,  0.1895, -0.1605,  0.1799, -0.3470,  0.2639,\n",
              "        -0.0279,  0.1208, -0.0002, -0.1331,  0.0515, -0.0742, -0.0718, -0.0943,\n",
              "         0.0460, -0.0566, -0.0444,  0.1222,  0.1252,  0.1181, -0.1571, -0.1455,\n",
              "         0.0063,  0.0591, -0.2001, -0.0302, -0.0281, -0.1427, -0.0172, -0.0336,\n",
              "        -0.0222, -0.1038, -0.0331,  0.0001, -0.1582,  0.0262,  0.4928, -0.0186,\n",
              "        -0.1369, -0.1133,  0.0135,  0.0364,  0.1875, -0.0480,  0.0424,  0.1370,\n",
              "        -0.1212,  0.0776,  0.1613,  0.1746, -0.0796,  0.2153,  0.0297, -0.0753,\n",
              "         0.0664, -0.0660,  0.2754,  0.1332,  0.0320,  0.0823, -0.0666,  0.1211,\n",
              "        -0.0784,  0.0059, -0.3059, -0.1010,  0.1475, -0.2038,  0.1403, -0.0860,\n",
              "         0.0278,  0.0723, -0.1368, -0.1716,  0.0442,  0.0866,  0.1782, -0.2004,\n",
              "         0.0252,  0.1130, -0.1269, -0.1549, -0.0814, -0.0181, -0.1005, -0.1016,\n",
              "        -0.0432,  0.0319, -0.0503, -0.2501, -0.1788,  0.0587,  0.1694, -0.1800,\n",
              "         0.1864,  0.0518, -0.0809,  0.0426, -0.0796,  0.1179, -0.2064,  0.0351,\n",
              "         0.0599, -0.1230, -0.0540,  0.0895, -0.0642,  0.1507, -0.1129,  0.0973,\n",
              "        -0.2186,  0.0616,  0.1776,  0.1714, -0.0476, -0.1254, -0.0499,  0.1656,\n",
              "         0.0072, -0.0975,  0.0036,  0.0188,  0.0779,  0.0514,  0.1057,  0.1097,\n",
              "         0.0863, -0.1850,  0.1000, -0.1321,  0.2129,  0.0798, -0.0471,  0.0270,\n",
              "         0.0865, -0.1282,  0.0261,  0.1125,  0.0097,  0.0426, -0.2114,  0.1963,\n",
              "        -0.0299,  0.1154,  0.0867, -0.0684,  0.1431,  0.1590,  0.1820, -0.0875,\n",
              "        -0.1133,  0.2867, -0.1591, -0.1754, -0.0891, -0.1831, -0.0792, -0.0438,\n",
              "         0.0420,  0.0544,  0.1429, -0.1938,  0.2777, -0.0533, -0.0446,  0.0484,\n",
              "        -0.0216, -0.0199, -0.2832, -0.1396, -0.0149, -0.5460, -0.0609,  0.0465,\n",
              "        -0.2143,  0.1313,  0.0798,  0.0994, -0.1314,  0.2913,  0.2143, -0.4836,\n",
              "        -0.0991, -0.1593,  0.1209, -0.1384,  0.0333,  0.0617, -0.0799,  0.2597,\n",
              "         0.0003,  0.0891, -0.2095, -0.0082,  0.1359,  0.1182, -0.3410,  0.1097,\n",
              "         0.0750,  0.1149,  0.1792, -0.1038, -0.0776,  0.0662, -0.1101, -0.0706,\n",
              "        -0.0418,  0.1085, -0.0134, -0.0855, -0.0419, -0.3518, -0.0430, -0.0544,\n",
              "         0.0481,  0.0576, -0.0136,  0.0308,  0.1305, -0.7567, -0.0389,  0.7089,\n",
              "         0.0686, -0.1809, -0.1014,  0.0946,  0.0929, -0.0077,  0.0370, -0.1058,\n",
              "         0.0552,  0.1871, -0.0393,  0.0735,  0.0484, -0.1429,  0.1469, -0.0420,\n",
              "         0.1573, -0.1037,  0.1145,  0.1111, -0.0564, -0.0439,  0.2143, -0.0264,\n",
              "        -0.1127,  0.2052,  0.0880, -0.1414, -0.2030,  0.0100, -0.0487, -0.0648,\n",
              "         0.2036,  0.0258, -0.1348, -0.1100,  0.0166,  0.0057,  0.0662,  0.2087,\n",
              "         0.2254,  0.1943, -0.2489,  0.1574,  0.0720,  0.1286,  0.0990, -0.0362,\n",
              "        -0.3353,  0.0576, -0.1324,  0.0377, -0.1747, -0.0127, -0.1299,  0.0257,\n",
              "        -0.6348, -0.0361, -0.1602, -0.0574,  0.1557, -0.1146,  0.2789, -0.0687,\n",
              "        -0.1191, -0.0521,  0.1003, -0.1053],\n",
              "       device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "8cyLitkK4mic",
        "colab_type": "code",
        "outputId": "30e7a00f-1736-40d4-c8ad-166af67c2a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "cell_type": "code",
      "source": [
        "loaded_embeddings_zh[528]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.420e-02,  1.610e-01,  5.834e-01, -2.587e-01, -9.240e-02,\n",
              "        3.677e-01, -6.800e-03,  1.639e-01, -2.562e-01, -3.890e-02,\n",
              "       -8.930e-02,  1.895e-01, -1.605e-01,  1.799e-01, -3.470e-01,\n",
              "        2.639e-01, -2.790e-02,  1.208e-01, -2.000e-04, -1.331e-01,\n",
              "        5.150e-02, -7.420e-02, -7.180e-02, -9.430e-02,  4.600e-02,\n",
              "       -5.660e-02, -4.440e-02,  1.222e-01,  1.252e-01,  1.181e-01,\n",
              "       -1.571e-01, -1.455e-01,  6.300e-03,  5.910e-02, -2.001e-01,\n",
              "       -3.020e-02, -2.810e-02, -1.427e-01, -1.720e-02, -3.360e-02,\n",
              "       -2.220e-02, -1.038e-01, -3.310e-02,  1.000e-04, -1.582e-01,\n",
              "        2.620e-02,  4.928e-01, -1.860e-02, -1.369e-01, -1.133e-01,\n",
              "        1.350e-02,  3.640e-02,  1.875e-01, -4.800e-02,  4.240e-02,\n",
              "        1.370e-01, -1.212e-01,  7.760e-02,  1.613e-01,  1.746e-01,\n",
              "       -7.960e-02,  2.153e-01,  2.970e-02, -7.530e-02,  6.640e-02,\n",
              "       -6.600e-02,  2.754e-01,  1.332e-01,  3.200e-02,  8.230e-02,\n",
              "       -6.660e-02,  1.211e-01, -7.840e-02,  5.900e-03, -3.059e-01,\n",
              "       -1.010e-01,  1.475e-01, -2.038e-01,  1.403e-01, -8.600e-02,\n",
              "        2.780e-02,  7.230e-02, -1.368e-01, -1.716e-01,  4.420e-02,\n",
              "        8.660e-02,  1.782e-01, -2.004e-01,  2.520e-02,  1.130e-01,\n",
              "       -1.269e-01, -1.549e-01, -8.140e-02, -1.810e-02, -1.005e-01,\n",
              "       -1.016e-01, -4.320e-02,  3.190e-02, -5.030e-02, -2.501e-01,\n",
              "       -1.788e-01,  5.870e-02,  1.694e-01, -1.800e-01,  1.864e-01,\n",
              "        5.180e-02, -8.090e-02,  4.260e-02, -7.960e-02,  1.179e-01,\n",
              "       -2.064e-01,  3.510e-02,  5.990e-02, -1.230e-01, -5.400e-02,\n",
              "        8.950e-02, -6.420e-02,  1.507e-01, -1.129e-01,  9.730e-02,\n",
              "       -2.186e-01,  6.160e-02,  1.776e-01,  1.714e-01, -4.760e-02,\n",
              "       -1.254e-01, -4.990e-02,  1.656e-01,  7.200e-03, -9.750e-02,\n",
              "        3.600e-03,  1.880e-02,  7.790e-02,  5.140e-02,  1.057e-01,\n",
              "        1.097e-01,  8.630e-02, -1.850e-01,  1.000e-01, -1.321e-01,\n",
              "        2.129e-01,  7.980e-02, -4.710e-02,  2.700e-02,  8.650e-02,\n",
              "       -1.282e-01,  2.610e-02,  1.125e-01,  9.700e-03,  4.260e-02,\n",
              "       -2.114e-01,  1.963e-01, -2.990e-02,  1.154e-01,  8.670e-02,\n",
              "       -6.840e-02,  1.431e-01,  1.590e-01,  1.820e-01, -8.750e-02,\n",
              "       -1.133e-01,  2.867e-01, -1.591e-01, -1.754e-01, -8.910e-02,\n",
              "       -1.831e-01, -7.920e-02, -4.380e-02,  4.200e-02,  5.440e-02,\n",
              "        1.429e-01, -1.938e-01,  2.777e-01, -5.330e-02, -4.460e-02,\n",
              "        4.840e-02, -2.160e-02, -1.990e-02, -2.832e-01, -1.396e-01,\n",
              "       -1.490e-02, -5.460e-01, -6.090e-02,  4.650e-02, -2.143e-01,\n",
              "        1.313e-01,  7.980e-02,  9.940e-02, -1.314e-01,  2.913e-01,\n",
              "        2.143e-01, -4.836e-01, -9.910e-02, -1.593e-01,  1.209e-01,\n",
              "       -1.384e-01,  3.330e-02,  6.170e-02, -7.990e-02,  2.597e-01,\n",
              "        3.000e-04,  8.910e-02, -2.095e-01, -8.200e-03,  1.359e-01,\n",
              "        1.182e-01, -3.410e-01,  1.097e-01,  7.500e-02,  1.149e-01,\n",
              "        1.792e-01, -1.038e-01, -7.760e-02,  6.620e-02, -1.101e-01,\n",
              "       -7.060e-02, -4.180e-02,  1.085e-01, -1.340e-02, -8.550e-02,\n",
              "       -4.190e-02, -3.518e-01, -4.300e-02, -5.440e-02,  4.810e-02,\n",
              "        5.760e-02, -1.360e-02,  3.080e-02,  1.305e-01, -7.567e-01,\n",
              "       -3.890e-02,  7.089e-01,  6.860e-02, -1.809e-01, -1.014e-01,\n",
              "        9.460e-02,  9.290e-02, -7.700e-03,  3.700e-02, -1.058e-01,\n",
              "        5.520e-02,  1.871e-01, -3.930e-02,  7.350e-02,  4.840e-02,\n",
              "       -1.429e-01,  1.469e-01, -4.200e-02,  1.573e-01, -1.037e-01,\n",
              "        1.145e-01,  1.111e-01, -5.640e-02, -4.390e-02,  2.143e-01,\n",
              "       -2.640e-02, -1.127e-01,  2.052e-01,  8.800e-02, -1.414e-01,\n",
              "       -2.030e-01,  1.000e-02, -4.870e-02, -6.480e-02,  2.036e-01,\n",
              "        2.580e-02, -1.348e-01, -1.100e-01,  1.660e-02,  5.700e-03,\n",
              "        6.620e-02,  2.087e-01,  2.254e-01,  1.943e-01, -2.489e-01,\n",
              "        1.574e-01,  7.200e-02,  1.286e-01,  9.900e-02, -3.620e-02,\n",
              "       -3.353e-01,  5.760e-02, -1.324e-01,  3.770e-02, -1.747e-01,\n",
              "       -1.270e-02, -1.299e-01,  2.570e-02, -6.348e-01, -3.610e-02,\n",
              "       -1.602e-01, -5.740e-02,  1.557e-01, -1.146e-01,  2.789e-01,\n",
              "       -6.870e-02, -1.191e-01, -5.210e-02,  1.003e-01, -1.053e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "eVIfRDKA0Z2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def generate_weights_matrix(idx2words,loaded_embeddings):\n",
        "   \n",
        "#     matrix_len = len(idx2words)\n",
        "#     weights_matrix = np.zeros((matrix_len, 300))\n",
        "    \n",
        "#     for key in idx2words.keys():\n",
        "        \n",
        "#         try: \n",
        "#             weights_matrix[key]\n",
        "#             loaded_embeddings[key]\n",
        "#             weights_matrix[key] = loaded_embeddings[key]\n",
        "#         except KeyError:\n",
        "#             weights_matrix[key] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
        "#     return weights_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZkL7tHO1Gk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# words2id_eng,idx2words_eng,loaded_embeddings_eng = load_emb_matrix('english')\n",
        "# words2id_zh,idx2words_zh,loaded_embeddings_zh = load_emb_matrix('chinese')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ku5T3oez1RPH",
        "colab_type": "code",
        "outputId": "1851ecb6-30ea-4ef1-ad3d-3d7c449fa6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# weights_matrix_eng=generate_weights_matrix(idx2words,loaded_embeddings_eng)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tsDhDz0Jo57e",
        "colab_type": "code",
        "outputId": "1f95321c-c53f-4776-8309-4c3c312013b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# weights_matrix_eng.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([79366, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "metadata": {
        "id": "htO3kD7foNsy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# words2id_eng = output_lang.word2index\n",
        "# idx2words_eng= output_lang.index2word\n",
        "# loaded_embeddings_eng= pkl.load(open(folder_path +'data/embedding_matrix_eng.pkl', 'rb'))\n",
        "# words2id_zh = input_lang.word2index\n",
        "# idx2words_zh= input_lang.index2word\n",
        "# loaded_embeddings_zh= pkl.load(open(folder_path +'data/embedding_matrix_zh.pkl', 'rb'))\n",
        "# weights_matrix_eng = generate_weights_matrix(idx2words_eng,loaded_embeddings_eng)\n",
        "# weights_matrix_zh = generate_weights_matrix(idx2words_zh,loaded_embeddings_zh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNs5BYqvuiIq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #weights_matrix_eng = generate_weights_matrix(idx2words_eng,loaded_embeddings_eng)\n",
        "# #pkl.dump(weights_matrix_eng, open(folder_path + 'data/weights_matrix_eng.pkl', 'wb'))\n",
        "\n",
        "# weights_matrix_eng=pkl.load(open(folder_path + 'data/weights_matrix_eng.pkl', 'rb'))\n",
        "# weights_matrix_eng = torch.from_numpy(weights_matrix_eng).to(device)\n",
        "\n",
        "# #weights_matrix_zh = generate_weights_matrix(idx2words_zh,loaded_embeddings_zh)\n",
        "# #pkl.dump(weights_matrix_zh, open(folder_path + 'data/weights_matrix_zh.pkl', 'wb'))\n",
        "\n",
        "# weights_matrix_zh=pkl.load(open(folder_path + 'data/weights_matrix_zh.pkl', 'rb'))\n",
        "# weights_matrix_zh = torch.from_numpy(weights_matrix_zh).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yt7SPyNP-afV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# #define a class of language\n",
        "# class Language:\n",
        "#     def __init__(self, name,word2index,index2word):\n",
        "#         self.name = name\n",
        "#         self.word2index = word2index\n",
        "#         #self.word2count = {}\n",
        "#         self.index2word = index2word\n",
        "#         self.n_words = len(word2index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jRnrrZg-d1B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Turn a Unicode string to plain ASCII, thanks to\n",
        "# # http://stackoverflow.com/a/518232/2809427\n",
        "# def unicodeToAscii(s):\n",
        "#     return ''.join(\n",
        "#         c for c in unicodedata.normalize('NFD', s)\n",
        "#         if unicodedata.category(c) != 'Mn'\n",
        "#     )\n",
        "\n",
        "# # Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "# def normalizeString(s):\n",
        "#     s = s.replace(r\"&quot;\",\"\")\n",
        "#     s = s.replace(r\"&apos;\",\"'\")\n",
        "#     s = unicodeToAscii(s.strip())\n",
        "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "#     return s\n",
        "  \n",
        "  \n",
        "# def filterPair(p):\n",
        "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "#         len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "# def filterPairs(pairs):\n",
        "#     return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RzerrpTv-zkX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #To read the data file we will split the file into lines, and then split lines into pairs. \n",
        "\n",
        "# def readLanguages_sample(input_lang,target_lang):\n",
        "#     print(\"\\nReading lines...\")\n",
        "\n",
        "#     # Read the file and split into lines\n",
        "#     input_lines = open(folder_path + input_lang, encoding='utf-8').\\\n",
        "#         read().strip().split('\\n')\n",
        "#     target_lines = open(folder_path + target_lang, encoding='utf-8').\\\n",
        "#         read().strip().split('\\n')\n",
        "\n",
        "#     # Split every line and normalize\n",
        "#     #for chinese input, strip the space at the begining and end of the sentence\n",
        "#     #for english output, use normalizeString function\n",
        "#     sample_input = input_lines[:10000]\n",
        "#     sample_target = target_lines[:10000]\n",
        "    \n",
        "#     input_lines_norm = [l.strip() for l in sample_input]\n",
        "#     target_lines_norm = [normalizeString(l) for l in sample_target]\n",
        "    \n",
        "#     #build pairs\n",
        "#     #drop pair if both zh and en are empty strings\n",
        "#     pairs = [[item[0],item[1]] for item in zip(input_lines_norm,target_lines_norm) if len(item[0])+len(item[1]) != 0]\n",
        "    \n",
        "#     input_lines = Language(\"zh\")\n",
        "#     target_lines = Language(\"en\")\n",
        "\n",
        "#     return input_lines, target_lines, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FRwKm-ji-fQI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #To read the data file we will split the file into lines, and then split lines into pairs. \n",
        "\n",
        "# def readLanguages(input_lang,target_lang):\n",
        "#     print(\"\\nReading lines...\")\n",
        "\n",
        "#     # Read the file and split into lines\n",
        "#     input_lines = open(folder_path + input_lang, encoding='utf-8').\\\n",
        "#         read().strip().split('\\n')\n",
        "#     target_lines = open(folder_path + target_lang, encoding='utf-8').\\\n",
        "#         read().strip().split('\\n')\n",
        "\n",
        "#     # Split every line and normalize\n",
        "#     #for chinese input, strip the space at the begining and end of the sentence\n",
        "#     #for english output, use normalizeString function\n",
        "#     input_lines_norm = [l.strip() for l in input_lines]\n",
        "#     target_lines_norm = [normalizeString(l) for l in target_lines]\n",
        "    \n",
        "#     #build pairs\n",
        "#     #drop pair if both zh and en are empty strings\n",
        "#     pairs = [[item[0],item[1]] for item in zip(input_lines_norm,target_lines_norm) if len(item[0])+len(item[1]) != 0]\n",
        "    \n",
        "#     input_lines = Language(\"zh\",words2id_zh,idx2words_zh)\n",
        "#     target_lines = Language(\"en\",words2id_eng,idx2words_eng)\n",
        "\n",
        "#     return input_lines, target_lines, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FX6hvr77-pUq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def prepareData(input_lang, target_lang):\n",
        "#     input_lang, output_lang, pairs = readLanguages(input_lang, target_lang)\n",
        "#     print(\"Read %s sentence pairs\" % len(pairs))\n",
        "#     pairs = filterPairs(pairs)\n",
        "#     print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "#     print(\"Counting words...\")\n",
        "# #     for pair in pairs:\n",
        "# #         input_lang.addSentence(pair[0])\n",
        "# #         output_lang.addSentence(pair[1])\n",
        "#     print(\"Counted words:\")\n",
        "#     print(input_lang.name, input_lang.n_words)\n",
        "#     print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "#     return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gsOpgjHA-hMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_input_lang, train_output_lang, train_pairs = prepareData(train_zh, train_en)\n",
        "# print(\"print a random pair of training pairs:\")\n",
        "# print(random.choice(train_pairs))\n",
        "\n",
        "\n",
        "\n",
        "# val_input_lang, val_output_lang, val_pairs = prepareData(val_zh, val_en)\n",
        "# print(\"print a random pair of validation pairs:\")\n",
        "# print(random.choice(val_pairs))\n",
        "\n",
        "\n",
        "# # pkl.dump(train_input, open(folder_path +'data/train_input.pkl', 'wb'))\n",
        "# # pkl.dump(train_output, open(folder_path +'data/train_output.pkl', 'wb'))\n",
        "# # pkl.dump(train_pairs, open(folder_path +'data/train_pairs.pkl', 'wb'))\n",
        "# # pkl.dump(val_input, open(folder_path +'data/val_input.pkl', 'wb'))\n",
        "# # pkl.dump(val_output, open(folder_path +'data/val_output.pkl', 'wb'))\n",
        "# # pkl.dump(val_pairs, open(folder_path +'data/val_pairs.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ozVXC-QyNPR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ]
    },
    {
      "metadata": {
        "id": "O_batbZJ-jUl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index else UNK_IDX for word in sentence.split(' ')] + [EOS_token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLz7Ne86_Op7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "class VocabDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pairs,input_language, output_language):\n",
        "        \"\"\"\n",
        "        @param pairs: pairs of input and target sentences(raw text sentences)\n",
        "        @param input_language: Class Lang of input languages (zh in this case)\n",
        "        @param output_language: Class Lang of output languages (en in this case)\n",
        "\n",
        "        \"\"\"\n",
        "        self.pairs = pairs\n",
        "        self.inputs = [pair[0] for pair in pairs]\n",
        "        self.input_lang = input_language\n",
        "        self.output_lang = output_language\n",
        "        self.outputs = [pair[1] for pair in pairs]\n",
        "        \n",
        "        \n",
        "        #assert self.input_lang == self.target_lang\n",
        "       \n",
        "    def __len__(self):\n",
        "         return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        #turn raw text sentecens into indices\n",
        "        input_ = indexesFromSentence(self.input_lang, self.inputs[key])\n",
        "        output = indexesFromSentence(self.output_lang, self.outputs[key])\n",
        "        #print (output)\n",
        "        #print both the length of the source sequence and the target sequence\n",
        "        return [input_,len(input_),output,len(output)]\n",
        "    \n",
        "    \n",
        "    def __gettext__(self,key):\n",
        "      return [self.inputs[key],self.outputs[key]]\n",
        "\n",
        "def vocab_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all\n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    input_data_list = []\n",
        "    output_data_list = []\n",
        "   \n",
        "    \n",
        "    for datum in batch:\n",
        "      input_data_list.append(datum[0])\n",
        "      output_data_list.append(datum[2])\n",
        "      \n",
        "      \n",
        "    # Zip into pairs, sort by length (descending), unzip\n",
        "    seq_pairs = sorted(zip(input_data_list, output_data_list), key=lambda p: len(p[0]), reverse=True)\n",
        "    input_seqs, output_seqs = zip(*seq_pairs)\n",
        "    \n",
        "    #store the length of the sequences \n",
        "    input_data_len = [len(p) for p in input_seqs]\n",
        "    output_data_len = [len(p) for p in output_seqs]\n",
        "    \n",
        "    #padding\n",
        "    padded_vec_input = [np.pad(np.array(p),\n",
        "                                 pad_width=((0,MAX_LENGTH-len(p))),\n",
        "                                 mode=\"constant\", constant_values=PAD_IDX) for p in input_seqs]\n",
        "        \n",
        "    padded_vec_output = [np.pad(np.array(p),\n",
        "                                 pad_width=((0,MAX_LENGTH-len(p))),\n",
        "                                 mode=\"constant\", constant_values=PAD_IDX) for p in output_seqs]      \n",
        "    \n",
        "    \n",
        "    input_var = Variable(torch.LongTensor(padded_vec_input))\n",
        "    output_var = Variable(torch.LongTensor(padded_vec_output))\n",
        "    input_data_len = Variable(torch.LongTensor(input_data_len))\n",
        "    output_data_len = Variable(torch.LongTensor(output_data_len))\n",
        "    \n",
        "    \n",
        "    return [input_var,input_data_len,output_var,output_data_len]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rRbCrU0z_v2o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build train and valid dataloaders\n",
        "\n",
        "# train_dataset = VocabDataset(train_pairs,train_input_lang, train_output_lang)\n",
        "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "#                                            batch_size=BATCH_SIZE,\n",
        "#                                            collate_fn=vocab_collate_func,\n",
        "#                                            shuffle=True,\n",
        "#                                            drop_last = True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9z1LH4LvTMwS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test"
      ]
    },
    {
      "metadata": {
        "id": "NFX3YvX-yz2B",
        "colab_type": "code",
        "outputId": "8eb2272a-8448-4f08-d3a6-2a65c73bc968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "EOS_token"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "KjCdG1QI_xNu",
        "colab_type": "code",
        "outputId": "3aa4a01f-ca5b-4a93-9ebd-b68b3b7e2a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 5, 6, 7, 8, 8, 9, 8, 8, 10, 8, 1], 12, [4, 5, 6, 7, 8, 1], 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "aM4ltrQY3Trg",
        "colab_type": "code",
        "outputId": "f4a8776f-6fef-495d-ab9f-97645a0c35ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_pairs[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['深海 海中 的 生命   大卫   盖罗 ', 'life in the deep oceans']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "kCANjXu91Nyx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in train_loader:\n",
        "  [input_var,input_data_len,output_var,output_data_len]=i\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WGNnOUkZ1uWk",
        "colab_type": "code",
        "outputId": "0cbcf89f-c966-4b88-8b6b-231788de116f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "input_var[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5239,     8,     8,  1812, 23555,     8,     8,     6,  4421,     8,\n",
              "            8,     8,   959,   137,  7136, 16848,  3124, 40532,     6,  1031,\n",
              "            8,     8,   266, 18779,  1157,     6,  1156,   978,     8,     8,\n",
              "          388, 41393, 10137,  1157,     6, 15598,   978,     8,     8,     1,\n",
              "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "XpDtRegB11cL",
        "colab_type": "code",
        "outputId": "9b833e91-df6e-4e91-dc87-16cd1b07cbf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_input_lang.index2word[960]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'玩耍'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "-8dk3XNi6ycF",
        "colab_type": "code",
        "outputId": "cb94d3b9-1c6c-460f-cef4-2cf3b007bc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "for x in input_var[0]:\n",
        "  print (train_input_lang.index2word[x.data.tolist()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "在\n",
            "展出\n",
            "的\n",
            "照片\n",
            "\n",
            "\n",
            "男子\n",
            "和\n",
            "妇女\n",
            "被\n",
            "剥光\n",
            "衣服\n",
            "\n",
            "\n",
            "\n",
            "一些\n",
            "试图\n",
            "掩盖\n",
            "他们\n",
            "的\n",
            "生殖\n",
            "生殖器\n",
            "\n",
            "\n",
            "\n",
            "别人\n",
            "不敢\n",
            "打扰\n",
            "\n",
            "\n",
            "\n",
            "白色\n",
            "的\n",
            "背景\n",
            "\n",
            "\n",
            "一字排开\n",
            "排开\n",
            "\n",
            "\n",
            "等待\n",
            "拍摄\n",
            "成\n",
            "一条\n",
            "水渠\n",
            "里\n",
            "扔\n",
            "\n",
            "\n",
            "<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GbqLi-5P5Ek6",
        "colab_type": "code",
        "outputId": "b6c9212c-6d11-462e-9d91-d0e49b3959fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "output_var[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    6,  1803,   388,   108,  6760,    13,  3498,    19,  1423,  3791,\n",
              "        13800,    26,   352,    23,  1270,   262, 38357,  1680,   605, 10960,\n",
              "           23,  8849, 14783,   163,     5,  7587,   607,    23,   160,  1942,\n",
              "           19,  3553,   116,   108,  6587,    13,     1,     2,     2,     2,\n",
              "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "RHKS4rPD5IhB",
        "colab_type": "code",
        "outputId": "ef9ecf86-4d2b-4653-c279-8819447b58c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "for x in output_var[0]:\n",
        "  print (train_output_lang.index2word[x.data.tolist()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the\n",
            "display\n",
            "was\n",
            "a\n",
            "photo\n",
            ".\n",
            "men\n",
            "and\n",
            "women\n",
            "stripped\n",
            "naked\n",
            "some\n",
            "trying\n",
            "to\n",
            "cover\n",
            "their\n",
            "genitals\n",
            "others\n",
            "too\n",
            "frightened\n",
            "to\n",
            "bother\n",
            "lined\n",
            "up\n",
            "in\n",
            "snow\n",
            "waiting\n",
            "to\n",
            "be\n",
            "shot\n",
            "and\n",
            "thrown\n",
            "into\n",
            "a\n",
            "ditch\n",
            ".\n",
            "<EOS>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jDJsqrFayR4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ]
    },
    {
      "metadata": {
        "id": "Ac8yy73v_QiC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, weights_matrix, input_size, hidden_size,n_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "     \n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.n_layers = n_layers\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "        self.embedding.from_pretrained(weights_matrix, freeze=True, sparse=False)\n",
        "        #self.embedding.weight.requires_grad = True\n",
        "\n",
        "        \n",
        "        self.gru = nn.GRU(self.embedding_dim, hidden_size, n_layers, bidirectional=True)\n",
        "        \n",
        "\n",
        "    def forward(self, input_seqs, input_len, hidden=None):\n",
        "\n",
        "       \n",
        "        embedded = self.embedding(input_seqs)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_len)\n",
        "        output, hidden = self.gru(packed, hidden)\n",
        "\n",
        "        output, output_len = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
        "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
        "        \n",
        "        return output,hidden\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JXSMSJUHyVMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ]
    },
    {
      "metadata": {
        "id": "NN6-E9GQ_SYx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, weights_matrix, hidden_size, output_size,n_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "        \n",
        "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "#         self.embedding.weight.data.copy_(weights_matrix)\n",
        "#         self.embedding.weight.requires_grad = True\n",
        "        #self.embedding.from_pretrained(weights_matrix, freeze=True, sparse=False)\n",
        "        \n",
        "        self.gru1 = nn.GRU(self.embedding_dim, hidden_size,n_layers)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size,n_layers)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_seq, hidden):\n",
        "        \n",
        "        embedded = self.embedding(input_seq) # dim = Batch_Size x embedding_dim\n",
        "        embedded = embedded.view(1, self.batch_size, self.embedding_dim) # S=1 x Batch_Size x embedding_dim\n",
        "        \n",
        "        rnn_output, hidden = self.gru1(embedded, hidden)\n",
        "        output = F.relu(rnn_output)\n",
        "        \n",
        "        output, hidden = self.gru2(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        \n",
        "        return output,hidden\n",
        "\n",
        "\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "veJMFCR__UzN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, weights_matrix, hidden_size, output_size, n_layers=1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "\n",
        "        #self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "#         self.embedding.weight.data.copy_(weights_matrix)\n",
        "#         self.embedding.weight.requires_grad = True\n",
        "        #self.embedding.from_pretrained(weights_matrix, freeze=True, sparse=False)\n",
        "  \n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "       \n",
        "        self.gru1 = nn.GRU(self.embedding_dim, hidden_size,n_layers)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size,n_layers)\n",
        "        \n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input_seq, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_seq).view(1, 1, -1)\n",
        " \n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxYk3UJQ_W8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        \n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
        "     \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "      energy = self.score(hidden, encoder_outputs)\n",
        "      \n",
        "      score = F.softmax(energy, dim = 1).view(1, self.batch_size, -1)\n",
        "      \n",
        "      context_vector = torch.bmm(score.transpose(1,0), encoder_outputs.transpose(1,0))\n",
        "      \n",
        "      return context_vector, score\n",
        "#         max_len = encoder_outputs.size(0)\n",
        "#         this_batch_size = encoder_outputs.size(1)\n",
        "        \n",
        "#         # Create variable to store attention energies\n",
        "#         attn_energies = Variable(torch.zeros(this_batch_size, max_len)).to(device) # Batch_Size x Seq_Length\n",
        "        \n",
        "        \n",
        "# #         # For each batch of encoder outputs\n",
        "#         for b in range(this_batch_size):\n",
        "#             # Calculate energy for each encoder output\n",
        "#             for i in range(max_len):\n",
        "#                 attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
        "\n",
        "#         # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
        "#         return F.softmax(attn_energies,dim=1).unsqueeze(1)\n",
        "      \n",
        "    def score(self, hidden, encoder_output):\n",
        "        self.batch_size = hidden.shape[1]\n",
        "#         print (hidden.shape)\n",
        "#         print (encoder_output.shape)\n",
        "        if self.method == 'dot':\n",
        "#             print (hidden.shape)\n",
        "#             print (encoder_output.shape)\n",
        "            energy = hidden[0].dot(encoder_output[0])\n",
        "            return energy\n",
        "        \n",
        "        elif self.method == 'general':\n",
        "            energy = torch.bmm(encoder_output.transpose(1,0), self.attn(hidden.squeeze(0)).unsqueeze(2)) \n",
        "            \n",
        "            #energy = self.attn(encoder_output)\n",
        "#             print(hidden.shape)\n",
        "#             print(energy.shape)\n",
        "            #energy = torch.mm(hidden, energy.transpose(0,1))\n",
        "            return energy\n",
        "        \n",
        "        elif self.method == 'concat':\n",
        "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
        "            #print (energy.shape)\n",
        "            energy = self.v.dot(energy[0])\n",
        "            return energy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nik892T7_YoJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, weights_matrix, hidden_size, output_size, n_layers=1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        #self.batch_size = BATCH_SIZE\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "\n",
        "\n",
        "        # Define layers\n",
        "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "#         self.embedding.weight.data.copy_(weights_matrix)\n",
        "#         self.embedding.weight.requires_grad = True\n",
        "        self.embedding.from_pretrained(weights_matrix, freeze=True, sparse=False)\n",
        "        \n",
        "        \n",
        "        self.gru1 = nn.GRU(self.embedding_dim, hidden_size,n_layers)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size,n_layers)\n",
        "        \n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # Choose attention model\n",
        "        if attn_model != 'none':\n",
        "            self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step at a time\n",
        "\n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        \n",
        "        self.batch_size=input_seq.size(0)\n",
        "        #print (self.batch_size)\n",
        "        embedded = self.embedding(input_seq) # dim = Batch_Size x embedding_dim\n",
        "        embedded = embedded.view(1, self.batch_size, self.embedding_dim) # S=1 x Batch_Size x embedding_dim\n",
        "\n",
        "        # Get current hidden state from input word and last hidden state\n",
        "        # rnn_output : [1 x batch_size x hidden_size]\n",
        "        # hidden: [layer x batch_size x hidden_size]\n",
        "#         print (embedded.shape)\n",
        "#         print (last_hidden.shape)\n",
        "        rnn_output, hidden = self.gru1(embedded, last_hidden)\n",
        "        \n",
        "        # Calculate attention from current RNN state and all encoder outputs;\n",
        "        # apply to encoder outputs to get weighted average\n",
        "        context, attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "\n",
        "        \n",
        "#         attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "#         context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
        "\n",
        "        # Attentional vector using the RNN hidden state and context vector\n",
        "        # concatenated together (Luong eq. 5)\n",
        "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "\n",
        "        # Finally predict next token (Luong eq. 6, without softmax)\n",
        "        output = self.out(concat_output)\n",
        "\n",
        "        #Return final output, hidden state, and attention weights (for visualization)\n",
        "        return output, hidden, attn_weights\n",
        "        #return attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8N3Gbi_t_aJo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BahdanauAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, weights_matrix, hidden_size, output_size, n_layers=1):\n",
        "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
        "        \n",
        "        # Define parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "        \n",
        "        \n",
        "        # Define layers\n",
        "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "#         self.embedding.weight.data.copy_(weights_matrix)\n",
        "#         self.embedding.weight.requires_grad = True\n",
        "        \n",
        "        self.attn = Attn('concat', hidden_size)\n",
        "        self.gru1 = nn.GRU(self.embedding_dim, hidden_size,n_layers)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size,n_layers)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step at a time\n",
        "        # TODO: FIX BATCHING\n",
        "        \n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
        "        \n",
        "        # Calculate attention weights and apply to encoder outputs\n",
        "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
        "        context = context.transpose(0, 1) # 1 x B x N\n",
        "        \n",
        "        # Combine embedded input word and attended context, run through RNN\n",
        "        rnn_input = torch.cat((word_embedded, context), 2)\n",
        "        output, hidden = self.gru(rnn_input, last_hidden)\n",
        "        \n",
        "        # Final output layer\n",
        "        output = output.squeeze(0) # B x N\n",
        "        output = F.log_softmax(self.out(torch.cat((output, context), 1)),dim=1)\n",
        "        \n",
        "        # Return final output, hidden state, and attention weights (for visualization)\n",
        "        return output, hidden, attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bct0uwzoybjL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ]
    },
    {
      "metadata": {
        "id": "fR4JgAnI_by8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#record the run time\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wkWNQq3E_dGY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "82n4HDUwygzV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss function"
      ]
    },
    {
      "metadata": {
        "id": "Atk9GiIo_hkC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def masked_cross_entropy(logits, target, length):\n",
        "    length = Variable(torch.LongTensor(length)).to(device)\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "    \"\"\"\n",
        "    \n",
        "    # logits_flat: (batch * max_len, num_classes)\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # log_probs_flat: (batch * max_len, num_classes)\n",
        "    log_probs_flat = F.log_softmax(logits_flat,dim=1)\n",
        "    \n",
        "    # target_flat: (batch * max_len, 1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    \n",
        "    # losses_flat: (batch * max_len, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    # losses: (batch, max_len)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    # mask: (batch, max_len)\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / length.float().sum()\n",
        "    return loss\n",
        "  \n",
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    #print(sequence_length)\n",
        "    batch_size = len(sequence_length)\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    #batch_size = BATCH_SIZE\n",
        "    \n",
        "    seq_range = torch.arange(0, max_len).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
        "    seq_range_expand = Variable(seq_range_expand)\n",
        "    \n",
        "    seq_range_expand = seq_range_expand.to(device)\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    return seq_range_expand < seq_length_expand"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzgAAl74_ko5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the train function is now taking a batch at a time\n",
        "def train(input_batch, input_lengths, output_batch, output_lengths, encoder, decoder, encoder_optimizer, \n",
        "          decoder_optimizer, criterion, max_length=MAX_LENGTH, if_attention = True):\n",
        "    \n",
        "    encoder_outputs, encoder_hidden = encoder(input_batch, input_lengths, None)\n",
        "    #print (input_batch.size())\n",
        "    BATCH_SIZE=input_batch.size()[1]\n",
        "    # Prepare decoder input and outputs\n",
        "    decoder_input = Variable(torch.LongTensor([SOS_token] * BATCH_SIZE)).to(device)\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
        "    all_decoder_outputs = Variable(torch.zeros(max_length, BATCH_SIZE, decoder.output_size)).to(device)\n",
        "    \n",
        "    # Run through decoder one time step at a time\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    \n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "    if use_teacher_forcing:\n",
        "        # Run through decoder one time step at a time\n",
        "        for di in range(max_length):\n",
        "            if if_attention == True:\n",
        "                decoder_output, decoder_hidden, decoder_attn = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "            else:\n",
        "\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden)\n",
        "                \n",
        "            all_decoder_outputs[di] = decoder_output # Store this step's outputs\n",
        "            decoder_input = output_batch[di] # Next input is current target\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(max_length):\n",
        "            if if_attention == True:\n",
        "                decoder_output, decoder_hidden, decoder_attn = decoder(\n",
        "                  decoder_input, decoder_hidden, encoder_outputs)\n",
        "            else:\n",
        "\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "                \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            all_decoder_outputs[di] = decoder_output\n",
        "\n",
        "    \n",
        "    # Loss calculation and backpropagation\n",
        "    loss = masked_cross_entropy(\n",
        "            all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
        "            output_batch.transpose(0, 1).contiguous(), # -> batch x seq\n",
        "            output_lengths)    \n",
        "\n",
        "    loss.backward()\n",
        "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "    \n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "#     ec=0\n",
        "#     dc=0\n",
        "\n",
        "\n",
        "    return loss.item(), ec, dc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ViJaygPT_mY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(iters, criterion,  encoder, decoder, encoder_optimizer, decoder_optimizer, n_iters, train_loader, loss_list, print_every=1000, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    loss_list=[]\n",
        "    loss_avg=[]\n",
        "#     encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "#     decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "    #iters = 0\n",
        "  \n",
        "    while iters <= n_iters:\n",
        "      \n",
        "      \n",
        "      \n",
        "      for i, (input_var,input_data_len,output_var,output_data_len) in enumerate(train_loader):\n",
        "        print(\"Iteration:\", iters)\n",
        "        iters += 1\n",
        "        input_batch = input_var.transpose(0,1).to(device)\n",
        "        output_batch = output_var.transpose(0,1).to(device)\n",
        "        \n",
        "        loss, _, _ = train(input_batch,input_data_len,output_batch,output_data_len, encoder,\n",
        "                       decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        \n",
        "        \n",
        "        # Keep track of loss\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "        \n",
        "          \n",
        "        print('Loss: %s (%d %d%%) %.4f' % (timeSince(start, iters / n_iters),\n",
        "                                           iters, iters / n_iters * 100, loss))\n",
        "        loss_list.append(loss)\n",
        "        \n",
        "        if iters % print_every == 0:\n",
        "          \n",
        "          \n",
        "          \n",
        "          print_loss_avg = print_loss_total/print_every\n",
        "          print_loss_total = 0\n",
        "          loss_avg.append(print_loss_avg)\n",
        "          ##Learning Rate Decay\n",
        "          if (len(loss_avg)!=1):\n",
        "            loss_change  = loss_avg[-2]-loss_avg[-1]\n",
        "            print (\"loss_change: \", loss_change)\n",
        "            if (loss_change < 0.05):\n",
        "              \n",
        "              print(\"Learning Rate Decays:\")\n",
        "              for param_group in encoder_optimizer.param_groups:\n",
        "                \n",
        "                param_group['lr'] = param_group['lr']*0.5\n",
        "                print (\"Current Encoder Learning Rate: {}\". format (param_group['lr']))\n",
        "              for param_group in decoder_optimizer.param_groups:\n",
        "                \n",
        "                param_group['lr'] = param_group['lr']*0.5\n",
        "                print (\"Current Decoder Learning Rate: {}\". format (param_group['lr']))\n",
        "                \n",
        "              \n",
        "          print('Average Loss: %s (%d %d%%) %.4f' % (timeSince(start, iters / n_iters),\n",
        "                                           iters, iters / n_iters * 100, print_loss_avg))\n",
        "        \n",
        "          state = {'epoch': iters + 1, 'encoder_state_dict': encoder.state_dict(), 'decoder_state_dict': decoder.state_dict(),\n",
        "             'encoder_optimizer': encoder_optimizer.state_dict(), 'decoder_optimizer': decoder_optimizer.state_dict(), \"loss_list\": loss_list, \"loss_avg\": loss_avg}\n",
        "          \n",
        "          torch.save(state, folder_path+\"model_saved/Dec_4_state_val{}.pt\".format(iters))\n",
        "        if iters % plot_every == 0:\n",
        "         \n",
        "          plot_loss_avg = plot_loss_total / plot_every\n",
        "          plot_losses.append(plot_loss_avg)\n",
        "          plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_TjxDZP6ASEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "29c49979-5c32-455b-fdd2-bd48a8111f94"
      },
      "cell_type": "code",
      "source": [
        "# Configure models\n",
        "attn_model = 'dot'\n",
        "hidden_size = 300\n",
        "layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "start_epoch=0\n",
        "teacher_forcing_ratio = 0.8\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iters = 100000\n",
        "#def __init__(self, attn_model, weights_matrix, hidden_size, output_size, n_layers=1):\n",
        "# def __init__(self, weights_matrix, input_size, hidden_size,n_layers=1):\n",
        "# Initialize models\n",
        "encoder = EncoderRNN(weights_matrix_zh, train_input_lang.n_words, hidden_size, n_layers = layers).to(device)\n",
        "#decoder = DecoderRNN(weights_matrix_eng, hidden_size, train_output_lang.n_words, n_layers = layers).to(device)\n",
        "decoder= LuongAttnDecoderRNN(attn_model, weights_matrix_eng, hidden_size, train_output_lang.n_words,n_layers = layers).to(device)\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-672cb2786783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# def __init__(self, weights_matrix, input_size, hidden_size,n_layers=1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initialize models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_matrix_zh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#decoder = DecoderRNN(weights_matrix_eng, hidden_size, train_output_lang.n_words, n_layers = layers).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLuongAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_matrix_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_input_lang' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "X0Wyk3XmJnY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##Val\n",
        "\n",
        "# Configure models\n",
        "attn_model = 'dot'\n",
        "hidden_size = 300\n",
        "layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "start_epoch=0\n",
        "teacher_forcing_ratio = 0.8\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iters = 100000\n",
        "#def __init__(self, attn_model, weights_matrix, hidden_size, output_size, n_layers=1):\n",
        "# def __init__(self, weights_matrix, input_size, hidden_size,n_layers=1):\n",
        "# Initialize models\n",
        "encoder = EncoderRNN(weights_matrix_zh_val, val_input_lang.n_words, hidden_size, n_layers = layers).to(device)\n",
        "#decoder = DecoderRNN(weights_matrix_eng, hidden_size, train_output_lang.n_words, n_layers = layers).to(device)\n",
        "decoder= LuongAttnDecoderRNN(attn_model, weights_matrix_eng_val, hidden_size, val_output_lang.n_words,n_layers = layers).to(device)\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cK0Z6BEm4Ygp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load pretrained model"
      ]
    },
    {
      "metadata": {
        "id": "GYjTFdB8ui9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, iteration_num):\n",
        "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
        "    folder_path = os.getcwd() + '/gdrive/My Drive/NLP_Project/'\n",
        "    start_epoch = 0\n",
        "    filename=folder_path+\"model_saved/Dec_3_state_{}.pt\".format(iteration_num)\n",
        "    loss_list=[]\n",
        "    if os.path.isfile(filename):\n",
        "        print(\"=> loading checkpoint '{}'\".format(iteration_num))\n",
        "        checkpoint = torch.load(filename, map_location=device)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        #model.load_state_dict(checkpoint['state_dict'])\n",
        "        encoder.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
        "        decoder.load_state_dict(checkpoint[\"decoder_state_dict\"])\n",
        "        encoder_optimizer.load_state_dict(checkpoint[\"encoder_optimizer\"])\n",
        "        decoder_optimizer.load_state_dict(checkpoint[\"decoder_optimizer\"])\n",
        "        loss_list=checkpoint[\"loss_list\"]\n",
        "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        #losslogger = checkpoint['losslogger']\n",
        "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(filename, checkpoint['epoch']))\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "\n",
        "    return start_epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-bf8HU0ukUr",
        "colab_type": "code",
        "outputId": "45da7362-63f6-4a45-927f-29e4f46f0d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "start_epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_list=\\\n",
        "load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, 35)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint '35'\n",
            "=> loaded checkpoint '/content/gdrive/My Drive/NLP_Project/model_saved/Dec_3_state_35.pt' (epoch 36)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rdjz6yeSh0vH",
        "colab_type": "code",
        "outputId": "eecf768e-9ab4-41af-d115-27e4638d2cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "loss_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10.691746711730957,\n",
              " 10.640111923217773,\n",
              " 10.577738761901855,\n",
              " 10.501910209655762,\n",
              " 10.415942192077637,\n",
              " 10.232355117797852,\n",
              " 10.017264366149902,\n",
              " 9.824087142944336,\n",
              " 9.400732040405273,\n",
              " 9.257454872131348,\n",
              " 9.004199981689453,\n",
              " 8.721540451049805,\n",
              " 8.34797477722168,\n",
              " 8.137267112731934,\n",
              " 7.931396007537842,\n",
              " 7.693742752075195,\n",
              " 7.594590663909912,\n",
              " 7.484679222106934,\n",
              " 7.406115531921387,\n",
              " 7.229878902435303,\n",
              " 7.07252311706543,\n",
              " 7.083251953125,\n",
              " 6.836131572723389,\n",
              " 6.9537458419799805,\n",
              " 6.863166809082031,\n",
              " 6.8203349113464355,\n",
              " 6.656647682189941,\n",
              " 6.779574394226074,\n",
              " 6.719008922576904,\n",
              " 6.801124095916748,\n",
              " 6.802772045135498,\n",
              " 6.680784225463867,\n",
              " 6.685135841369629,\n",
              " 6.88391637802124,\n",
              " 6.49432897567749]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "CfaMD-xfAui8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "YlGoYaEv4d4d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "78d283c0-2eef-48f5-8ce3-dffaaca4e538",
        "id": "_mYK7qLG0C8q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "loss_list=[]\n",
        "#learning_rate=learning_rate*0.5\n",
        "# encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "# decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "trainIters(start_epoch, criterion, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iters, train_loader, loss_list, print_every=10, plot_every=1000000)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-4e81c4627eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mencoder_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "WHnOC8EYK5Yw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##Val\n",
        "\n",
        "# Configure models\n",
        "attn_model = 'general'\n",
        "hidden_size = 256\n",
        "layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "start_epoch=0\n",
        "teacher_forcing_ratio = 0.5\n",
        "learning_rate = 0.0005\n",
        "#decoder_learning_ratio = 5.0\n",
        "n_iters = 100000\n",
        "#def __init__(self, attn_model, weights_matrix, hidden_size, output_size, n_layers=1):\n",
        "# def __init__(self, weights_matrix, input_size, hidden_size,n_layers=1):\n",
        "# Initialize models\n",
        "encoder = EncoderRNN(weights_matrix_zh_val, val_input_lang.n_words, hidden_size, n_layers = layers).to(device)\n",
        "#decoder = DecoderRNN(weights_matrix_eng, hidden_size, train_output_lang.n_words, n_layers = layers).to(device)\n",
        "decoder= LuongAttnDecoderRNN(attn_model, weights_matrix_eng_val, hidden_size, val_output_lang.n_words,n_layers = layers).to(device)\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3GpH8wWDQr5s",
        "colab_type": "code",
        "outputId": "1a22f169-6d2e-480d-b544-439567a69409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
        "print (sum([np.prod(p.size()) for p in model_parameters]))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3424308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oJ4HON1BQhku",
        "colab_type": "code",
        "outputId": "fe66c26d-ad15-4310-f926-efacffb4ad51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "encoder"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderRNN(\n",
              "  (embedding): Embedding(4615, 300)\n",
              "  (gru): GRU(300, 256, num_layers=2, bidirectional=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "z2yTgMUOTd4E",
        "colab_type": "code",
        "outputId": "a4260f4b-5c3d-4080-8f07-6c55f118c1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "decoder"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LuongAttnDecoderRNN(\n",
              "  (embedding): Embedding(4615, 300)\n",
              "  (gru1): GRU(300, 256, num_layers=2)\n",
              "  (gru2): GRU(256, 256, num_layers=2)\n",
              "  (concat): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (out): Linear(in_features=256, out_features=2840, bias=True)\n",
              "  (attn): Attn(\n",
              "    (attn): Linear(in_features=512, out_features=256, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "MfZRR3qWOD2F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_dataset = VocabDataset(val_pairs,val_input_lang,val_output_lang)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S376gKDMe_04",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test"
      ]
    },
    {
      "metadata": {
        "id": "9-eWYQt2edbD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, (input_var,input_data_len,output_var,output_data_len) in enumerate(val_loader):\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9b1OniqJxs-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "478c4930-3d1b-4b29-f245-7d8676a358c2"
      },
      "cell_type": "code",
      "source": [
        "input_var.size()[0]"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "RJaDTgyneodi",
        "colab_type": "code",
        "outputId": "7415a4aa-14e0-44ae-f71e-06f56bf57be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "for x in output_var[0]:\n",
        "  print (val_output_lang.index2word[x.data.tolist()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "so\n",
            "one\n",
            "year\n",
            "ago\n",
            "i\n",
            "was\n",
            "just\n",
            "a\n",
            "boy\n",
            "in\n",
            "the\n",
            "savanna\n",
            "grassland\n",
            "herding\n",
            "my\n",
            "father\n",
            "apos\n",
            "s\n",
            "cows\n",
            "and\n",
            "i\n",
            "used\n",
            "to\n",
            "see\n",
            "planes\n",
            "flying\n",
            "over\n",
            "and\n",
            "i\n",
            "told\n",
            "myself\n",
            "that\n",
            "one\n",
            "day\n",
            "i\n",
            "apos\n",
            "ll\n",
            "be\n",
            "there\n",
            "inside\n",
            ".\n",
            "<EOS>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n",
            "<PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yR6DrBZefD6b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "QkXeKS703d8d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "HrhWbI71qTO7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        \n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
        "     \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "      energy = self.score(hidden, encoder_outputs)\n",
        "      \n",
        "      score = F.softmax(energy, dim = 1).view(1, self.batch_size, -1)\n",
        "      \n",
        "      context_vector = torch.bmm(score.transpose(1,0), encoder_outputs.transpose(1,0))\n",
        "      \n",
        "      return context_vector, score\n",
        "#         max_len = encoder_outputs.size(0)\n",
        "#         this_batch_size = encoder_outputs.size(1)\n",
        "        \n",
        "#         # Create variable to store attention energies\n",
        "#         attn_energies = Variable(torch.zeros(this_batch_size, max_len)).to(device) # Batch_Size x Seq_Length\n",
        "        \n",
        "        \n",
        "# #         # For each batch of encoder outputs\n",
        "#         for b in range(this_batch_size):\n",
        "#             # Calculate energy for each encoder output\n",
        "#             for i in range(max_len):\n",
        "#                 attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
        "\n",
        "#         # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
        "#         return F.softmax(attn_energies,dim=1).unsqueeze(1)\n",
        "      \n",
        "    def score(self, hidden, encoder_output):\n",
        "        self.batch_size = hidden.shape[1]\n",
        "#         print (hidden.shape)\n",
        "#         print (encoder_output.shape)\n",
        "        if self.method == 'dot':\n",
        "#             print (hidden.shape)\n",
        "#             print (encoder_output.shape)\n",
        "            energy = hidden[0].dot(encoder_output[0])\n",
        "            return energy\n",
        "        \n",
        "        elif self.method == 'general':\n",
        "            energy = torch.bmm(encoder_output.transpose(1,0), self.attn(hidden.squeeze(0)).unsqueeze(2)) \n",
        "            \n",
        "            #energy = self.attn(encoder_output)\n",
        "#             print(hidden.shape)\n",
        "#             print(energy.shape)\n",
        "            #energy = torch.mm(hidden, energy.transpose(0,1))\n",
        "            return energy\n",
        "        \n",
        "        elif self.method == 'concat':\n",
        "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
        "            #print (energy.shape)\n",
        "            energy = self.v.dot(energy[0])\n",
        "            return energy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZgDEzJZrMt1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, weights_matrix, hidden_size, output_size, n_layers=1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        #self.batch_size = BATCH_SIZE\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "\n",
        "\n",
        "        # Define layers\n",
        "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "#         self.embedding.weight.data.copy_(weights_matrix)\n",
        "#         self.embedding.weight.requires_grad = True\n",
        "        self.embedding.from_pretrained(weights_matrix, freeze=True, sparse=False)\n",
        "        \n",
        "        \n",
        "        self.gru1 = nn.GRU(self.embedding_dim, hidden_size,n_layers)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size,n_layers)\n",
        "        \n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # Choose attention model\n",
        "        if attn_model != 'none':\n",
        "            self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step at a time\n",
        "\n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        \n",
        "        self.batch_size=input_seq.size(0)\n",
        "        #print (self.batch_size)\n",
        "        embedded = self.embedding(input_seq) # dim = Batch_Size x embedding_dim\n",
        "        embedded = embedded.view(1, self.batch_size, self.embedding_dim) # S=1 x Batch_Size x embedding_dim\n",
        "\n",
        "        # Get current hidden state from input word and last hidden state\n",
        "        # rnn_output : [1 x batch_size x hidden_size]\n",
        "        # hidden: [layer x batch_size x hidden_size]\n",
        "#         print (embedded.shape)\n",
        "#         print (last_hidden.shape)\n",
        "        rnn_output, hidden = self.gru1(embedded, last_hidden)\n",
        "        \n",
        "        # Calculate attention from current RNN state and all encoder outputs;\n",
        "        # apply to encoder outputs to get weighted average\n",
        "        context, attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "\n",
        "        \n",
        "#         attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "#         context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
        "\n",
        "        # Attentional vector using the RNN hidden state and context vector\n",
        "        # concatenated together (Luong eq. 5)\n",
        "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "\n",
        "        # Finally predict next token (Luong eq. 6, without softmax)\n",
        "        output = self.out(concat_output)\n",
        "\n",
        "        #Return final output, hidden state, and attention weights (for visualization)\n",
        "        return output, hidden, attn_weights\n",
        "        #return attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yNRYMHc10pah",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def masked_cross_entropy(logits, target, length):\n",
        "    length = Variable(torch.LongTensor(length)).to(device)\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "    \"\"\"\n",
        "    \n",
        "    # logits_flat: (batch * max_len, num_classes)\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # log_probs_flat: (batch * max_len, num_classes)\n",
        "    log_probs_flat = F.log_softmax(logits_flat,dim=1)\n",
        "    \n",
        "    # target_flat: (batch * max_len, 1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    \n",
        "    # losses_flat: (batch * max_len, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    # losses: (batch, max_len)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    # mask: (batch, max_len)\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / length.float().sum()\n",
        "    return loss\n",
        "  \n",
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    #print(sequence_length)\n",
        "    batch_size = len(sequence_length)\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    #batch_size = BATCH_SIZE\n",
        "    \n",
        "    seq_range = torch.arange(0, max_len).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
        "    seq_range_expand = Variable(seq_range_expand)\n",
        "    \n",
        "    seq_range_expand = seq_range_expand.to(device)\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    return seq_range_expand < seq_length_expand"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vMyo8aCY3h34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Real"
      ]
    },
    {
      "metadata": {
        "id": "qiFD_lNeKc9z",
        "colab_type": "code",
        "outputId": "e9ac2865-610a-44cb-8050-d126b836266d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34034
        }
      },
      "cell_type": "code",
      "source": [
        "##Val\n",
        "\n",
        "# Configure models\n",
        "attn_model = 'general'\n",
        "hidden_size = 256\n",
        "layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "start_epoch=0\n",
        "teacher_forcing_ratio = 0.5\n",
        "learning_rate = 0.0005\n",
        "#decoder_learning_ratio = 5.0\n",
        "n_iters = 100000\n",
        "#def __init__(self, attn_model, weights_matrix, hidden_size, output_size, n_layers=1):\n",
        "# def __init__(self, weights_matrix, input_size, hidden_size,n_layers=1):\n",
        "# Initialize models\n",
        "encoder = EncoderRNN(weights_matrix_zh_val, val_input_lang.n_words, hidden_size, n_layers = layers).to(device)\n",
        "#decoder = DecoderRNN(weights_matrix_eng, hidden_size, train_output_lang.n_words, n_layers = layers).to(device)\n",
        "decoder= LuongAttnDecoderRNN(attn_model, weights_matrix_eng_val, hidden_size, val_output_lang.n_words,n_layers = layers).to(device)\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# learning_rate = 0.0001\n",
        "# decoder_learning_ratio = 2.0\n",
        "loss_list=[]\n",
        "#learning_rate=learning_rate*0.5\n",
        "# encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "# decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "# encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "# decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "trainIters(start_epoch, criterion, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iters, val_loader, loss_list, print_every=10, plot_every=1000000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0\n",
            "Loss: 0m 0s (- 777m 57s) (1 0%) 7.9641\n",
            "Iteration: 1\n",
            "Loss: 0m 0s (- 769m 12s) (2 0%) 7.9167\n",
            "Iteration: 2\n",
            "Loss: 0m 1s (- 769m 1s) (3 0%) 7.7977\n",
            "Iteration: 3\n",
            "Loss: 0m 1s (- 756m 59s) (4 0%) 7.7784\n",
            "Iteration: 4\n",
            "Loss: 0m 2s (- 756m 39s) (5 0%) 7.5290\n",
            "Iteration: 5\n",
            "Loss: 0m 2s (- 752m 8s) (6 0%) 7.4764\n",
            "Iteration: 6\n",
            "Loss: 0m 3s (- 755m 5s) (7 0%) 7.2631\n",
            "Iteration: 7\n",
            "Loss: 0m 3s (- 756m 39s) (8 0%) 7.0663\n",
            "Iteration: 8\n",
            "Loss: 0m 4s (- 767m 39s) (9 0%) 6.9366\n",
            "Iteration: 9\n",
            "Loss: 0m 4s (- 775m 4s) (10 0%) 6.7604\n",
            "Average Loss: 0m 4s (- 775m 9s) (10 0%) 7.4489\n",
            "Iteration: 10\n",
            "Loss: 0m 5s (- 822m 32s) (11 0%) 6.6655\n",
            "Iteration: 11\n",
            "Loss: 0m 5s (- 814m 33s) (12 0%) 6.5762\n",
            "Iteration: 12\n",
            "Loss: 0m 6s (- 811m 9s) (13 0%) 6.5510\n",
            "Iteration: 13\n",
            "Loss: 0m 6s (- 808m 4s) (14 0%) 6.4295\n",
            "Iteration: 14\n",
            "Loss: 0m 7s (- 803m 34s) (15 0%) 6.3370\n",
            "Iteration: 15\n",
            "Loss: 0m 7s (- 801m 36s) (16 0%) 6.3294\n",
            "Iteration: 16\n",
            "Loss: 0m 8s (- 798m 51s) (17 0%) 6.2711\n",
            "Iteration: 17\n",
            "Loss: 0m 8s (- 796m 55s) (18 0%) 6.1075\n",
            "Iteration: 18\n",
            "Loss: 0m 9s (- 793m 51s) (19 0%) 6.2003\n",
            "Iteration: 19\n",
            "Loss: 0m 9s (- 791m 7s) (20 0%) 6.1415\n",
            "loss_change:  1.0879787445068354\n",
            "Average Loss: 0m 9s (- 791m 13s) (20 0%) 6.3609\n",
            "Iteration: 20\n",
            "Loss: 0m 10s (- 814m 44s) (21 0%) 6.1535\n",
            "Iteration: 21\n",
            "Loss: 0m 10s (- 813m 19s) (22 0%) 6.0488\n",
            "Iteration: 22\n",
            "Loss: 0m 11s (- 810m 57s) (23 0%) 5.9437\n",
            "Iteration: 23\n",
            "Loss: 0m 11s (- 808m 54s) (24 0%) 5.9819\n",
            "Iteration: 24\n",
            "Loss: 0m 12s (- 805m 34s) (25 0%) 6.0755\n",
            "Iteration: 25\n",
            "Loss: 0m 12s (- 802m 52s) (26 0%) 6.1025\n",
            "Iteration: 26\n",
            "Loss: 0m 12s (- 800m 1s) (27 0%) 6.1626\n",
            "Iteration: 27\n",
            "Loss: 0m 13s (- 798m 6s) (28 0%) 6.0656\n",
            "Iteration: 28\n",
            "Loss: 0m 13s (- 796m 46s) (29 0%) 6.2086\n",
            "Iteration: 29\n",
            "Loss: 0m 14s (- 795m 6s) (30 0%) 6.2283\n",
            "loss_change:  0.26380395889282227\n",
            "Average Loss: 0m 14s (- 795m 8s) (30 0%) 6.0971\n",
            "Iteration: 30\n",
            "Loss: 0m 15s (- 813m 22s) (31 0%) 6.3414\n",
            "Iteration: 31\n",
            "Loss: 0m 15s (- 813m 8s) (32 0%) 6.2410\n",
            "Iteration: 32\n",
            "Loss: 0m 16s (- 815m 1s) (33 0%) 6.3382\n",
            "Iteration: 33\n",
            "Loss: 0m 16s (- 815m 28s) (34 0%) 6.4881\n",
            "Iteration: 34\n",
            "Loss: 0m 17s (- 813m 24s) (35 0%) 6.4110\n",
            "Iteration: 35\n",
            "Loss: 0m 17s (- 812m 23s) (36 0%) 6.4781\n",
            "Iteration: 36\n",
            "Loss: 0m 18s (- 811m 18s) (37 0%) 6.3754\n",
            "Iteration: 37\n",
            "Loss: 0m 18s (- 810m 15s) (38 0%) 6.6795\n",
            "Iteration: 38\n",
            "Loss: 0m 18s (- 809m 22s) (39 0%) 6.6031\n",
            "Iteration: 39\n",
            "Loss: 0m 19s (- 809m 44s) (40 0%) 6.6240\n",
            "loss_change:  -0.3608846664428711\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 0.00025\n",
            "Current Decoder Learning Rate: 0.00025\n",
            "Average Loss: 0m 19s (- 809m 47s) (40 0%) 6.4580\n",
            "Iteration: 40\n",
            "Loss: 0m 20s (- 825m 3s) (41 0%) 6.8899\n",
            "Iteration: 41\n",
            "Loss: 0m 20s (- 826m 51s) (42 0%) 6.8266\n",
            "Iteration: 42\n",
            "Loss: 0m 21s (- 826m 49s) (43 0%) 6.7622\n",
            "Iteration: 43\n",
            "Loss: 0m 21s (- 825m 45s) (44 0%) 6.6475\n",
            "Iteration: 44\n",
            "Loss: 0m 22s (- 825m 47s) (45 0%) 6.7783\n",
            "Iteration: 45\n",
            "Loss: 0m 22s (- 825m 13s) (46 0%) 6.9330\n",
            "Iteration: 46\n",
            "Loss: 0m 23s (- 823m 37s) (47 0%) 6.7741\n",
            "Iteration: 47\n",
            "Loss: 0m 23s (- 821m 41s) (48 0%) 6.9355\n",
            "Iteration: 48\n",
            "Loss: 0m 24s (- 820m 5s) (49 0%) 6.9905\n",
            "Iteration: 49\n",
            "Loss: 0m 24s (- 818m 22s) (50 0%) 6.8453\n",
            "loss_change:  -0.38030204772949183\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 0.000125\n",
            "Current Decoder Learning Rate: 0.000125\n",
            "Average Loss: 0m 24s (- 818m 23s) (50 0%) 6.8383\n",
            "Iteration: 50\n",
            "Loss: 0m 25s (- 829m 20s) (51 0%) 6.8988\n",
            "Iteration: 51\n",
            "Loss: 0m 25s (- 827m 37s) (52 0%) 6.8457\n",
            "Iteration: 52\n",
            "Loss: 0m 26s (- 826m 8s) (53 0%) 6.6907\n",
            "Iteration: 53\n",
            "Loss: 0m 26s (- 824m 45s) (54 0%) 6.5719\n",
            "Iteration: 54\n",
            "Loss: 0m 27s (- 822m 56s) (55 0%) 6.6744\n",
            "Iteration: 55\n",
            "Loss: 0m 27s (- 821m 53s) (56 0%) 6.9440\n",
            "Iteration: 56\n",
            "Loss: 0m 28s (- 820m 14s) (57 0%) 6.8919\n",
            "Iteration: 57\n",
            "Loss: 0m 28s (- 819m 8s) (58 0%) 6.8330\n",
            "Iteration: 58\n",
            "Loss: 0m 28s (- 817m 57s) (59 0%) 6.7507\n",
            "Iteration: 59\n",
            "Loss: 0m 29s (- 816m 47s) (60 0%) 6.8700\n",
            "loss_change:  0.041176986694336115\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 6.25e-05\n",
            "Current Decoder Learning Rate: 6.25e-05\n",
            "Average Loss: 0m 29s (- 816m 49s) (60 0%) 6.7971\n",
            "Iteration: 60\n",
            "Loss: 0m 30s (- 826m 9s) (61 0%) 6.8955\n",
            "Iteration: 61\n",
            "Loss: 0m 30s (- 824m 58s) (62 0%) 6.7539\n",
            "Iteration: 62\n",
            "Loss: 0m 31s (- 824m 37s) (63 0%) 6.7488\n",
            "Iteration: 63\n",
            "Loss: 0m 31s (- 824m 34s) (64 0%) 6.8410\n",
            "Iteration: 64\n",
            "Loss: 0m 32s (- 823m 47s) (65 0%) 6.6295\n",
            "Iteration: 65\n",
            "Loss: 0m 32s (- 823m 39s) (66 0%) 6.6520\n",
            "Iteration: 66\n",
            "Loss: 0m 33s (- 822m 57s) (67 0%) 6.6960\n",
            "Iteration: 67\n",
            "Loss: 0m 33s (- 821m 23s) (68 0%) 6.6763\n",
            "Iteration: 68\n",
            "Loss: 0m 33s (- 820m 12s) (69 0%) 6.7714\n",
            "Iteration: 69\n",
            "Loss: 0m 34s (- 819m 2s) (70 0%) 6.5915\n",
            "loss_change:  0.07151761054992622\n",
            "Average Loss: 0m 34s (- 819m 3s) (70 0%) 6.7256\n",
            "Iteration: 70\n",
            "Loss: 0m 35s (- 828m 2s) (71 0%) 6.6491\n",
            "Iteration: 71\n",
            "Loss: 0m 35s (- 828m 59s) (72 0%) 6.8058\n",
            "Iteration: 72\n",
            "Loss: 0m 36s (- 828m 37s) (73 0%) 6.7675\n",
            "Iteration: 73\n",
            "Loss: 0m 36s (- 829m 10s) (74 0%) 6.4587\n",
            "Iteration: 74\n",
            "Loss: 0m 37s (- 829m 30s) (75 0%) 6.5345\n",
            "Iteration: 75\n",
            "Loss: 0m 37s (- 830m 2s) (76 0%) 6.6872\n",
            "Iteration: 76\n",
            "Loss: 0m 38s (- 830m 29s) (77 0%) 6.4660\n",
            "Iteration: 77\n",
            "Loss: 0m 38s (- 829m 24s) (78 0%) 6.6133\n",
            "Iteration: 78\n",
            "Loss: 0m 39s (- 828m 2s) (79 0%) 6.4455\n",
            "Iteration: 79\n",
            "Loss: 0m 39s (- 826m 56s) (80 0%) 6.5333\n",
            "loss_change:  0.12949242591857946\n",
            "Average Loss: 0m 39s (- 826m 57s) (80 0%) 6.5961\n",
            "Iteration: 80\n",
            "Loss: 0m 40s (- 834m 24s) (81 0%) 6.5009\n",
            "Iteration: 81\n",
            "Loss: 0m 41s (- 834m 45s) (82 0%) 6.4027\n",
            "Iteration: 82\n",
            "Loss: 0m 41s (- 833m 51s) (83 0%) 6.4278\n",
            "Iteration: 83\n",
            "Loss: 0m 42s (- 833m 57s) (84 0%) 6.5111\n",
            "Iteration: 84\n",
            "Loss: 0m 42s (- 833m 31s) (85 0%) 6.4478\n",
            "Iteration: 85\n",
            "Loss: 0m 43s (- 833m 20s) (86 0%) 6.4277\n",
            "Iteration: 86\n",
            "Loss: 0m 43s (- 832m 51s) (87 0%) 6.4506\n",
            "Iteration: 87\n",
            "Loss: 0m 43s (- 831m 34s) (88 0%) 6.5143\n",
            "Iteration: 88\n",
            "Loss: 0m 44s (- 830m 31s) (89 0%) 6.4965\n",
            "Iteration: 89\n",
            "Loss: 0m 44s (- 829m 36s) (90 0%) 6.2894\n",
            "loss_change:  0.14920673370361293\n",
            "Average Loss: 0m 44s (- 829m 37s) (90 0%) 6.4469\n",
            "Iteration: 90\n",
            "Loss: 0m 45s (- 835m 35s) (91 0%) 6.5621\n",
            "Iteration: 91\n",
            "Loss: 0m 46s (- 835m 23s) (92 0%) 6.4154\n",
            "Iteration: 92\n",
            "Loss: 0m 46s (- 834m 26s) (93 0%) 6.4206\n",
            "Iteration: 93\n",
            "Loss: 0m 47s (- 833m 54s) (94 0%) 6.3743\n",
            "Iteration: 94\n",
            "Loss: 0m 47s (- 833m 26s) (95 0%) 6.3643\n",
            "Iteration: 95\n",
            "Loss: 0m 48s (- 833m 0s) (96 0%) 6.2913\n",
            "Iteration: 96\n",
            "Loss: 0m 48s (- 832m 35s) (97 0%) 6.3852\n",
            "Iteration: 97\n",
            "Loss: 0m 48s (- 832m 29s) (98 0%) 6.3764\n",
            "Iteration: 98\n",
            "Loss: 0m 49s (- 831m 37s) (99 0%) 6.3729\n",
            "Iteration: 99\n",
            "Loss: 0m 49s (- 830m 34s) (100 0%) 6.3351\n",
            "loss_change:  0.05714483261108416\n",
            "Average Loss: 0m 49s (- 830m 35s) (100 0%) 6.3898\n",
            "Iteration: 100\n",
            "Loss: 0m 50s (- 835m 11s) (101 0%) 6.2323\n",
            "Iteration: 101\n",
            "Loss: 0m 51s (- 834m 36s) (102 0%) 6.2104\n",
            "Iteration: 102\n",
            "Loss: 0m 51s (- 834m 21s) (103 0%) 6.3423\n",
            "Iteration: 103\n",
            "Loss: 0m 52s (- 833m 30s) (104 0%) 6.1615\n",
            "Iteration: 104\n",
            "Loss: 0m 52s (- 833m 3s) (105 0%) 6.2954\n",
            "Iteration: 105\n",
            "Loss: 0m 53s (- 832m 55s) (106 0%) 6.4122\n",
            "Iteration: 106\n",
            "Loss: 0m 53s (- 832m 40s) (107 0%) 6.2556\n",
            "Iteration: 107\n",
            "Loss: 0m 53s (- 832m 24s) (108 0%) 6.2148\n",
            "Iteration: 108\n",
            "Loss: 0m 54s (- 832m 7s) (109 0%) 6.2730\n",
            "Iteration: 109\n",
            "Loss: 0m 54s (- 831m 15s) (110 0%) 6.3371\n",
            "loss_change:  0.11629600524902362\n",
            "Average Loss: 0m 54s (- 831m 15s) (110 0%) 6.2735\n",
            "Iteration: 110\n",
            "Loss: 0m 55s (- 835m 49s) (111 0%) 6.2711\n",
            "Iteration: 111\n",
            "Loss: 0m 56s (- 835m 2s) (112 0%) 6.1743\n",
            "Iteration: 112\n",
            "Loss: 0m 56s (- 834m 32s) (113 0%) 6.2405\n",
            "Iteration: 113\n",
            "Loss: 0m 57s (- 834m 17s) (114 0%) 6.5348\n",
            "Iteration: 114\n",
            "Loss: 0m 57s (- 833m 39s) (115 0%) 6.1235\n",
            "Iteration: 115\n",
            "Loss: 0m 58s (- 833m 22s) (116 0%) 6.1699\n",
            "Iteration: 116\n",
            "Loss: 0m 58s (- 832m 53s) (117 0%) 6.1514\n",
            "Iteration: 117\n",
            "Loss: 0m 59s (- 832m 23s) (118 0%) 6.2944\n",
            "Iteration: 118\n",
            "Loss: 0m 59s (- 832m 7s) (119 0%) 6.4505\n",
            "Iteration: 119\n",
            "Loss: 0m 59s (- 831m 46s) (120 0%) 6.3453\n",
            "loss_change:  -0.002122163772583008\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.125e-05\n",
            "Current Decoder Learning Rate: 3.125e-05\n",
            "Average Loss: 0m 59s (- 831m 47s) (120 0%) 6.2756\n",
            "Iteration: 120\n",
            "Loss: 1m 0s (- 836m 5s) (121 0%) 6.1951\n",
            "Iteration: 121\n",
            "Loss: 1m 1s (- 835m 14s) (122 0%) 6.2493\n",
            "Iteration: 122\n",
            "Loss: 1m 1s (- 834m 32s) (123 0%) 6.1823\n",
            "Iteration: 123\n",
            "Loss: 1m 2s (- 833m 58s) (124 0%) 6.2227\n",
            "Iteration: 124\n",
            "Loss: 1m 2s (- 833m 53s) (125 0%) 6.2202\n",
            "Iteration: 125\n",
            "Loss: 1m 3s (- 833m 40s) (126 0%) 6.2351\n",
            "Iteration: 126\n",
            "Loss: 1m 3s (- 833m 29s) (127 0%) 6.3970\n",
            "Iteration: 127\n",
            "Loss: 1m 4s (- 833m 20s) (128 0%) 6.3327\n",
            "Iteration: 128\n",
            "Loss: 1m 4s (- 832m 49s) (129 0%) 6.1432\n",
            "Iteration: 129\n",
            "Loss: 1m 5s (- 832m 28s) (130 0%) 6.2631\n",
            "loss_change:  0.03150854110717738\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.5625e-05\n",
            "Current Decoder Learning Rate: 1.5625e-05\n",
            "Average Loss: 1m 5s (- 832m 29s) (130 0%) 6.2441\n",
            "Iteration: 130\n",
            "Loss: 1m 5s (- 836m 6s) (131 0%) 6.2630\n",
            "Iteration: 131\n",
            "Loss: 1m 6s (- 835m 28s) (132 0%) 6.0967\n",
            "Iteration: 132\n",
            "Loss: 1m 6s (- 835m 4s) (133 0%) 6.3150\n",
            "Iteration: 133\n",
            "Loss: 1m 7s (- 834m 36s) (134 0%) 6.2793\n",
            "Iteration: 134\n",
            "Loss: 1m 7s (- 834m 15s) (135 0%) 6.1366\n",
            "Iteration: 135\n",
            "Loss: 1m 8s (- 834m 17s) (136 0%) 6.1547\n",
            "Iteration: 136\n",
            "Loss: 1m 8s (- 834m 2s) (137 0%) 6.2750\n",
            "Iteration: 137\n",
            "Loss: 1m 9s (- 834m 0s) (138 0%) 6.3663\n",
            "Iteration: 138\n",
            "Loss: 1m 9s (- 833m 24s) (139 0%) 6.2681\n",
            "Iteration: 139\n",
            "Loss: 1m 10s (- 832m 51s) (140 0%) 6.0645\n",
            "loss_change:  0.022155618667603072\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 7.8125e-06\n",
            "Current Decoder Learning Rate: 7.8125e-06\n",
            "Average Loss: 1m 10s (- 832m 52s) (140 0%) 6.2219\n",
            "Iteration: 140\n",
            "Loss: 1m 10s (- 836m 4s) (141 0%) 6.1984\n",
            "Iteration: 141\n",
            "Loss: 1m 11s (- 835m 36s) (142 0%) 6.2165\n",
            "Iteration: 142\n",
            "Loss: 1m 11s (- 835m 31s) (143 0%) 6.2482\n",
            "Iteration: 143\n",
            "Loss: 1m 12s (- 835m 17s) (144 0%) 6.3005\n",
            "Iteration: 144\n",
            "Loss: 1m 12s (- 835m 10s) (145 0%) 6.1428\n",
            "Iteration: 145\n",
            "Loss: 1m 13s (- 835m 4s) (146 0%) 6.2200\n",
            "Iteration: 146\n",
            "Loss: 1m 13s (- 834m 41s) (147 0%) 6.1486\n",
            "Iteration: 147\n",
            "Loss: 1m 14s (- 834m 40s) (148 0%) 6.3045\n",
            "Iteration: 148\n",
            "Loss: 1m 14s (- 833m 57s) (149 0%) 6.2953\n",
            "Iteration: 149\n",
            "Loss: 1m 15s (- 833m 25s) (150 0%) 6.0979\n",
            "loss_change:  0.004644155502319336\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.90625e-06\n",
            "Current Decoder Learning Rate: 3.90625e-06\n",
            "Average Loss: 1m 15s (- 833m 26s) (150 0%) 6.2173\n",
            "Iteration: 150\n",
            "Loss: 1m 15s (- 836m 20s) (151 0%) 6.2452\n",
            "Iteration: 151\n",
            "Loss: 1m 16s (- 836m 7s) (152 0%) 6.1016\n",
            "Iteration: 152\n",
            "Loss: 1m 16s (- 835m 57s) (153 0%) 6.3142\n",
            "Iteration: 153\n",
            "Loss: 1m 17s (- 835m 44s) (154 0%) 6.2585\n",
            "Iteration: 154\n",
            "Loss: 1m 17s (- 835m 29s) (155 0%) 6.0310\n",
            "Iteration: 155\n",
            "Loss: 1m 18s (- 835m 30s) (156 0%) 6.3321\n",
            "Iteration: 156\n",
            "Loss: 1m 18s (- 835m 18s) (157 0%) 6.0496\n",
            "Iteration: 157\n",
            "Loss: 1m 19s (- 835m 6s) (158 0%) 6.3204\n",
            "Iteration: 158\n",
            "Loss: 1m 19s (- 834m 33s) (159 0%) 6.3275\n",
            "Iteration: 159\n",
            "Loss: 1m 20s (- 833m 53s) (160 0%) 6.1505\n",
            "loss_change:  0.004215192794799094\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.953125e-06\n",
            "Current Decoder Learning Rate: 1.953125e-06\n",
            "Average Loss: 1m 20s (- 833m 53s) (160 0%) 6.2131\n",
            "Iteration: 160\n",
            "Loss: 1m 20s (- 837m 6s) (161 0%) 6.2228\n",
            "Iteration: 161\n",
            "Loss: 1m 21s (- 837m 0s) (162 0%) 6.1578\n",
            "Iteration: 162\n",
            "Loss: 1m 21s (- 836m 34s) (163 0%) 6.1545\n",
            "Iteration: 163\n",
            "Loss: 1m 22s (- 836m 11s) (164 0%) 6.1742\n",
            "Iteration: 164\n",
            "Loss: 1m 22s (- 835m 50s) (165 0%) 6.2250\n",
            "Iteration: 165\n",
            "Loss: 1m 23s (- 835m 30s) (166 0%) 6.1318\n",
            "Iteration: 166\n",
            "Loss: 1m 23s (- 835m 15s) (167 0%) 6.1200\n",
            "Iteration: 167\n",
            "Loss: 1m 24s (- 835m 1s) (168 0%) 6.4061\n",
            "Iteration: 168\n",
            "Loss: 1m 24s (- 834m 49s) (169 0%) 6.3222\n",
            "Iteration: 169\n",
            "Loss: 1m 25s (- 834m 17s) (170 0%) 6.2846\n",
            "loss_change:  -0.006842041015624467\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 9.765625e-07\n",
            "Current Decoder Learning Rate: 9.765625e-07\n",
            "Average Loss: 1m 25s (- 834m 17s) (170 0%) 6.2199\n",
            "Iteration: 170\n",
            "Loss: 1m 26s (- 837m 1s) (171 0%) 6.2274\n",
            "Iteration: 171\n",
            "Loss: 1m 26s (- 836m 34s) (172 0%) 6.3015\n",
            "Iteration: 172\n",
            "Loss: 1m 26s (- 836m 25s) (173 0%) 6.1772\n",
            "Iteration: 173\n",
            "Loss: 1m 27s (- 835m 48s) (174 0%) 6.1305\n",
            "Iteration: 174\n",
            "Loss: 1m 27s (- 835m 36s) (175 0%) 6.2885\n",
            "Iteration: 175\n",
            "Loss: 1m 28s (- 835m 21s) (176 0%) 6.1047\n",
            "Iteration: 176\n",
            "Loss: 1m 28s (- 835m 12s) (177 0%) 6.1764\n",
            "Iteration: 177\n",
            "Loss: 1m 29s (- 835m 0s) (178 0%) 6.3421\n",
            "Iteration: 178\n",
            "Loss: 1m 29s (- 834m 42s) (179 0%) 6.2538\n",
            "Iteration: 179\n",
            "Loss: 1m 30s (- 834m 11s) (180 0%) 6.2556\n",
            "loss_change:  -0.005887508392333984\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 4.8828125e-07\n",
            "Current Decoder Learning Rate: 4.8828125e-07\n",
            "Average Loss: 1m 30s (- 834m 12s) (180 0%) 6.2258\n",
            "Iteration: 180\n",
            "Loss: 1m 31s (- 837m 2s) (181 0%) 6.2400\n",
            "Iteration: 181\n",
            "Loss: 1m 31s (- 836m 34s) (182 0%) 6.2915\n",
            "Iteration: 182\n",
            "Loss: 1m 32s (- 836m 21s) (183 0%) 6.1884\n",
            "Iteration: 183\n",
            "Loss: 1m 32s (- 835m 52s) (184 0%) 6.0799\n",
            "Iteration: 184\n",
            "Loss: 1m 32s (- 835m 38s) (185 0%) 6.2047\n",
            "Iteration: 185\n",
            "Loss: 1m 33s (- 835m 33s) (186 0%) 6.2481\n",
            "Iteration: 186\n",
            "Loss: 1m 33s (- 835m 14s) (187 0%) 6.0845\n",
            "Iteration: 187\n",
            "Loss: 1m 34s (- 834m 58s) (188 0%) 6.2216\n",
            "Iteration: 188\n",
            "Loss: 1m 34s (- 834m 33s) (189 0%) 6.2143\n",
            "Iteration: 189\n",
            "Loss: 1m 35s (- 834m 0s) (190 0%) 6.2788\n",
            "loss_change:  0.02060308456420845\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.44140625e-07\n",
            "Current Decoder Learning Rate: 2.44140625e-07\n",
            "Average Loss: 1m 35s (- 834m 0s) (190 0%) 6.2052\n",
            "Iteration: 190\n",
            "Loss: 1m 36s (- 836m 37s) (191 0%) 6.3433\n",
            "Iteration: 191\n",
            "Loss: 1m 36s (- 836m 15s) (192 0%) 6.1722\n",
            "Iteration: 192\n",
            "Loss: 1m 37s (- 836m 13s) (193 0%) 6.2919\n",
            "Iteration: 193\n",
            "Loss: 1m 37s (- 835m 44s) (194 0%) 6.1133\n",
            "Iteration: 194\n",
            "Loss: 1m 37s (- 835m 32s) (195 0%) 6.2620\n",
            "Iteration: 195\n",
            "Loss: 1m 38s (- 835m 17s) (196 0%) 6.2394\n",
            "Iteration: 196\n",
            "Loss: 1m 38s (- 834m 55s) (197 0%) 6.0981\n",
            "Iteration: 197\n",
            "Loss: 1m 39s (- 834m 36s) (198 0%) 6.1546\n",
            "Iteration: 198\n",
            "Loss: 1m 39s (- 834m 17s) (199 0%) 6.2242\n",
            "Iteration: 199\n",
            "Loss: 1m 40s (- 833m 52s) (200 0%) 6.2194\n",
            "loss_change:  -0.0066378116607666016\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.220703125e-07\n",
            "Current Decoder Learning Rate: 1.220703125e-07\n",
            "Average Loss: 1m 40s (- 833m 53s) (200 0%) 6.2118\n",
            "Iteration: 200\n",
            "Loss: 1m 41s (- 836m 37s) (201 0%) 6.1045\n",
            "Iteration: 201\n",
            "Loss: 1m 41s (- 836m 11s) (202 0%) 6.2968\n",
            "Iteration: 202\n",
            "Loss: 1m 42s (- 835m 54s) (203 0%) 6.2578\n",
            "Iteration: 203\n",
            "Loss: 1m 42s (- 835m 41s) (204 0%) 6.0602\n",
            "Iteration: 204\n",
            "Loss: 1m 42s (- 835m 26s) (205 0%) 6.2871\n",
            "Iteration: 205\n",
            "Loss: 1m 43s (- 835m 16s) (206 0%) 6.2815\n",
            "Iteration: 206\n",
            "Loss: 1m 43s (- 835m 3s) (207 0%) 6.1884\n",
            "Iteration: 207\n",
            "Loss: 1m 44s (- 834m 56s) (208 0%) 6.2040\n",
            "Iteration: 208\n",
            "Loss: 1m 44s (- 834m 40s) (209 0%) 6.0613\n",
            "Iteration: 209\n",
            "Loss: 1m 45s (- 834m 25s) (210 0%) 6.2521\n",
            "loss_change:  0.012430381774902521\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 6.103515625e-08\n",
            "Current Decoder Learning Rate: 6.103515625e-08\n",
            "Average Loss: 1m 45s (- 834m 26s) (210 0%) 6.1994\n",
            "Iteration: 210\n",
            "Loss: 1m 46s (- 837m 17s) (211 0%) 6.1698\n",
            "Iteration: 211\n",
            "Loss: 1m 46s (- 836m 51s) (212 0%) 6.1557\n",
            "Iteration: 212\n",
            "Loss: 1m 47s (- 836m 30s) (213 0%) 6.2330\n",
            "Iteration: 213\n",
            "Loss: 1m 47s (- 836m 6s) (214 0%) 6.2114\n",
            "Iteration: 214\n",
            "Loss: 1m 48s (- 835m 58s) (215 0%) 6.2594\n",
            "Iteration: 215\n",
            "Loss: 1m 48s (- 835m 40s) (216 0%) 6.2534\n",
            "Iteration: 216\n",
            "Loss: 1m 49s (- 835m 32s) (217 0%) 6.2978\n",
            "Iteration: 217\n",
            "Loss: 1m 49s (- 835m 36s) (218 0%) 6.1778\n",
            "Iteration: 218\n",
            "Loss: 1m 50s (- 835m 27s) (219 0%) 6.3107\n",
            "Iteration: 219\n",
            "Loss: 1m 50s (- 835m 15s) (220 0%) 6.1676\n",
            "loss_change:  -0.02425675392150861\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.0517578125e-08\n",
            "Current Decoder Learning Rate: 3.0517578125e-08\n",
            "Average Loss: 1m 50s (- 835m 16s) (220 0%) 6.2236\n",
            "Iteration: 220\n",
            "Loss: 1m 51s (- 837m 31s) (221 0%) 6.0424\n",
            "Iteration: 221\n",
            "Loss: 1m 51s (- 837m 8s) (222 0%) 6.1609\n",
            "Iteration: 222\n",
            "Loss: 1m 52s (- 836m 41s) (223 0%) 6.2487\n",
            "Iteration: 223\n",
            "Loss: 1m 52s (- 836m 32s) (224 0%) 6.3147\n",
            "Iteration: 224\n",
            "Loss: 1m 53s (- 836m 30s) (225 0%) 6.2796\n",
            "Iteration: 225\n",
            "Loss: 1m 53s (- 836m 8s) (226 0%) 6.2239\n",
            "Iteration: 226\n",
            "Loss: 1m 54s (- 835m 54s) (227 0%) 6.1693\n",
            "Iteration: 227\n",
            "Loss: 1m 54s (- 835m 53s) (228 0%) 6.1625\n",
            "Iteration: 228\n",
            "Loss: 1m 55s (- 835m 44s) (229 0%) 6.1950\n",
            "Iteration: 229\n",
            "Loss: 1m 55s (- 835m 32s) (230 0%) 6.2278\n",
            "loss_change:  0.02116680145263672\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.52587890625e-08\n",
            "Current Decoder Learning Rate: 1.52587890625e-08\n",
            "Average Loss: 1m 55s (- 835m 34s) (230 0%) 6.2025\n",
            "Iteration: 230\n",
            "Loss: 1m 56s (- 837m 57s) (231 0%) 6.1826\n",
            "Iteration: 231\n",
            "Loss: 1m 56s (- 837m 35s) (232 0%) 6.2286\n",
            "Iteration: 232\n",
            "Loss: 1m 57s (- 837m 10s) (233 0%) 6.0491\n",
            "Iteration: 233\n",
            "Loss: 1m 57s (- 836m 55s) (234 0%) 6.1768\n",
            "Iteration: 234\n",
            "Loss: 1m 58s (- 836m 49s) (235 0%) 6.2604\n",
            "Iteration: 235\n",
            "Loss: 1m 58s (- 836m 28s) (236 0%) 6.2891\n",
            "Iteration: 236\n",
            "Loss: 1m 59s (- 836m 17s) (237 0%) 6.2663\n",
            "Iteration: 237\n",
            "Loss: 1m 59s (- 836m 14s) (238 0%) 6.1513\n",
            "Iteration: 238\n",
            "Loss: 2m 0s (- 836m 3s) (239 0%) 6.1893\n",
            "Iteration: 239\n",
            "Loss: 2m 0s (- 836m 0s) (240 0%) 6.2063\n",
            "loss_change:  0.002495670318603871\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 7.62939453125e-09\n",
            "Current Decoder Learning Rate: 7.62939453125e-09\n",
            "Average Loss: 2m 0s (- 836m 0s) (240 0%) 6.2000\n",
            "Iteration: 240\n",
            "Loss: 2m 1s (- 838m 33s) (241 0%) 6.3871\n",
            "Iteration: 241\n",
            "Loss: 2m 2s (- 838m 13s) (242 0%) 6.3007\n",
            "Iteration: 242\n",
            "Loss: 2m 2s (- 837m 51s) (243 0%) 6.0495\n",
            "Iteration: 243\n",
            "Loss: 2m 2s (- 837m 32s) (244 0%) 6.1374\n",
            "Iteration: 244\n",
            "Loss: 2m 3s (- 837m 9s) (245 0%) 6.2959\n",
            "Iteration: 245\n",
            "Loss: 2m 3s (- 837m 2s) (246 0%) 6.0983\n",
            "Iteration: 246\n",
            "Loss: 2m 4s (- 836m 49s) (247 0%) 6.1887\n",
            "Iteration: 247\n",
            "Loss: 2m 4s (- 836m 44s) (248 0%) 6.1896\n",
            "Iteration: 248\n",
            "Loss: 2m 5s (- 836m 34s) (249 0%) 6.3124\n",
            "Iteration: 249\n",
            "Loss: 2m 5s (- 836m 24s) (250 0%) 6.2018\n",
            "loss_change:  -0.016168451309204634\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.814697265625e-09\n",
            "Current Decoder Learning Rate: 3.814697265625e-09\n",
            "Average Loss: 2m 5s (- 836m 25s) (250 0%) 6.2162\n",
            "Iteration: 250\n",
            "Loss: 2m 6s (- 839m 4s) (251 0%) 6.1368\n",
            "Iteration: 251\n",
            "Loss: 2m 7s (- 838m 46s) (252 0%) 6.2913\n",
            "Iteration: 252\n",
            "Loss: 2m 7s (- 838m 20s) (253 0%) 6.1824\n",
            "Iteration: 253\n",
            "Loss: 2m 8s (- 837m 57s) (254 0%) 6.1068\n",
            "Iteration: 254\n",
            "Loss: 2m 8s (- 837m 39s) (255 0%) 6.2928\n",
            "Iteration: 255\n",
            "Loss: 2m 8s (- 837m 34s) (256 0%) 6.1965\n",
            "Iteration: 256\n",
            "Loss: 2m 9s (- 837m 17s) (257 0%) 6.3074\n",
            "Iteration: 257\n",
            "Loss: 2m 9s (- 837m 16s) (258 0%) 6.0645\n",
            "Iteration: 258\n",
            "Loss: 2m 10s (- 837m 7s) (259 0%) 6.1369\n",
            "Iteration: 259\n",
            "Loss: 2m 10s (- 836m 55s) (260 0%) 6.1760\n",
            "loss_change:  0.027008438110351918\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.9073486328125e-09\n",
            "Current Decoder Learning Rate: 1.9073486328125e-09\n",
            "Average Loss: 2m 10s (- 836m 56s) (260 0%) 6.1891\n",
            "Iteration: 260\n",
            "Loss: 2m 11s (- 839m 20s) (261 0%) 6.3775\n",
            "Iteration: 261\n",
            "Loss: 2m 12s (- 839m 5s) (262 0%) 6.3012\n",
            "Iteration: 262\n",
            "Loss: 2m 12s (- 838m 42s) (263 0%) 6.2237\n",
            "Iteration: 263\n",
            "Loss: 2m 13s (- 838m 24s) (264 0%) 6.2425\n",
            "Iteration: 264\n",
            "Loss: 2m 13s (- 838m 7s) (265 0%) 6.1830\n",
            "Iteration: 265\n",
            "Loss: 2m 14s (- 838m 4s) (266 0%) 6.2208\n",
            "Iteration: 266\n",
            "Loss: 2m 14s (- 837m 50s) (267 0%) 5.9875\n",
            "Iteration: 267\n",
            "Loss: 2m 15s (- 837m 45s) (268 0%) 6.2340\n",
            "Iteration: 268\n",
            "Loss: 2m 15s (- 837m 37s) (269 0%) 6.0730\n",
            "Iteration: 269\n",
            "Loss: 2m 16s (- 837m 30s) (270 0%) 6.2472\n",
            "loss_change:  -0.019908046722412642\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 9.5367431640625e-10\n",
            "Current Decoder Learning Rate: 9.5367431640625e-10\n",
            "Average Loss: 2m 16s (- 837m 31s) (270 0%) 6.2091\n",
            "Iteration: 270\n",
            "Loss: 2m 16s (- 839m 55s) (271 0%) 6.3090\n",
            "Iteration: 271\n",
            "Loss: 2m 17s (- 839m 32s) (272 0%) 6.2737\n",
            "Iteration: 272\n",
            "Loss: 2m 17s (- 839m 6s) (273 0%) 6.1714\n",
            "Iteration: 273\n",
            "Loss: 2m 18s (- 838m 44s) (274 0%) 6.2030\n",
            "Iteration: 274\n",
            "Loss: 2m 18s (- 838m 24s) (275 0%) 6.2474\n",
            "Iteration: 275\n",
            "Loss: 2m 19s (- 838m 17s) (276 0%) 6.2355\n",
            "Iteration: 276\n",
            "Loss: 2m 19s (- 838m 1s) (277 0%) 6.2962\n",
            "Iteration: 277\n",
            "Loss: 2m 20s (- 837m 51s) (278 0%) 6.3336\n",
            "Iteration: 278\n",
            "Loss: 2m 20s (- 837m 39s) (279 0%) 6.2523\n",
            "Iteration: 279\n",
            "Loss: 2m 21s (- 837m 21s) (280 0%) 6.0374\n",
            "loss_change:  -0.026895380020141246\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 4.76837158203125e-10\n",
            "Current Decoder Learning Rate: 4.76837158203125e-10\n",
            "Average Loss: 2m 21s (- 837m 21s) (280 0%) 6.2359\n",
            "Iteration: 280\n",
            "Loss: 2m 22s (- 839m 54s) (281 0%) 6.2116\n",
            "Iteration: 281\n",
            "Loss: 2m 22s (- 839m 48s) (282 0%) 6.2841\n",
            "Iteration: 282\n",
            "Loss: 2m 22s (- 839m 22s) (283 0%) 6.1392\n",
            "Iteration: 283\n",
            "Loss: 2m 23s (- 838m 57s) (284 0%) 6.2068\n",
            "Iteration: 284\n",
            "Loss: 2m 23s (- 838m 34s) (285 0%) 6.2154\n",
            "Iteration: 285\n",
            "Loss: 2m 24s (- 838m 12s) (286 0%) 6.2254\n",
            "Iteration: 286\n",
            "Loss: 2m 24s (- 838m 7s) (287 0%) 6.2885\n",
            "Iteration: 287\n",
            "Loss: 2m 25s (- 837m 52s) (288 0%) 6.1194\n",
            "Iteration: 288\n",
            "Loss: 2m 25s (- 837m 46s) (289 0%) 6.0983\n",
            "Iteration: 289\n",
            "Loss: 2m 26s (- 837m 36s) (290 0%) 6.1217\n",
            "loss_change:  0.044915771484374645\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.384185791015625e-10\n",
            "Current Decoder Learning Rate: 2.384185791015625e-10\n",
            "Average Loss: 2m 26s (- 837m 37s) (290 0%) 6.1910\n",
            "Iteration: 290\n",
            "Loss: 2m 27s (- 840m 0s) (291 0%) 6.2048\n",
            "Iteration: 291\n",
            "Loss: 2m 27s (- 839m 54s) (292 0%) 6.1959\n",
            "Iteration: 292\n",
            "Loss: 2m 28s (- 839m 30s) (293 0%) 6.1948\n",
            "Iteration: 293\n",
            "Loss: 2m 28s (- 839m 5s) (294 0%) 6.1405\n",
            "Iteration: 294\n",
            "Loss: 2m 28s (- 838m 45s) (295 0%) 6.1257\n",
            "Iteration: 295\n",
            "Loss: 2m 29s (- 838m 39s) (296 0%) 6.2624\n",
            "Iteration: 296\n",
            "Loss: 2m 29s (- 838m 29s) (297 0%) 6.1982\n",
            "Iteration: 297\n",
            "Loss: 2m 30s (- 838m 24s) (298 0%) 6.3297\n",
            "Iteration: 298\n",
            "Loss: 2m 30s (- 838m 14s) (299 0%) 6.2574\n",
            "Iteration: 299\n",
            "Loss: 2m 31s (- 838m 6s) (300 0%) 6.2999\n",
            "loss_change:  -0.02989301681518519\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.1920928955078125e-10\n",
            "Current Decoder Learning Rate: 1.1920928955078125e-10\n",
            "Average Loss: 2m 31s (- 838m 7s) (300 0%) 6.2209\n",
            "Iteration: 300\n",
            "Loss: 2m 32s (- 840m 0s) (301 0%) 6.3432\n",
            "Iteration: 301\n",
            "Loss: 2m 32s (- 839m 37s) (302 0%) 5.9886\n",
            "Iteration: 302\n",
            "Loss: 2m 33s (- 839m 27s) (303 0%) 6.3594\n",
            "Iteration: 303\n",
            "Loss: 2m 33s (- 839m 6s) (304 0%) 6.1428\n",
            "Iteration: 304\n",
            "Loss: 2m 34s (- 839m 3s) (305 0%) 6.2034\n",
            "Iteration: 305\n",
            "Loss: 2m 34s (- 838m 49s) (306 0%) 6.2594\n",
            "Iteration: 306\n",
            "Loss: 2m 34s (- 838m 38s) (307 0%) 6.2779\n",
            "Iteration: 307\n",
            "Loss: 2m 35s (- 838m 28s) (308 0%) 6.1237\n",
            "Iteration: 308\n",
            "Loss: 2m 35s (- 838m 27s) (309 0%) 6.3441\n",
            "Iteration: 309\n",
            "Loss: 2m 36s (- 838m 17s) (310 0%) 6.2301\n",
            "loss_change:  -0.006349802017211914\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 5.960464477539063e-11\n",
            "Current Decoder Learning Rate: 5.960464477539063e-11\n",
            "Average Loss: 2m 36s (- 838m 17s) (310 0%) 6.2273\n",
            "Iteration: 310\n",
            "Loss: 2m 37s (- 840m 1s) (311 0%) 6.1432\n",
            "Iteration: 311\n",
            "Loss: 2m 37s (- 839m 41s) (312 0%) 6.1476\n",
            "Iteration: 312\n",
            "Loss: 2m 38s (- 839m 21s) (313 0%) 6.3457\n",
            "Iteration: 313\n",
            "Loss: 2m 38s (- 839m 10s) (314 0%) 6.3325\n",
            "Iteration: 314\n",
            "Loss: 2m 39s (- 839m 7s) (315 0%) 6.1513\n",
            "Iteration: 315\n",
            "Loss: 2m 39s (- 838m 49s) (316 0%) 6.0144\n",
            "Iteration: 316\n",
            "Loss: 2m 40s (- 838m 42s) (317 0%) 6.2204\n",
            "Iteration: 317\n",
            "Loss: 2m 40s (- 838m 45s) (318 0%) 6.2177\n",
            "Iteration: 318\n",
            "Loss: 2m 41s (- 838m 34s) (319 0%) 6.3057\n",
            "Iteration: 319\n",
            "Loss: 2m 41s (- 838m 23s) (320 0%) 6.1150\n",
            "loss_change:  0.02792687416076678\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.980232238769531e-11\n",
            "Current Decoder Learning Rate: 2.980232238769531e-11\n",
            "Average Loss: 2m 41s (- 838m 23s) (320 0%) 6.1993\n",
            "Iteration: 320\n",
            "Loss: 2m 42s (- 839m 59s) (321 0%) 6.2572\n",
            "Iteration: 321\n",
            "Loss: 2m 42s (- 839m 35s) (322 0%) 6.1517\n",
            "Iteration: 322\n",
            "Loss: 2m 43s (- 839m 18s) (323 0%) 6.1593\n",
            "Iteration: 323\n",
            "Loss: 2m 43s (- 839m 4s) (324 0%) 6.2377\n",
            "Iteration: 324\n",
            "Loss: 2m 44s (- 838m 59s) (325 0%) 6.2874\n",
            "Iteration: 325\n",
            "Loss: 2m 44s (- 838m 42s) (326 0%) 6.0458\n",
            "Iteration: 326\n",
            "Loss: 2m 45s (- 838m 34s) (327 0%) 6.3207\n",
            "Iteration: 327\n",
            "Loss: 2m 45s (- 838m 25s) (328 0%) 6.1599\n",
            "Iteration: 328\n",
            "Loss: 2m 46s (- 838m 17s) (329 0%) 6.3555\n",
            "Iteration: 329\n",
            "Loss: 2m 46s (- 838m 13s) (330 0%) 6.3300\n",
            "loss_change:  -0.03116197586059588\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.4901161193847657e-11\n",
            "Current Decoder Learning Rate: 1.4901161193847657e-11\n",
            "Average Loss: 2m 46s (- 838m 13s) (330 0%) 6.2305\n",
            "Iteration: 330\n",
            "Loss: 2m 47s (- 839m 52s) (331 0%) 6.1079\n",
            "Iteration: 331\n",
            "Loss: 2m 47s (- 839m 30s) (332 0%) 6.2722\n",
            "Iteration: 332\n",
            "Loss: 2m 48s (- 839m 10s) (333 0%) 6.2376\n",
            "Iteration: 333\n",
            "Loss: 2m 48s (- 838m 51s) (334 0%) 6.1074\n",
            "Iteration: 334\n",
            "Loss: 2m 49s (- 838m 41s) (335 0%) 6.2158\n",
            "Iteration: 335\n",
            "Loss: 2m 49s (- 838m 51s) (336 0%) 6.1194\n",
            "Iteration: 336\n",
            "Loss: 2m 50s (- 838m 40s) (337 0%) 6.2212\n",
            "Iteration: 337\n",
            "Loss: 2m 50s (- 838m 36s) (338 0%) 6.1715\n",
            "Iteration: 338\n",
            "Loss: 2m 51s (- 838m 26s) (339 0%) 6.2419\n",
            "Iteration: 339\n",
            "Loss: 2m 51s (- 838m 19s) (340 0%) 6.1479\n",
            "loss_change:  0.04623718261718768\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 7.450580596923828e-12\n",
            "Current Decoder Learning Rate: 7.450580596923828e-12\n",
            "Average Loss: 2m 51s (- 838m 20s) (340 0%) 6.1843\n",
            "Iteration: 340\n",
            "Loss: 2m 52s (- 839m 53s) (341 0%) 6.2434\n",
            "Iteration: 341\n",
            "Loss: 2m 52s (- 839m 36s) (342 0%) 6.3179\n",
            "Iteration: 342\n",
            "Loss: 2m 53s (- 839m 20s) (343 0%) 6.1718\n",
            "Iteration: 343\n",
            "Loss: 2m 53s (- 839m 9s) (344 0%) 6.1035\n",
            "Iteration: 344\n",
            "Loss: 2m 54s (- 838m 56s) (345 0%) 6.2572\n",
            "Iteration: 345\n",
            "Loss: 2m 54s (- 838m 58s) (346 0%) 6.3376\n",
            "Iteration: 346\n",
            "Loss: 2m 55s (- 838m 51s) (347 0%) 6.2149\n",
            "Iteration: 347\n",
            "Loss: 2m 55s (- 838m 39s) (348 0%) 6.2289\n",
            "Iteration: 348\n",
            "Loss: 2m 56s (- 838m 29s) (349 0%) 6.1476\n",
            "Iteration: 349\n",
            "Loss: 2m 56s (- 838m 21s) (350 0%) 6.2020\n",
            "loss_change:  -0.038215351104736506\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.725290298461914e-12\n",
            "Current Decoder Learning Rate: 3.725290298461914e-12\n",
            "Average Loss: 2m 56s (- 838m 21s) (350 0%) 6.2225\n",
            "Iteration: 350\n",
            "Loss: 2m 57s (- 840m 3s) (351 0%) 6.0400\n",
            "Iteration: 351\n",
            "Loss: 2m 57s (- 839m 46s) (352 0%) 6.0938\n",
            "Iteration: 352\n",
            "Loss: 2m 58s (- 839m 25s) (353 0%) 6.1947\n",
            "Iteration: 353\n",
            "Loss: 2m 58s (- 839m 11s) (354 0%) 6.2135\n",
            "Iteration: 354\n",
            "Loss: 2m 59s (- 838m 57s) (355 0%) 6.3797\n",
            "Iteration: 355\n",
            "Loss: 2m 59s (- 838m 49s) (356 0%) 6.2400\n",
            "Iteration: 356\n",
            "Loss: 3m 0s (- 838m 40s) (357 0%) 6.1731\n",
            "Iteration: 357\n",
            "Loss: 3m 0s (- 838m 36s) (358 0%) 6.2248\n",
            "Iteration: 358\n",
            "Loss: 3m 1s (- 838m 34s) (359 0%) 6.2046\n",
            "Iteration: 359\n",
            "Loss: 3m 1s (- 838m 28s) (360 0%) 6.1906\n",
            "loss_change:  0.026998519897460938\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.862645149230957e-12\n",
            "Current Decoder Learning Rate: 1.862645149230957e-12\n",
            "Average Loss: 3m 1s (- 838m 28s) (360 0%) 6.1955\n",
            "Iteration: 360\n",
            "Loss: 3m 2s (- 840m 2s) (361 0%) 6.2171\n",
            "Iteration: 361\n",
            "Loss: 3m 3s (- 839m 48s) (362 0%) 6.1515\n",
            "Iteration: 362\n",
            "Loss: 3m 3s (- 839m 32s) (363 0%) 6.2653\n",
            "Iteration: 363\n",
            "Loss: 3m 3s (- 839m 17s) (364 0%) 6.2306\n",
            "Iteration: 364\n",
            "Loss: 3m 4s (- 839m 1s) (365 0%) 6.2293\n",
            "Iteration: 365\n",
            "Loss: 3m 4s (- 838m 55s) (366 0%) 6.1142\n",
            "Iteration: 366\n",
            "Loss: 3m 5s (- 838m 54s) (367 0%) 6.3273\n",
            "Iteration: 367\n",
            "Loss: 3m 5s (- 838m 47s) (368 0%) 6.1674\n",
            "Iteration: 368\n",
            "Loss: 3m 6s (- 838m 44s) (369 0%) 6.1354\n",
            "Iteration: 369\n",
            "Loss: 3m 6s (- 838m 38s) (370 0%) 6.2574\n",
            "loss_change:  -0.014076566696166637\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 9.313225746154785e-13\n",
            "Current Decoder Learning Rate: 9.313225746154785e-13\n",
            "Average Loss: 3m 6s (- 838m 39s) (370 0%) 6.2096\n",
            "Iteration: 370\n",
            "Loss: 3m 7s (- 840m 18s) (371 0%) 6.3031\n",
            "Iteration: 371\n",
            "Loss: 3m 8s (- 840m 2s) (372 0%) 6.2362\n",
            "Iteration: 372\n",
            "Loss: 3m 8s (- 839m 44s) (373 0%) 6.1462\n",
            "Iteration: 373\n",
            "Loss: 3m 9s (- 839m 30s) (374 0%) 6.1726\n",
            "Iteration: 374\n",
            "Loss: 3m 9s (- 839m 10s) (375 0%) 6.2193\n",
            "Iteration: 375\n",
            "Loss: 3m 9s (- 838m 57s) (376 0%) 6.2341\n",
            "Iteration: 376\n",
            "Loss: 3m 10s (- 838m 55s) (377 0%) 6.3159\n",
            "Iteration: 377\n",
            "Loss: 3m 10s (- 838m 52s) (378 0%) 6.2248\n",
            "Iteration: 378\n",
            "Loss: 3m 11s (- 838m 43s) (379 0%) 6.2467\n",
            "Iteration: 379\n",
            "Loss: 3m 11s (- 838m 39s) (380 0%) 6.0518\n",
            "loss_change:  -0.005500507354736506\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 4.656612873077393e-13\n",
            "Current Decoder Learning Rate: 4.656612873077393e-13\n",
            "Average Loss: 3m 11s (- 838m 40s) (380 0%) 6.2151\n",
            "Iteration: 380\n",
            "Loss: 3m 12s (- 840m 12s) (381 0%) 6.3311\n",
            "Iteration: 381\n",
            "Loss: 3m 13s (- 839m 59s) (382 0%) 6.1379\n",
            "Iteration: 382\n",
            "Loss: 3m 13s (- 839m 40s) (383 0%) 6.2917\n",
            "Iteration: 383\n",
            "Loss: 3m 14s (- 839m 28s) (384 0%) 6.2588\n",
            "Iteration: 384\n",
            "Loss: 3m 14s (- 839m 10s) (385 0%) 6.2792\n",
            "Iteration: 385\n",
            "Loss: 3m 15s (- 838m 56s) (386 0%) 6.1749\n",
            "Iteration: 386\n",
            "Loss: 3m 15s (- 838m 46s) (387 0%) 6.2473\n",
            "Iteration: 387\n",
            "Loss: 3m 16s (- 838m 40s) (388 0%) 6.1425\n",
            "Iteration: 388\n",
            "Loss: 3m 16s (- 838m 32s) (389 0%) 6.1213\n",
            "Iteration: 389\n",
            "Loss: 3m 16s (- 838m 29s) (390 0%) 6.2607\n",
            "loss_change:  -0.009476184844970703\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.3283064365386963e-13\n",
            "Current Decoder Learning Rate: 2.3283064365386963e-13\n",
            "Average Loss: 3m 16s (- 838m 30s) (390 0%) 6.2245\n",
            "Iteration: 390\n",
            "Loss: 3m 17s (- 840m 7s) (391 0%) 6.0628\n",
            "Iteration: 391\n",
            "Loss: 3m 18s (- 840m 0s) (392 0%) 6.2937\n",
            "Iteration: 392\n",
            "Loss: 3m 18s (- 839m 42s) (393 0%) 6.0802\n",
            "Iteration: 393\n",
            "Loss: 3m 19s (- 839m 23s) (394 0%) 6.2158\n",
            "Iteration: 394\n",
            "Loss: 3m 19s (- 839m 8s) (395 0%) 6.1757\n",
            "Iteration: 395\n",
            "Loss: 3m 20s (- 838m 51s) (396 0%) 6.1429\n",
            "Iteration: 396\n",
            "Loss: 3m 20s (- 838m 44s) (397 0%) 6.1904\n",
            "Iteration: 397\n",
            "Loss: 3m 21s (- 838m 42s) (398 0%) 6.3937\n",
            "Iteration: 398\n",
            "Loss: 3m 21s (- 838m 30s) (399 0%) 6.2691\n",
            "Iteration: 399\n",
            "Loss: 3m 22s (- 838m 18s) (400 0%) 6.3201\n",
            "loss_change:  0.01009721755981463\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.1641532182693482e-13\n",
            "Current Decoder Learning Rate: 1.1641532182693482e-13\n",
            "Average Loss: 3m 22s (- 838m 18s) (400 0%) 6.2144\n",
            "Iteration: 400\n",
            "Loss: 3m 22s (- 839m 52s) (401 0%) 6.0177\n",
            "Iteration: 401\n",
            "Loss: 3m 23s (- 839m 45s) (402 0%) 6.2071\n",
            "Iteration: 402\n",
            "Loss: 3m 23s (- 839m 35s) (403 0%) 6.1981\n",
            "Iteration: 403\n",
            "Loss: 3m 24s (- 839m 26s) (404 0%) 6.2137\n",
            "Iteration: 404\n",
            "Loss: 3m 24s (- 839m 12s) (405 0%) 6.2530\n",
            "Iteration: 405\n",
            "Loss: 3m 25s (- 838m 54s) (406 0%) 6.1684\n",
            "Iteration: 406\n",
            "Loss: 3m 25s (- 838m 48s) (407 0%) 6.1833\n",
            "Iteration: 407\n",
            "Loss: 3m 26s (- 838m 43s) (408 0%) 6.2670\n",
            "Iteration: 408\n",
            "Loss: 3m 26s (- 838m 32s) (409 0%) 6.1661\n",
            "Iteration: 409\n",
            "Loss: 3m 27s (- 838m 26s) (410 0%) 6.2771\n",
            "loss_change:  0.019308280944823508\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 5.820766091346741e-14\n",
            "Current Decoder Learning Rate: 5.820766091346741e-14\n",
            "Average Loss: 3m 27s (- 838m 27s) (410 0%) 6.1951\n",
            "Iteration: 410\n",
            "Loss: 3m 27s (- 839m 48s) (411 0%) 6.1781\n",
            "Iteration: 411\n",
            "Loss: 3m 28s (- 839m 38s) (412 0%) 6.1252\n",
            "Iteration: 412\n",
            "Loss: 3m 28s (- 839m 28s) (413 0%) 6.2876\n",
            "Iteration: 413\n",
            "Loss: 3m 29s (- 839m 13s) (414 0%) 6.4298\n",
            "Iteration: 414\n",
            "Loss: 3m 29s (- 839m 2s) (415 0%) 6.0478\n",
            "Iteration: 415\n",
            "Loss: 3m 30s (- 838m 46s) (416 0%) 6.1768\n",
            "Iteration: 416\n",
            "Loss: 3m 30s (- 838m 36s) (417 0%) 6.2874\n",
            "Iteration: 417\n",
            "Loss: 3m 31s (- 838m 38s) (418 0%) 6.3379\n",
            "Iteration: 418\n",
            "Loss: 3m 31s (- 838m 24s) (419 0%) 6.2050\n",
            "Iteration: 419\n",
            "Loss: 3m 32s (- 838m 15s) (420 0%) 6.1450\n",
            "loss_change:  -0.026912212371826172\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.9103830456733704e-14\n",
            "Current Decoder Learning Rate: 2.9103830456733704e-14\n",
            "Average Loss: 3m 32s (- 838m 15s) (420 0%) 6.2220\n",
            "Iteration: 420\n",
            "Loss: 3m 33s (- 839m 41s) (421 0%) 6.1305\n",
            "Iteration: 421\n",
            "Loss: 3m 33s (- 839m 33s) (422 0%) 6.2883\n",
            "Iteration: 422\n",
            "Loss: 3m 33s (- 839m 18s) (423 0%) 6.1379\n",
            "Iteration: 423\n",
            "Loss: 3m 34s (- 839m 1s) (424 0%) 6.1157\n",
            "Iteration: 424\n",
            "Loss: 3m 34s (- 838m 45s) (425 0%) 6.2680\n",
            "Iteration: 425\n",
            "Loss: 3m 35s (- 838m 31s) (426 0%) 6.2414\n",
            "Iteration: 426\n",
            "Loss: 3m 35s (- 838m 21s) (427 0%) 6.1429\n",
            "Iteration: 427\n",
            "Loss: 3m 36s (- 838m 18s) (428 0%) 6.2773\n",
            "Iteration: 428\n",
            "Loss: 3m 36s (- 838m 12s) (429 0%) 6.1258\n",
            "Iteration: 429\n",
            "Loss: 3m 37s (- 838m 5s) (430 0%) 6.2033\n",
            "loss_change:  0.02894597053527903\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.4551915228366852e-14\n",
            "Current Decoder Learning Rate: 1.4551915228366852e-14\n",
            "Average Loss: 3m 37s (- 838m 5s) (430 0%) 6.1931\n",
            "Iteration: 430\n",
            "Loss: 3m 37s (- 839m 21s) (431 0%) 6.1853\n",
            "Iteration: 431\n",
            "Loss: 3m 38s (- 839m 12s) (432 0%) 6.2320\n",
            "Iteration: 432\n",
            "Loss: 3m 38s (- 838m 57s) (433 0%) 6.1894\n",
            "Iteration: 433\n",
            "Loss: 3m 39s (- 838m 40s) (434 0%) 6.2369\n",
            "Iteration: 434\n",
            "Loss: 3m 39s (- 838m 27s) (435 0%) 6.3367\n",
            "Iteration: 435\n",
            "Loss: 3m 40s (- 838m 12s) (436 0%) 6.3308\n",
            "Iteration: 436\n",
            "Loss: 3m 40s (- 838m 1s) (437 0%) 6.2742\n",
            "Iteration: 437\n",
            "Loss: 3m 41s (- 837m 58s) (438 0%) 6.1435\n",
            "Iteration: 438\n",
            "Loss: 3m 41s (- 837m 49s) (439 0%) 6.1344\n",
            "Iteration: 439\n",
            "Loss: 3m 42s (- 837m 40s) (440 0%) 6.2418\n",
            "loss_change:  -0.03741331100463885\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 7.275957614183426e-15\n",
            "Current Decoder Learning Rate: 7.275957614183426e-15\n",
            "Average Loss: 3m 42s (- 837m 40s) (440 0%) 6.2305\n",
            "Iteration: 440\n",
            "Loss: 3m 43s (- 839m 6s) (441 0%) 6.1961\n",
            "Iteration: 441\n",
            "Loss: 3m 43s (- 838m 53s) (442 0%) 6.0579\n",
            "Iteration: 442\n",
            "Loss: 3m 43s (- 838m 39s) (443 0%) 6.1005\n",
            "Iteration: 443\n",
            "Loss: 3m 44s (- 838m 26s) (444 0%) 6.3678\n",
            "Iteration: 444\n",
            "Loss: 3m 44s (- 838m 13s) (445 0%) 6.2064\n",
            "Iteration: 445\n",
            "Loss: 3m 45s (- 838m 3s) (446 0%) 6.2375\n",
            "Iteration: 446\n",
            "Loss: 3m 45s (- 837m 56s) (447 0%) 6.4445\n",
            "Iteration: 447\n",
            "Loss: 3m 46s (- 837m 54s) (448 0%) 6.1710\n",
            "Iteration: 448\n",
            "Loss: 3m 46s (- 837m 46s) (449 0%) 6.0995\n",
            "Iteration: 449\n",
            "Loss: 3m 47s (- 837m 45s) (450 0%) 6.2500\n",
            "loss_change:  0.017409467697143022\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.637978807091713e-15\n",
            "Current Decoder Learning Rate: 3.637978807091713e-15\n",
            "Average Loss: 3m 47s (- 837m 45s) (450 0%) 6.2131\n",
            "Iteration: 450\n",
            "Loss: 3m 48s (- 839m 6s) (451 0%) 6.1227\n",
            "Iteration: 451\n",
            "Loss: 3m 48s (- 838m 59s) (452 0%) 6.1793\n",
            "Iteration: 452\n",
            "Loss: 3m 49s (- 838m 46s) (453 0%) 6.1797\n",
            "Iteration: 453\n",
            "Loss: 3m 49s (- 838m 34s) (454 0%) 6.1421\n",
            "Iteration: 454\n",
            "Loss: 3m 49s (- 838m 19s) (455 0%) 6.2384\n",
            "Iteration: 455\n",
            "Loss: 3m 50s (- 838m 12s) (456 0%) 6.1357\n",
            "Iteration: 456\n",
            "Loss: 3m 50s (- 838m 11s) (457 0%) 6.1604\n",
            "Iteration: 457\n",
            "Loss: 3m 51s (- 838m 6s) (458 0%) 6.2782\n",
            "Iteration: 458\n",
            "Loss: 3m 51s (- 837m 59s) (459 0%) 6.2900\n",
            "Iteration: 459\n",
            "Loss: 3m 52s (- 837m 53s) (460 0%) 6.1225\n",
            "loss_change:  0.028212785720825195\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.8189894035458565e-15\n",
            "Current Decoder Learning Rate: 1.8189894035458565e-15\n",
            "Average Loss: 3m 52s (- 837m 53s) (460 0%) 6.1849\n",
            "Iteration: 460\n",
            "Loss: 3m 53s (- 839m 18s) (461 0%) 6.2892\n",
            "Iteration: 461\n",
            "Loss: 3m 53s (- 839m 17s) (462 0%) 6.0405\n",
            "Iteration: 462\n",
            "Loss: 3m 54s (- 839m 6s) (463 0%) 6.1841\n",
            "Iteration: 463\n",
            "Loss: 3m 54s (- 838m 54s) (464 0%) 6.3381\n",
            "Iteration: 464\n",
            "Loss: 3m 55s (- 838m 38s) (465 0%) 6.2454\n",
            "Iteration: 465\n",
            "Loss: 3m 55s (- 838m 33s) (466 0%) 6.1135\n",
            "Iteration: 466\n",
            "Loss: 3m 56s (- 838m 25s) (467 0%) 6.2027\n",
            "Iteration: 467\n",
            "Loss: 3m 56s (- 838m 22s) (468 0%) 6.1694\n",
            "Iteration: 468\n",
            "Loss: 3m 56s (- 838m 13s) (469 0%) 6.1848\n",
            "Iteration: 469\n",
            "Loss: 3m 57s (- 838m 6s) (470 0%) 6.1701\n",
            "loss_change:  -0.008890628814697266\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 9.094947017729283e-16\n",
            "Current Decoder Learning Rate: 9.094947017729283e-16\n",
            "Average Loss: 3m 57s (- 838m 6s) (470 0%) 6.1938\n",
            "Iteration: 470\n",
            "Loss: 3m 58s (- 839m 22s) (471 0%) 6.4485\n",
            "Iteration: 471\n",
            "Loss: 3m 58s (- 839m 11s) (472 0%) 6.3010\n",
            "Iteration: 472\n",
            "Loss: 3m 59s (- 838m 55s) (473 0%) 6.0537\n",
            "Iteration: 473\n",
            "Loss: 3m 59s (- 838m 49s) (474 0%) 6.1834\n",
            "Iteration: 474\n",
            "Loss: 4m 0s (- 838m 43s) (475 0%) 6.3758\n",
            "Iteration: 475\n",
            "Loss: 4m 0s (- 838m 40s) (476 0%) 6.1348\n",
            "Iteration: 476\n",
            "Loss: 4m 1s (- 838m 36s) (477 0%) 6.1702\n",
            "Iteration: 477\n",
            "Loss: 4m 1s (- 838m 31s) (478 0%) 6.2789\n",
            "Iteration: 478\n",
            "Loss: 4m 2s (- 838m 24s) (479 0%) 6.2616\n",
            "Iteration: 479\n",
            "Loss: 4m 2s (- 838m 15s) (480 0%) 6.4399\n",
            "loss_change:  -0.07099123001098562\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 4.547473508864641e-16\n",
            "Current Decoder Learning Rate: 4.547473508864641e-16\n",
            "Average Loss: 4m 2s (- 838m 15s) (480 0%) 6.2648\n",
            "Iteration: 480\n",
            "Loss: 4m 3s (- 839m 28s) (481 0%) 6.2144\n",
            "Iteration: 481\n",
            "Loss: 4m 3s (- 839m 17s) (482 0%) 6.1384\n",
            "Iteration: 482\n",
            "Loss: 4m 4s (- 839m 10s) (483 0%) 6.1527\n",
            "Iteration: 483\n",
            "Loss: 4m 4s (- 839m 4s) (484 0%) 6.1715\n",
            "Iteration: 484\n",
            "Loss: 4m 5s (- 838m 59s) (485 0%) 6.1921\n",
            "Iteration: 485\n",
            "Loss: 4m 5s (- 838m 54s) (486 0%) 6.2299\n",
            "Iteration: 486\n",
            "Loss: 4m 6s (- 838m 46s) (487 0%) 6.2165\n",
            "Iteration: 487\n",
            "Loss: 4m 6s (- 838m 37s) (488 0%) 6.1410\n",
            "Iteration: 488\n",
            "Loss: 4m 7s (- 838m 24s) (489 0%) 6.2682\n",
            "Iteration: 489\n",
            "Loss: 4m 7s (- 838m 13s) (490 0%) 6.1574\n",
            "loss_change:  0.07655577659606916\n",
            "Average Loss: 4m 7s (- 838m 13s) (490 0%) 6.1882\n",
            "Iteration: 490\n",
            "Loss: 4m 8s (- 839m 41s) (491 0%) 6.3051\n",
            "Iteration: 491\n",
            "Loss: 4m 9s (- 839m 40s) (492 0%) 6.0715\n",
            "Iteration: 492\n",
            "Loss: 4m 9s (- 839m 31s) (493 0%) 6.1576\n",
            "Iteration: 493\n",
            "Loss: 4m 10s (- 839m 30s) (494 0%) 6.3626\n",
            "Iteration: 494\n",
            "Loss: 4m 10s (- 839m 20s) (495 0%) 6.0246\n",
            "Iteration: 495\n",
            "Loss: 4m 11s (- 839m 16s) (496 0%) 6.2088\n",
            "Iteration: 496\n",
            "Loss: 4m 11s (- 839m 6s) (497 0%) 6.0687\n",
            "Iteration: 497\n",
            "Loss: 4m 11s (- 838m 53s) (498 0%) 6.2336\n",
            "Iteration: 498\n",
            "Loss: 4m 12s (- 838m 40s) (499 0%) 6.2020\n",
            "Iteration: 499\n",
            "Loss: 4m 12s (- 838m 26s) (500 0%) 6.2061\n",
            "loss_change:  0.004146575927734375\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.2737367544323206e-16\n",
            "Current Decoder Learning Rate: 2.2737367544323206e-16\n",
            "Average Loss: 4m 12s (- 838m 26s) (500 0%) 6.1841\n",
            "Iteration: 500\n",
            "Loss: 4m 13s (- 839m 41s) (501 0%) 6.3250\n",
            "Iteration: 501\n",
            "Loss: 4m 14s (- 839m 41s) (502 0%) 6.1793\n",
            "Iteration: 502\n",
            "Loss: 4m 14s (- 839m 37s) (503 0%) 6.0603\n",
            "Iteration: 503\n",
            "Loss: 4m 15s (- 839m 31s) (504 0%) 6.1991\n",
            "Iteration: 504\n",
            "Loss: 4m 15s (- 839m 27s) (505 0%) 6.3669\n",
            "Iteration: 505\n",
            "Loss: 4m 16s (- 839m 19s) (506 0%) 6.2800\n",
            "Iteration: 506\n",
            "Loss: 4m 16s (- 839m 9s) (507 0%) 6.1904\n",
            "Iteration: 507\n",
            "Loss: 4m 17s (- 838m 56s) (508 0%) 6.1976\n",
            "Iteration: 508\n",
            "Loss: 4m 17s (- 838m 45s) (509 0%) 6.2975\n",
            "Iteration: 509\n",
            "Loss: 4m 17s (- 838m 31s) (510 0%) 6.1745\n",
            "loss_change:  -0.042983293533325195\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.1368683772161603e-16\n",
            "Current Decoder Learning Rate: 1.1368683772161603e-16\n",
            "Average Loss: 4m 17s (- 838m 31s) (510 0%) 6.2271\n",
            "Iteration: 510\n",
            "Loss: 4m 18s (- 839m 49s) (511 0%) 6.2841\n",
            "Iteration: 511\n",
            "Loss: 4m 19s (- 839m 45s) (512 0%) 6.1711\n",
            "Iteration: 512\n",
            "Loss: 4m 19s (- 839m 36s) (513 0%) 6.1534\n",
            "Iteration: 513\n",
            "Loss: 4m 20s (- 839m 31s) (514 0%) 6.1315\n",
            "Iteration: 514\n",
            "Loss: 4m 20s (- 839m 26s) (515 0%) 6.3157\n",
            "Iteration: 515\n",
            "Loss: 4m 21s (- 839m 18s) (516 0%) 6.1614\n",
            "Iteration: 516\n",
            "Loss: 4m 21s (- 839m 12s) (517 0%) 6.0778\n",
            "Iteration: 517\n",
            "Loss: 4m 22s (- 839m 2s) (518 0%) 6.2568\n",
            "Iteration: 518\n",
            "Loss: 4m 22s (- 838m 51s) (519 0%) 6.0720\n",
            "Iteration: 519\n",
            "Loss: 4m 23s (- 838m 40s) (520 0%) 6.3092\n",
            "loss_change:  0.03375573158264178\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 5.684341886080802e-17\n",
            "Current Decoder Learning Rate: 5.684341886080802e-17\n",
            "Average Loss: 4m 23s (- 838m 40s) (520 0%) 6.1933\n",
            "Iteration: 520\n",
            "Loss: 4m 24s (- 841m 33s) (521 0%) 6.2739\n",
            "Iteration: 521\n",
            "Loss: 4m 24s (- 841m 27s) (522 0%) 6.1444\n",
            "Iteration: 522\n",
            "Loss: 4m 25s (- 841m 24s) (523 0%) 6.2169\n",
            "Iteration: 523\n",
            "Loss: 4m 25s (- 841m 15s) (524 0%) 6.1667\n",
            "Iteration: 524\n",
            "Loss: 4m 26s (- 841m 5s) (525 0%) 6.2262\n",
            "Iteration: 525\n",
            "Loss: 4m 26s (- 840m 57s) (526 0%) 6.1982\n",
            "Iteration: 526\n",
            "Loss: 4m 27s (- 840m 49s) (527 0%) 6.4417\n",
            "Iteration: 527\n",
            "Loss: 4m 27s (- 840m 39s) (528 0%) 6.2297\n",
            "Iteration: 528\n",
            "Loss: 4m 28s (- 840m 26s) (529 0%) 6.2006\n",
            "Iteration: 529\n",
            "Loss: 4m 28s (- 840m 18s) (530 0%) 6.3218\n",
            "loss_change:  -0.048711919784546254\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.842170943040401e-17\n",
            "Current Decoder Learning Rate: 2.842170943040401e-17\n",
            "Average Loss: 4m 28s (- 840m 19s) (530 0%) 6.2420\n",
            "Iteration: 530\n",
            "Loss: 4m 30s (- 843m 8s) (531 0%) 6.1703\n",
            "Iteration: 531\n",
            "Loss: 4m 30s (- 843m 0s) (532 0%) 6.2367\n",
            "Iteration: 532\n",
            "Loss: 4m 31s (- 842m 53s) (533 0%) 5.9448\n",
            "Iteration: 533\n",
            "Loss: 4m 31s (- 842m 44s) (534 0%) 6.0160\n",
            "Iteration: 534\n",
            "Loss: 4m 31s (- 842m 42s) (535 0%) 6.3945\n",
            "Iteration: 535\n",
            "Loss: 4m 32s (- 842m 32s) (536 0%) 6.2562\n",
            "Iteration: 536\n",
            "Loss: 4m 32s (- 842m 22s) (537 0%) 6.3153\n",
            "Iteration: 537\n",
            "Loss: 4m 33s (- 842m 10s) (538 0%) 6.0973\n",
            "Iteration: 538\n",
            "Loss: 4m 33s (- 841m 59s) (539 0%) 6.2764\n",
            "Iteration: 539\n",
            "Loss: 4m 34s (- 841m 50s) (540 0%) 6.3528\n",
            "loss_change:  0.03597912788391078\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.4210854715202004e-17\n",
            "Current Decoder Learning Rate: 1.4210854715202004e-17\n",
            "Average Loss: 4m 34s (- 841m 50s) (540 0%) 6.2060\n",
            "Iteration: 540\n",
            "Loss: 4m 35s (- 844m 13s) (541 0%) 6.1952\n",
            "Iteration: 541\n",
            "Loss: 4m 36s (- 844m 7s) (542 0%) 6.1400\n",
            "Iteration: 542\n",
            "Loss: 4m 36s (- 843m 58s) (543 0%) 6.1976\n",
            "Iteration: 543\n",
            "Loss: 4m 36s (- 843m 52s) (544 0%) 6.2221\n",
            "Iteration: 544\n",
            "Loss: 4m 37s (- 843m 45s) (545 0%) 6.2754\n",
            "Iteration: 545\n",
            "Loss: 4m 37s (- 843m 37s) (546 0%) 6.1906\n",
            "Iteration: 546\n",
            "Loss: 4m 38s (- 843m 27s) (547 0%) 6.1434\n",
            "Iteration: 547\n",
            "Loss: 4m 38s (- 843m 15s) (548 0%) 6.3773\n",
            "Iteration: 548\n",
            "Loss: 4m 39s (- 843m 2s) (549 0%) 6.1917\n",
            "Iteration: 549\n",
            "Loss: 4m 39s (- 842m 50s) (550 0%) 6.2322\n",
            "loss_change:  -0.010521221160888494\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 7.105427357601002e-18\n",
            "Current Decoder Learning Rate: 7.105427357601002e-18\n",
            "Average Loss: 4m 39s (- 842m 51s) (550 0%) 6.2166\n",
            "Iteration: 550\n",
            "Loss: 4m 41s (- 845m 55s) (551 0%) 6.3072\n",
            "Iteration: 551\n",
            "Loss: 4m 41s (- 845m 47s) (552 0%) 6.3533\n",
            "Iteration: 552\n",
            "Loss: 4m 42s (- 845m 42s) (553 0%) 6.0821\n",
            "Iteration: 553\n",
            "Loss: 4m 42s (- 845m 35s) (554 0%) 6.2015\n",
            "Iteration: 554\n",
            "Loss: 4m 43s (- 845m 28s) (555 0%) 6.1196\n",
            "Iteration: 555\n",
            "Loss: 4m 43s (- 845m 17s) (556 0%) 6.1441\n",
            "Iteration: 556\n",
            "Loss: 4m 44s (- 845m 7s) (557 0%) 6.2536\n",
            "Iteration: 557\n",
            "Loss: 4m 44s (- 844m 53s) (558 0%) 6.1868\n",
            "Iteration: 558\n",
            "Loss: 4m 44s (- 844m 39s) (559 0%) 6.1565\n",
            "Iteration: 559\n",
            "Loss: 4m 45s (- 844m 30s) (560 0%) 6.1834\n",
            "loss_change:  0.017746686935424805\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.552713678800501e-18\n",
            "Current Decoder Learning Rate: 3.552713678800501e-18\n",
            "Average Loss: 4m 45s (- 844m 31s) (560 0%) 6.1988\n",
            "Iteration: 560\n",
            "Loss: 4m 46s (- 847m 8s) (561 0%) 6.2069\n",
            "Iteration: 561\n",
            "Loss: 4m 47s (- 847m 0s) (562 0%) 6.2618\n",
            "Iteration: 562\n",
            "Loss: 4m 47s (- 846m 54s) (563 0%) 6.2498\n",
            "Iteration: 563\n",
            "Loss: 4m 48s (- 846m 49s) (564 0%) 6.0388\n",
            "Iteration: 564\n",
            "Loss: 4m 48s (- 846m 43s) (565 0%) 6.0635\n",
            "Iteration: 565\n",
            "Loss: 4m 49s (- 846m 33s) (566 0%) 6.2066\n",
            "Iteration: 566\n",
            "Loss: 4m 49s (- 846m 27s) (567 0%) 6.1157\n",
            "Iteration: 567\n",
            "Loss: 4m 50s (- 846m 17s) (568 0%) 6.2446\n",
            "Iteration: 568\n",
            "Loss: 4m 50s (- 846m 3s) (569 0%) 6.1241\n",
            "Iteration: 569\n",
            "Loss: 4m 50s (- 845m 53s) (570 0%) 6.2743\n",
            "loss_change:  0.02019147872924787\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.7763568394002505e-18\n",
            "Current Decoder Learning Rate: 1.7763568394002505e-18\n",
            "Average Loss: 4m 50s (- 845m 53s) (570 0%) 6.1786\n",
            "Iteration: 570\n",
            "Loss: 4m 52s (- 848m 30s) (571 0%) 6.1750\n",
            "Iteration: 571\n",
            "Loss: 4m 52s (- 848m 22s) (572 0%) 6.4036\n",
            "Iteration: 572\n",
            "Loss: 4m 53s (- 848m 17s) (573 0%) 6.2031\n",
            "Iteration: 573\n",
            "Loss: 4m 53s (- 848m 14s) (574 0%) 6.2898\n",
            "Iteration: 574\n",
            "Loss: 4m 54s (- 848m 5s) (575 0%) 6.1266\n",
            "Iteration: 575\n",
            "Loss: 4m 54s (- 848m 1s) (576 0%) 6.4988\n",
            "Iteration: 576\n",
            "Loss: 4m 55s (- 847m 49s) (577 0%) 6.1053\n",
            "Iteration: 577\n",
            "Loss: 4m 55s (- 847m 38s) (578 0%) 6.1419\n",
            "Iteration: 578\n",
            "Loss: 4m 56s (- 847m 26s) (579 0%) 6.2228\n",
            "Iteration: 579\n",
            "Loss: 4m 56s (- 847m 16s) (580 0%) 6.2664\n",
            "loss_change:  -0.06471495628356916\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 8.881784197001253e-19\n",
            "Current Decoder Learning Rate: 8.881784197001253e-19\n",
            "Average Loss: 4m 56s (- 847m 16s) (580 0%) 6.2433\n",
            "Iteration: 580\n",
            "Loss: 4m 57s (- 849m 46s) (581 0%) 6.3568\n",
            "Iteration: 581\n",
            "Loss: 4m 58s (- 849m 37s) (582 0%) 6.1029\n",
            "Iteration: 582\n",
            "Loss: 4m 58s (- 849m 33s) (583 0%) 6.2590\n",
            "Iteration: 583\n",
            "Loss: 4m 59s (- 849m 24s) (584 0%) 6.0767\n",
            "Iteration: 584\n",
            "Loss: 4m 59s (- 849m 18s) (585 0%) 6.3253\n",
            "Iteration: 585\n",
            "Loss: 5m 0s (- 849m 11s) (586 0%) 6.2687\n",
            "Iteration: 586\n",
            "Loss: 5m 0s (- 849m 1s) (587 0%) 6.1191\n",
            "Iteration: 587\n",
            "Loss: 5m 1s (- 848m 48s) (588 0%) 6.1761\n",
            "Iteration: 588\n",
            "Loss: 5m 1s (- 848m 37s) (589 0%) 6.2642\n",
            "Iteration: 589\n",
            "Loss: 5m 2s (- 848m 26s) (590 0%) 6.1723\n",
            "loss_change:  0.031218290328979492\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 4.440892098500626e-19\n",
            "Current Decoder Learning Rate: 4.440892098500626e-19\n",
            "Average Loss: 5m 2s (- 848m 26s) (590 0%) 6.2121\n",
            "Iteration: 590\n",
            "Loss: 5m 3s (- 850m 59s) (591 0%) 6.2336\n",
            "Iteration: 591\n",
            "Loss: 5m 4s (- 850m 55s) (592 0%) 6.2311\n",
            "Iteration: 592\n",
            "Loss: 5m 4s (- 850m 51s) (593 0%) 6.1544\n",
            "Iteration: 593\n",
            "Loss: 5m 5s (- 850m 48s) (594 0%) 6.1383\n",
            "Iteration: 594\n",
            "Loss: 5m 5s (- 850m 41s) (595 0%) 6.1961\n",
            "Iteration: 595\n",
            "Loss: 5m 5s (- 850m 32s) (596 0%) 6.1102\n",
            "Iteration: 596\n",
            "Loss: 5m 6s (- 850m 22s) (597 0%) 6.2682\n",
            "Iteration: 597\n",
            "Loss: 5m 6s (- 850m 12s) (598 0%) 6.2715\n",
            "Iteration: 598\n",
            "Loss: 5m 7s (- 849m 59s) (599 0%) 6.1229\n",
            "Iteration: 599\n",
            "Loss: 5m 7s (- 849m 48s) (600 0%) 6.3018\n",
            "loss_change:  0.009299135208130416\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.220446049250313e-19\n",
            "Current Decoder Learning Rate: 2.220446049250313e-19\n",
            "Average Loss: 5m 7s (- 849m 49s) (600 0%) 6.2028\n",
            "Iteration: 600\n",
            "Loss: 5m 8s (- 850m 55s) (601 0%) 6.2984\n",
            "Iteration: 601\n",
            "Loss: 5m 9s (- 850m 46s) (602 0%) 6.1375\n",
            "Iteration: 602\n",
            "Loss: 5m 9s (- 850m 35s) (603 0%) 6.1226\n",
            "Iteration: 603\n",
            "Loss: 5m 10s (- 850m 25s) (604 0%) 6.2929\n",
            "Iteration: 604\n",
            "Loss: 5m 10s (- 850m 13s) (605 0%) 6.1714\n",
            "Iteration: 605\n",
            "Loss: 5m 11s (- 850m 12s) (606 0%) 6.2428\n",
            "Iteration: 606\n",
            "Loss: 5m 11s (- 850m 4s) (607 0%) 6.1505\n",
            "Iteration: 607\n",
            "Loss: 5m 11s (- 850m 1s) (608 0%) 6.1155\n",
            "Iteration: 608\n",
            "Loss: 5m 12s (- 849m 53s) (609 0%) 6.3689\n",
            "Iteration: 609\n",
            "Loss: 5m 12s (- 849m 50s) (610 0%) 6.2470\n",
            "loss_change:  -0.011933708190918324\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.1102230246251566e-19\n",
            "Current Decoder Learning Rate: 1.1102230246251566e-19\n",
            "Average Loss: 5m 12s (- 849m 50s) (610 0%) 6.2147\n",
            "Iteration: 610\n",
            "Loss: 5m 13s (- 850m 44s) (611 0%) 6.1493\n",
            "Iteration: 611\n",
            "Loss: 5m 14s (- 850m 33s) (612 0%) 6.2136\n",
            "Iteration: 612\n",
            "Loss: 5m 14s (- 850m 22s) (613 0%) 6.3338\n",
            "Iteration: 613\n",
            "Loss: 5m 15s (- 850m 13s) (614 0%) 6.0956\n",
            "Iteration: 614\n",
            "Loss: 5m 15s (- 850m 7s) (615 0%) 6.2616\n",
            "Iteration: 615\n",
            "Loss: 5m 16s (- 850m 5s) (616 0%) 6.1581\n",
            "Iteration: 616\n",
            "Loss: 5m 16s (- 849m 57s) (617 0%) 6.2632\n",
            "Iteration: 617\n",
            "Loss: 5m 17s (- 849m 52s) (618 0%) 6.3238\n",
            "Iteration: 618\n",
            "Loss: 5m 17s (- 849m 45s) (619 0%) 6.0784\n",
            "Iteration: 619\n",
            "Loss: 5m 18s (- 849m 39s) (620 0%) 6.1955\n",
            "loss_change:  0.007450723648071644\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 5.551115123125783e-20\n",
            "Current Decoder Learning Rate: 5.551115123125783e-20\n",
            "Average Loss: 5m 18s (- 849m 39s) (620 0%) 6.2073\n",
            "Iteration: 620\n",
            "Loss: 5m 18s (- 850m 29s) (621 0%) 6.1680\n",
            "Iteration: 621\n",
            "Loss: 5m 19s (- 850m 20s) (622 0%) 6.2488\n",
            "Iteration: 622\n",
            "Loss: 5m 19s (- 850m 9s) (623 0%) 6.1661\n",
            "Iteration: 623\n",
            "Loss: 5m 20s (- 849m 59s) (624 0%) 6.2838\n",
            "Iteration: 624\n",
            "Loss: 5m 20s (- 849m 51s) (625 0%) 6.2044\n",
            "Iteration: 625\n",
            "Loss: 5m 21s (- 849m 45s) (626 0%) 6.2333\n",
            "Iteration: 626\n",
            "Loss: 5m 21s (- 849m 40s) (627 0%) 6.2030\n",
            "Iteration: 627\n",
            "Loss: 5m 22s (- 849m 34s) (628 0%) 6.0148\n",
            "Iteration: 628\n",
            "Loss: 5m 22s (- 849m 26s) (629 0%) 6.3450\n",
            "Iteration: 629\n",
            "Loss: 5m 23s (- 849m 20s) (630 0%) 6.1902\n",
            "loss_change:  0.0015601634979240941\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.7755575615628914e-20\n",
            "Current Decoder Learning Rate: 2.7755575615628914e-20\n",
            "Average Loss: 5m 23s (- 849m 20s) (630 0%) 6.2057\n",
            "Iteration: 630\n",
            "Loss: 5m 24s (- 850m 46s) (631 0%) 6.1975\n",
            "Iteration: 631\n",
            "Loss: 5m 24s (- 850m 37s) (632 0%) 6.3825\n",
            "Iteration: 632\n",
            "Loss: 5m 25s (- 850m 26s) (633 0%) 6.1353\n",
            "Iteration: 633\n",
            "Loss: 5m 25s (- 850m 25s) (634 0%) 6.1864\n",
            "Iteration: 634\n",
            "Loss: 5m 26s (- 850m 21s) (635 0%) 6.1414\n",
            "Iteration: 635\n",
            "Loss: 5m 26s (- 850m 13s) (636 0%) 6.1613\n",
            "Iteration: 636\n",
            "Loss: 5m 26s (- 850m 5s) (637 0%) 6.3110\n",
            "Iteration: 637\n",
            "Loss: 5m 27s (- 849m 58s) (638 0%) 6.2898\n",
            "Iteration: 638\n",
            "Loss: 5m 27s (- 849m 51s) (639 0%) 6.1067\n",
            "Iteration: 639\n",
            "Loss: 5m 28s (- 849m 45s) (640 0%) 6.1876\n",
            "loss_change:  -0.004216670989990234\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.3877787807814457e-20\n",
            "Current Decoder Learning Rate: 1.3877787807814457e-20\n",
            "Average Loss: 5m 28s (- 849m 45s) (640 0%) 6.2100\n",
            "Iteration: 640\n",
            "Loss: 5m 29s (- 851m 25s) (641 0%) 6.3114\n",
            "Iteration: 641\n",
            "Loss: 5m 30s (- 851m 16s) (642 0%) 6.2394\n",
            "Iteration: 642\n",
            "Loss: 5m 30s (- 851m 3s) (643 0%) 6.1258\n",
            "Iteration: 643\n",
            "Loss: 5m 30s (- 851m 0s) (644 0%) 6.1849\n",
            "Iteration: 644\n",
            "Loss: 5m 31s (- 850m 54s) (645 0%) 6.1967\n",
            "Iteration: 645\n",
            "Loss: 5m 31s (- 850m 50s) (646 0%) 6.2096\n",
            "Iteration: 646\n",
            "Loss: 5m 32s (- 850m 42s) (647 0%) 6.2992\n",
            "Iteration: 647\n",
            "Loss: 5m 32s (- 850m 36s) (648 0%) 6.2785\n",
            "Iteration: 648\n",
            "Loss: 5m 33s (- 850m 30s) (649 0%) 6.2548\n",
            "Iteration: 649\n",
            "Loss: 5m 33s (- 850m 20s) (650 0%) 6.1721\n",
            "loss_change:  -0.017289495468139293\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 6.938893903907229e-21\n",
            "Current Decoder Learning Rate: 6.938893903907229e-21\n",
            "Average Loss: 5m 33s (- 850m 20s) (650 0%) 6.2272\n",
            "Iteration: 650\n",
            "Loss: 5m 34s (- 851m 10s) (651 0%) 6.0571\n",
            "Iteration: 651\n",
            "Loss: 5m 35s (- 851m 1s) (652 0%) 6.0134\n",
            "Iteration: 652\n",
            "Loss: 5m 35s (- 850m 56s) (653 0%) 6.2589\n",
            "Iteration: 653\n",
            "Loss: 5m 36s (- 850m 51s) (654 0%) 6.3011\n",
            "Iteration: 654\n",
            "Loss: 5m 36s (- 850m 44s) (655 0%) 6.2210\n",
            "Iteration: 655\n",
            "Loss: 5m 37s (- 850m 40s) (656 0%) 6.2414\n",
            "Iteration: 656\n",
            "Loss: 5m 37s (- 850m 34s) (657 0%) 6.1569\n",
            "Iteration: 657\n",
            "Loss: 5m 38s (- 850m 30s) (658 0%) 6.3319\n",
            "Iteration: 658\n",
            "Loss: 5m 38s (- 850m 22s) (659 0%) 5.9800\n",
            "Iteration: 659\n",
            "Loss: 5m 38s (- 850m 13s) (660 0%) 6.1341\n",
            "loss_change:  0.05765271186828613\n",
            "Average Loss: 5m 38s (- 850m 13s) (660 0%) 6.1696\n",
            "Iteration: 660\n",
            "Loss: 5m 40s (- 851m 38s) (661 0%) 6.2679\n",
            "Iteration: 661\n",
            "Loss: 5m 40s (- 851m 32s) (662 0%) 6.2490\n",
            "Iteration: 662\n",
            "Loss: 5m 40s (- 851m 30s) (663 0%) 6.3711\n",
            "Iteration: 663\n",
            "Loss: 5m 41s (- 851m 24s) (664 0%) 6.2443\n",
            "Iteration: 664\n",
            "Loss: 5m 41s (- 851m 21s) (665 0%) 6.2989\n",
            "Iteration: 665\n",
            "Loss: 5m 42s (- 851m 14s) (666 0%) 6.1823\n",
            "Iteration: 666\n",
            "Loss: 5m 42s (- 851m 6s) (667 0%) 6.0323\n",
            "Iteration: 667\n",
            "Loss: 5m 43s (- 850m 58s) (668 0%) 6.1897\n",
            "Iteration: 668\n",
            "Loss: 5m 43s (- 850m 46s) (669 0%) 6.1435\n",
            "Iteration: 669\n",
            "Loss: 5m 44s (- 850m 38s) (670 0%) 6.2795\n",
            "loss_change:  -0.05626950263977015\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.469446951953614e-21\n",
            "Current Decoder Learning Rate: 3.469446951953614e-21\n",
            "Average Loss: 5m 44s (- 850m 38s) (670 0%) 6.2259\n",
            "Iteration: 670\n",
            "Loss: 5m 45s (- 851m 27s) (671 0%) 6.2054\n",
            "Iteration: 671\n",
            "Loss: 5m 45s (- 851m 15s) (672 0%) 6.0886\n",
            "Iteration: 672\n",
            "Loss: 5m 46s (- 851m 11s) (673 0%) 6.3352\n",
            "Iteration: 673\n",
            "Loss: 5m 46s (- 851m 1s) (674 0%) 6.2257\n",
            "Iteration: 674\n",
            "Loss: 5m 46s (- 850m 53s) (675 0%) 6.1874\n",
            "Iteration: 675\n",
            "Loss: 5m 47s (- 850m 48s) (676 0%) 6.1976\n",
            "Iteration: 676\n",
            "Loss: 5m 47s (- 850m 44s) (677 0%) 6.1755\n",
            "Iteration: 677\n",
            "Loss: 5m 48s (- 850m 38s) (678 0%) 6.2868\n",
            "Iteration: 678\n",
            "Loss: 5m 48s (- 850m 28s) (679 0%) 6.2371\n",
            "Iteration: 679\n",
            "Loss: 5m 49s (- 850m 19s) (680 0%) 6.2653\n",
            "loss_change:  0.005398178100585405\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.734723475976807e-21\n",
            "Current Decoder Learning Rate: 1.734723475976807e-21\n",
            "Average Loss: 5m 49s (- 850m 19s) (680 0%) 6.2205\n",
            "Iteration: 680\n",
            "Loss: 5m 50s (- 851m 9s) (681 0%) 6.1640\n",
            "Iteration: 681\n",
            "Loss: 5m 50s (- 851m 9s) (682 0%) 6.2112\n",
            "Iteration: 682\n",
            "Loss: 5m 51s (- 851m 3s) (683 0%) 6.3556\n",
            "Iteration: 683\n",
            "Loss: 5m 51s (- 851m 0s) (684 0%) 6.1263\n",
            "Iteration: 684\n",
            "Loss: 5m 52s (- 850m 52s) (685 0%) 6.3929\n",
            "Iteration: 685\n",
            "Loss: 5m 52s (- 850m 50s) (686 0%) 6.2726\n",
            "Iteration: 686\n",
            "Loss: 5m 53s (- 850m 37s) (687 0%) 5.9272\n",
            "Iteration: 687\n",
            "Loss: 5m 53s (- 850m 26s) (688 0%) 6.1906\n",
            "Iteration: 688\n",
            "Loss: 5m 53s (- 850m 15s) (689 0%) 6.2283\n",
            "Iteration: 689\n",
            "Loss: 5m 54s (- 850m 4s) (690 0%) 6.2538\n",
            "loss_change:  0.008205890655517578\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 8.673617379884036e-22\n",
            "Current Decoder Learning Rate: 8.673617379884036e-22\n",
            "Average Loss: 5m 54s (- 850m 4s) (690 0%) 6.2123\n",
            "Iteration: 690\n",
            "Loss: 5m 55s (- 850m 52s) (691 0%) 6.0394\n",
            "Iteration: 691\n",
            "Loss: 5m 55s (- 850m 51s) (692 0%) 6.2429\n",
            "Iteration: 692\n",
            "Loss: 5m 56s (- 850m 42s) (693 0%) 6.2235\n",
            "Iteration: 693\n",
            "Loss: 5m 56s (- 850m 38s) (694 0%) 6.4076\n",
            "Iteration: 694\n",
            "Loss: 5m 57s (- 850m 30s) (695 0%) 6.1007\n",
            "Iteration: 695\n",
            "Loss: 5m 57s (- 850m 24s) (696 0%) 6.1426\n",
            "Iteration: 696\n",
            "Loss: 5m 58s (- 850m 16s) (697 0%) 6.2667\n",
            "Iteration: 697\n",
            "Loss: 5m 58s (- 850m 10s) (698 0%) 6.2255\n",
            "Iteration: 698\n",
            "Loss: 5m 59s (- 850m 0s) (699 0%) 6.1731\n",
            "Iteration: 699\n",
            "Loss: 5m 59s (- 849m 50s) (700 0%) 6.1998\n",
            "loss_change:  0.010080862045287908\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 4.336808689942018e-22\n",
            "Current Decoder Learning Rate: 4.336808689942018e-22\n",
            "Average Loss: 5m 59s (- 849m 50s) (700 0%) 6.2022\n",
            "Iteration: 700\n",
            "Loss: 6m 0s (- 850m 39s) (701 0%) 6.1960\n",
            "Iteration: 701\n",
            "Loss: 6m 0s (- 850m 34s) (702 0%) 6.1625\n",
            "Iteration: 702\n",
            "Loss: 6m 1s (- 850m 26s) (703 0%) 6.1523\n",
            "Iteration: 703\n",
            "Loss: 6m 1s (- 850m 22s) (704 0%) 6.3139\n",
            "Iteration: 704\n",
            "Loss: 6m 2s (- 850m 19s) (705 0%) 6.2638\n",
            "Iteration: 705\n",
            "Loss: 6m 2s (- 850m 14s) (706 0%) 6.3032\n",
            "Iteration: 706\n",
            "Loss: 6m 3s (- 850m 9s) (707 0%) 6.3507\n",
            "Iteration: 707\n",
            "Loss: 6m 3s (- 850m 4s) (708 0%) 6.3678\n",
            "Iteration: 708\n",
            "Loss: 6m 4s (- 849m 53s) (709 0%) 6.0894\n",
            "Iteration: 709\n",
            "Loss: 6m 4s (- 849m 45s) (710 0%) 6.1385\n",
            "loss_change:  -0.03163785934448171\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.168404344971009e-22\n",
            "Current Decoder Learning Rate: 2.168404344971009e-22\n",
            "Average Loss: 6m 4s (- 849m 45s) (710 0%) 6.2338\n",
            "Iteration: 710\n",
            "Loss: 6m 5s (- 850m 34s) (711 0%) 6.1553\n",
            "Iteration: 711\n",
            "Loss: 6m 5s (- 850m 34s) (712 0%) 6.1841\n",
            "Iteration: 712\n",
            "Loss: 6m 6s (- 850m 28s) (713 0%) 6.1185\n",
            "Iteration: 713\n",
            "Loss: 6m 6s (- 850m 25s) (714 0%) 6.1610\n",
            "Iteration: 714\n",
            "Loss: 6m 7s (- 850m 19s) (715 0%) 6.0916\n",
            "Iteration: 715\n",
            "Loss: 6m 7s (- 850m 14s) (716 0%) 6.1688\n",
            "Iteration: 716\n",
            "Loss: 6m 8s (- 850m 8s) (717 0%) 6.3584\n",
            "Iteration: 717\n",
            "Loss: 6m 8s (- 849m 58s) (718 0%) 6.0832\n",
            "Iteration: 718\n",
            "Loss: 6m 9s (- 849m 50s) (719 0%) 6.3771\n",
            "Iteration: 719\n",
            "Loss: 6m 9s (- 849m 41s) (720 0%) 6.2434\n",
            "loss_change:  0.03966636657714773\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.0842021724855045e-22\n",
            "Current Decoder Learning Rate: 1.0842021724855045e-22\n",
            "Average Loss: 6m 9s (- 849m 41s) (720 0%) 6.1941\n",
            "Iteration: 720\n",
            "Loss: 6m 10s (- 850m 27s) (721 0%) 6.1931\n",
            "Iteration: 721\n",
            "Loss: 6m 11s (- 850m 28s) (722 0%) 6.2209\n",
            "Iteration: 722\n",
            "Loss: 6m 11s (- 850m 20s) (723 0%) 6.1880\n",
            "Iteration: 723\n",
            "Loss: 6m 12s (- 850m 16s) (724 0%) 6.1766\n",
            "Iteration: 724\n",
            "Loss: 6m 12s (- 850m 10s) (725 0%) 6.1800\n",
            "Iteration: 725\n",
            "Loss: 6m 13s (- 850m 6s) (726 0%) 6.1876\n",
            "Iteration: 726\n",
            "Loss: 6m 13s (- 849m 59s) (727 0%) 6.1518\n",
            "Iteration: 727\n",
            "Loss: 6m 13s (- 849m 55s) (728 0%) 6.1748\n",
            "Iteration: 728\n",
            "Loss: 6m 14s (- 849m 47s) (729 0%) 6.2488\n",
            "Iteration: 729\n",
            "Loss: 6m 14s (- 849m 38s) (730 0%) 6.2325\n",
            "loss_change:  -0.0012598991394039416\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 5.421010862427522e-23\n",
            "Current Decoder Learning Rate: 5.421010862427522e-23\n",
            "Average Loss: 6m 14s (- 849m 38s) (730 0%) 6.1954\n",
            "Iteration: 730\n",
            "Loss: 6m 15s (- 850m 23s) (731 0%) 6.3095\n",
            "Iteration: 731\n",
            "Loss: 6m 16s (- 850m 19s) (732 0%) 6.1139\n",
            "Iteration: 732\n",
            "Loss: 6m 16s (- 850m 17s) (733 0%) 6.3633\n",
            "Iteration: 733\n",
            "Loss: 6m 17s (- 850m 12s) (734 0%) 6.3344\n",
            "Iteration: 734\n",
            "Loss: 6m 17s (- 850m 9s) (735 0%) 6.1545\n",
            "Iteration: 735\n",
            "Loss: 6m 18s (- 850m 8s) (736 0%) 6.2740\n",
            "Iteration: 736\n",
            "Loss: 6m 18s (- 849m 59s) (737 0%) 6.0148\n",
            "Iteration: 737\n",
            "Loss: 6m 19s (- 849m 49s) (738 0%) 6.0988\n",
            "Iteration: 738\n",
            "Loss: 6m 19s (- 849m 40s) (739 0%) 6.2785\n",
            "Iteration: 739\n",
            "Loss: 6m 19s (- 849m 30s) (740 0%) 6.3400\n",
            "loss_change:  -0.03275694847106969\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.710505431213761e-23\n",
            "Current Decoder Learning Rate: 2.710505431213761e-23\n",
            "Average Loss: 6m 19s (- 849m 30s) (740 0%) 6.2282\n",
            "Iteration: 740\n",
            "Loss: 6m 20s (- 850m 19s) (741 0%) 6.2015\n",
            "Iteration: 741\n",
            "Loss: 6m 21s (- 850m 15s) (742 0%) 6.1508\n",
            "Iteration: 742\n",
            "Loss: 6m 21s (- 850m 9s) (743 0%) 6.2155\n",
            "Iteration: 743\n",
            "Loss: 6m 22s (- 850m 5s) (744 0%) 6.2812\n",
            "Iteration: 744\n",
            "Loss: 6m 22s (- 849m 59s) (745 0%) 6.1125\n",
            "Iteration: 745\n",
            "Loss: 6m 23s (- 849m 55s) (746 0%) 6.2741\n",
            "Iteration: 746\n",
            "Loss: 6m 23s (- 849m 50s) (747 0%) 6.1495\n",
            "Iteration: 747\n",
            "Loss: 6m 24s (- 849m 42s) (748 0%) 6.1837\n",
            "Iteration: 748\n",
            "Loss: 6m 24s (- 849m 34s) (749 0%) 6.1919\n",
            "Iteration: 749\n",
            "Loss: 6m 25s (- 849m 24s) (750 0%) 6.2861\n",
            "loss_change:  0.023504400253296254\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.3552527156068806e-23\n",
            "Current Decoder Learning Rate: 1.3552527156068806e-23\n",
            "Average Loss: 6m 25s (- 849m 24s) (750 0%) 6.2047\n",
            "Iteration: 750\n",
            "Loss: 6m 26s (- 850m 14s) (751 0%) 6.2647\n",
            "Iteration: 751\n",
            "Loss: 6m 26s (- 850m 11s) (752 0%) 6.0888\n",
            "Iteration: 752\n",
            "Loss: 6m 26s (- 850m 7s) (753 0%) 6.1489\n",
            "Iteration: 753\n",
            "Loss: 6m 27s (- 850m 5s) (754 0%) 6.1931\n",
            "Iteration: 754\n",
            "Loss: 6m 27s (- 850m 0s) (755 0%) 6.2541\n",
            "Iteration: 755\n",
            "Loss: 6m 28s (- 849m 58s) (756 0%) 6.2946\n",
            "Iteration: 756\n",
            "Loss: 6m 28s (- 849m 50s) (757 0%) 6.2007\n",
            "Iteration: 757\n",
            "Loss: 6m 29s (- 849m 42s) (758 0%) 6.3493\n",
            "Iteration: 758\n",
            "Loss: 6m 29s (- 849m 33s) (759 0%) 6.1676\n",
            "Iteration: 759\n",
            "Loss: 6m 30s (- 849m 25s) (760 0%) 6.1746\n",
            "loss_change:  -0.008964014053344904\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 6.776263578034403e-24\n",
            "Current Decoder Learning Rate: 6.776263578034403e-24\n",
            "Average Loss: 6m 30s (- 849m 26s) (760 0%) 6.2136\n",
            "Iteration: 760\n",
            "Loss: 6m 31s (- 851m 14s) (761 0%) 6.2103\n",
            "Iteration: 761\n",
            "Loss: 6m 32s (- 851m 11s) (762 0%) 6.2148\n",
            "Iteration: 762\n",
            "Loss: 6m 32s (- 851m 6s) (763 0%) 6.0837\n",
            "Iteration: 763\n",
            "Loss: 6m 33s (- 851m 1s) (764 0%) 6.1173\n",
            "Iteration: 764\n",
            "Loss: 6m 33s (- 850m 55s) (765 0%) 6.3462\n",
            "Iteration: 765\n",
            "Loss: 6m 34s (- 850m 45s) (766 0%) 6.2823\n",
            "Iteration: 766\n",
            "Loss: 6m 34s (- 850m 36s) (767 0%) 6.0806\n",
            "Iteration: 767\n",
            "Loss: 6m 34s (- 850m 29s) (768 0%) 6.2608\n",
            "Iteration: 768\n",
            "Loss: 6m 35s (- 850m 20s) (769 0%) 6.2274\n",
            "Iteration: 769\n",
            "Loss: 6m 35s (- 850m 12s) (770 0%) 6.1853\n",
            "loss_change:  0.012763357162476119\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 3.3881317890172014e-24\n",
            "Current Decoder Learning Rate: 3.3881317890172014e-24\n",
            "Average Loss: 6m 35s (- 850m 12s) (770 0%) 6.2009\n",
            "Iteration: 770\n",
            "Loss: 6m 36s (- 851m 25s) (771 0%) 6.2121\n",
            "Iteration: 771\n",
            "Loss: 6m 37s (- 851m 15s) (772 0%) 6.2276\n",
            "Iteration: 772\n",
            "Loss: 6m 37s (- 851m 6s) (773 0%) 6.1925\n",
            "Iteration: 773\n",
            "Loss: 6m 38s (- 850m 55s) (774 0%) 6.0770\n",
            "Iteration: 774\n",
            "Loss: 6m 38s (- 850m 44s) (775 0%) 6.1578\n",
            "Iteration: 775\n",
            "Loss: 6m 39s (- 850m 35s) (776 0%) 6.3180\n",
            "Iteration: 776\n",
            "Loss: 6m 39s (- 850m 25s) (777 0%) 6.2005\n",
            "Iteration: 777\n",
            "Loss: 6m 40s (- 850m 15s) (778 0%) 6.2272\n",
            "Iteration: 778\n",
            "Loss: 6m 40s (- 850m 6s) (779 0%) 6.3228\n",
            "Iteration: 779\n",
            "Loss: 6m 40s (- 850m 4s) (780 0%) 6.3311\n",
            "loss_change:  -0.025783777236938477\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 1.6940658945086007e-24\n",
            "Current Decoder Learning Rate: 1.6940658945086007e-24\n",
            "Average Loss: 6m 40s (- 850m 4s) (780 0%) 6.2266\n",
            "Iteration: 780\n",
            "Loss: 6m 42s (- 851m 20s) (781 0%) 6.0926\n",
            "Iteration: 781\n",
            "Loss: 6m 42s (- 851m 13s) (782 0%) 6.1715\n",
            "Iteration: 782\n",
            "Loss: 6m 43s (- 851m 7s) (783 0%) 6.2791\n",
            "Iteration: 783\n",
            "Loss: 6m 43s (- 851m 0s) (784 0%) 6.1213\n",
            "Iteration: 784\n",
            "Loss: 6m 43s (- 850m 53s) (785 0%) 6.3543\n",
            "Iteration: 785\n",
            "Loss: 6m 44s (- 850m 44s) (786 0%) 6.2675\n",
            "Iteration: 786\n",
            "Loss: 6m 44s (- 850m 34s) (787 0%) 6.1836\n",
            "Iteration: 787\n",
            "Loss: 6m 45s (- 850m 24s) (788 0%) 6.0880\n",
            "Iteration: 788\n",
            "Loss: 6m 45s (- 850m 18s) (789 0%) 6.1481\n",
            "Iteration: 789\n",
            "Loss: 6m 46s (- 850m 12s) (790 0%) 6.3288\n",
            "loss_change:  0.02317047119140625\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 8.470329472543004e-25\n",
            "Current Decoder Learning Rate: 8.470329472543004e-25\n",
            "Average Loss: 6m 46s (- 850m 13s) (790 0%) 6.2035\n",
            "Iteration: 790\n",
            "Loss: 6m 47s (- 851m 24s) (791 0%) 6.2080\n",
            "Iteration: 791\n",
            "Loss: 6m 47s (- 851m 18s) (792 0%) 6.1447\n",
            "Iteration: 792\n",
            "Loss: 6m 48s (- 851m 15s) (793 0%) 6.1224\n",
            "Iteration: 793\n",
            "Loss: 6m 48s (- 851m 5s) (794 0%) 6.1631\n",
            "Iteration: 794\n",
            "Loss: 6m 49s (- 850m 56s) (795 0%) 6.2304\n",
            "Iteration: 795\n",
            "Loss: 6m 49s (- 850m 48s) (796 0%) 6.1354\n",
            "Iteration: 796\n",
            "Loss: 6m 50s (- 850m 40s) (797 0%) 6.3247\n",
            "Iteration: 797\n",
            "Loss: 6m 50s (- 850m 40s) (798 0%) 6.2043\n",
            "Iteration: 798\n",
            "Loss: 6m 51s (- 850m 38s) (799 0%) 6.2822\n",
            "Iteration: 799\n",
            "Loss: 6m 51s (- 850m 31s) (800 0%) 6.0890\n",
            "loss_change:  0.013057184219359641\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 4.235164736271502e-25\n",
            "Current Decoder Learning Rate: 4.235164736271502e-25\n",
            "Average Loss: 6m 51s (- 850m 31s) (800 0%) 6.1904\n",
            "Iteration: 800\n",
            "Loss: 6m 52s (- 851m 45s) (801 0%) 6.2894\n",
            "Iteration: 801\n",
            "Loss: 6m 53s (- 851m 41s) (802 0%) 6.2293\n",
            "Iteration: 802\n",
            "Loss: 6m 53s (- 851m 32s) (803 0%) 6.4076\n",
            "Iteration: 803\n",
            "Loss: 6m 54s (- 851m 21s) (804 0%) 6.2161\n",
            "Iteration: 804\n",
            "Loss: 6m 54s (- 851m 13s) (805 0%) 6.1801\n",
            "Iteration: 805\n",
            "Loss: 6m 54s (- 851m 2s) (806 0%) 6.1182\n",
            "Iteration: 806\n",
            "Loss: 6m 55s (- 850m 55s) (807 0%) 6.2248\n",
            "Iteration: 807\n",
            "Loss: 6m 55s (- 850m 50s) (808 0%) 6.1927\n",
            "Iteration: 808\n",
            "Loss: 6m 56s (- 850m 44s) (809 0%) 6.3110\n",
            "Iteration: 809\n",
            "Loss: 6m 56s (- 850m 40s) (810 0%) 6.2038\n",
            "loss_change:  -0.04689140319824148\n",
            "Learning Rate Decays:\n",
            "Current Encoder Learning Rate: 2.117582368135751e-25\n",
            "Current Decoder Learning Rate: 2.117582368135751e-25\n",
            "Average Loss: 6m 56s (- 850m 41s) (810 0%) 6.2373\n",
            "Iteration: 810\n",
            "Loss: 6m 57s (- 851m 36s) (811 0%) 6.2669\n",
            "Iteration: 811\n",
            "Loss: 6m 58s (- 851m 33s) (812 0%) 6.3426\n",
            "Iteration: 812\n",
            "Loss: 6m 58s (- 851m 28s) (813 0%) 5.9839\n",
            "Iteration: 813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HqETPpIHny8i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evalutation"
      ]
    },
    {
      "metadata": {
        "id": "0kMYuG5dAVd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, iteration_num):\n",
        "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
        "    folder_path = os.getcwd() + '/gdrive/My Drive/NLP_Project/'\n",
        "    start_epoch = 0\n",
        "    filename=folder_path+\"model_saved/state_{}.pt\".format(iteration_num)\n",
        "    if os.path.isfile(filename):\n",
        "        print(\"=> loading checkpoint '{}'\".format(iteration_num))\n",
        "        checkpoint = torch.load(filename, map_location=device)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        #model.load_state_dict(checkpoint['state_dict'])\n",
        "        encoder.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
        "        decoder.load_state_dict(checkpoint[\"decoder_state_dict\"])\n",
        "        encoder_optimizer.load_state_dict(checkpoint[\"encoder_optimizer\"])\n",
        "        decoder_optimizer.load_state_dict(checkpoint[\"decoder_optimizer\"])\n",
        "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        #losslogger = checkpoint['losslogger']\n",
        "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(filename, checkpoint['epoch']))\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "\n",
        "    return start_epoch, encoder, decoder, encoder_optimizer, decoder_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LgEZtBTossgP",
        "colab_type": "code",
        "outputId": "3e187043-3f92-4712-de6e-9c159ae00ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "cell_type": "code",
      "source": [
        "start_epoch, encoder, decoder, encoder_optimizer, decoder_optimizer=\\\n",
        "load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, 190)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-da717e0fc166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m190\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uy1diI64ASWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_randomly(pairs, input_lang, output_lang):\n",
        "    [input_sentence, target_sentence] = random.choice(pairs)\n",
        "    evaluate_and_show_attention(input_sentence, input_lang, output_lang, target_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pRqWnSsHATib",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_attention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    show_plot_visdom()\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oANXKAONAVoo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_and_show_attention(input_sentence, input_lang, output_lang, target_sentence=None):\n",
        "    output_words, attentions = evaluate(input_sentence, input_lang, output_lang)\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    print('>', input_sentence)\n",
        "    if target_sentence is not None:\n",
        "        print('=', target_sentence)\n",
        "    print('<', output_sentence)\n",
        "    \n",
        "#     show_attention(input_sentence, output_words, attentions)\n",
        "    \n",
        "#     # Show input, target, output text in visdom\n",
        "#     win = 'evaluted (%s)' % hostname\n",
        "#     text = '<p>&gt; %s</p><p>= %s</p><p>&lt; %s</p>' % (input_sentence, target_sentence, output_sentence)\n",
        "#     vis.text(text, win=win, opts={'title': win})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4UW222G7AnCj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexes_from_sentence(lang, sentence):\n",
        "    index_list = []\n",
        "    for word in sentence.split(' '):\n",
        "      if (word in lang.word2index.keys()):\n",
        "        index = lang.word2index[word] \n",
        "      else:\n",
        "        index = UNK_IDX\n",
        "      index_list.append(index)\n",
        "    \n",
        "    return index_list + [EOS_token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXexOcvaAWxj",
        "colab_type": "code",
        "outputId": "f202e99b-30cd-4ca8-94da-6a1e9e3e67e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1881
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate_randomly(train_pairs, train_input_lang, train_output_lang)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "那 是 妳 的 一部 一部分 部分\n",
            "tensor([[ 110],\n",
            "        [   9],\n",
            "        [8050],\n",
            "        [   5],\n",
            "        [   3],\n",
            "        [6929],\n",
            "        [ 286],\n",
            "        [   1]])\n",
            "[17]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-6d6c2678e3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_randomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-6e0e502e984b>\u001b[0m in \u001b[0;36mevaluate_randomly\u001b[0;34m(pairs, input_lang, output_lang)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_randomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mevaluate_and_show_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-123-0ec2bb566402>\u001b[0m in \u001b[0;36mevaluate_and_show_attention\u001b[0;34m(input_sentence, input_lang, output_lang, target_sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_and_show_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_sentence\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-1f06e8c269c0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(input_seq, input_lang, output_lang, max_length)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Run through encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Create starting vectors for decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c256712bdd7e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seqs, input_len, hidden)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflat_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb_ih\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mi_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cannot unsqueeze empty tensor"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xEgPGz6kGbky",
        "colab_type": "code",
        "outputId": "8e06228b-0e2f-4508-a5e0-8f8a3840499d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "AlXBk0f1AZ0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(input_seq, input_lang, output_lang, max_length=MAX_LENGTH):\n",
        "\n",
        "    \n",
        "    \n",
        "    input_seqs = [indexesFromSentence(input_lang, input_seq)]\n",
        "    \n",
        "    input_lengths = [len(input_seq.split())]\n",
        "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
        "    \n",
        "    input_batches = input_batches.to(device)\n",
        "\n",
        "    # Set to not-training mode to disable dropout\n",
        "    encoder.train(False)\n",
        "    decoder.train(False)\n",
        "    # Run through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
        "   \n",
        "    \n",
        "\n",
        "    # Create starting vectors for decoder\n",
        "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
        "    print (encoder_hidden.shape)\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
        "    \n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Store output words and attention states\n",
        "    decoded_words = []\n",
        "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
        "    #return decoder_input, decoder_attentions\n",
        "    # Run through decoder\n",
        "#     print(decoder_input)\n",
        "#     print(decoder_hidden.shape)\n",
        "    \n",
        "    for di in range(max_length):\n",
        "      \n",
        "        print(decoder_input)\n",
        "        print(decoder_hidden.shape)\n",
        "        \n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs\n",
        "        )\n",
        "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
        "\n",
        "        # Choose top word from output\n",
        "        topv, topi = decoder_output.data.topk(1)\n",
        "        ni = topi[0][0]\n",
        "        print(ni.data.tolist())\n",
        "        if ni == EOS_token:\n",
        "            decoded_words.append('<EOS>')\n",
        "            break\n",
        "        else:\n",
        "            decoded_words.append(output_lang.index2word[ni.data.tolist()])\n",
        "            \n",
        "        # Next input is chosen word\n",
        "        decoder_input = Variable(torch.LongTensor([ni]))\n",
        "        decoder_input = decoder_input.to(device)\n",
        "    # Set back to training mode\n",
        "    encoder.train(True)\n",
        "    decoder.train(True)\n",
        "    \n",
        "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kdZC3C7SZX85",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(64):\n",
        "  [input_sentence, target_sentence] = random.choice(train_pairs)\n",
        "  input_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XuvyoZVEGFpD",
        "colab_type": "code",
        "outputId": "4342d657-68cd-451d-9fa3-a09288d1c905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "#[input_sentence, target_sentence] = random.choice(train_pairs)\n",
        "input_sentence=\"我 爱 你\"\n",
        "input_batches, input_lengths=evaluate(input_sentence, train_input_lang, train_output_lang, beam_size=3, max_length=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-87138c4f88b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"我 爱 你\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: evaluate() got an unexpected keyword argument 'beam_size'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "kJfuedE4AFlq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def old_evaluate(input_seq, input_lang, output_lang, max_length=MAX_LENGTH):\n",
        "    print (input_seq)\n",
        "    input_lengths = [len(input_seq)]\n",
        "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
        "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
        "    \n",
        "    input_batches = input_batches.to(device)\n",
        "        \n",
        "    # Set to not-training mode to disable dropout\n",
        "    encoder.train(False)\n",
        "    decoder.train(False)\n",
        "    print (input_batches)\n",
        "    print (input_lengths)\n",
        "    # Run through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
        "\n",
        "    # Create starting vectors for decoder\n",
        "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
        "    \n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Store output words and attention states\n",
        "    decoded_words = []\n",
        "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
        "    \n",
        "    # Run through decoder\n",
        "    for di in range(max_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs\n",
        "        )\n",
        "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
        "\n",
        "        # Choose top word from output\n",
        "        topv, topi = decoder_output.data.topk(1)\n",
        "        ni = topi[0][0]\n",
        "        if ni == EOS_token:\n",
        "            decoded_words.append('<EOS>')\n",
        "            break\n",
        "        else:\n",
        "            decoded_words.append(output_lang.index2word[ni])\n",
        "            \n",
        "        # Next input is chosen word\n",
        "        decoder_input = Variable(torch.LongTensor([ni]))\n",
        "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
        "\n",
        "    # Set back to training mode\n",
        "    encoder.train(True)\n",
        "    decoder.train(True)\n",
        "    \n",
        "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkGFBQryG6bJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input_seq = \"你\"\n",
        "# input_lang = train_input_lang \n",
        "# output_lang = train_output_lang \n",
        "# max_length = 1\n",
        "# beam_size = 10\n",
        "\n",
        "def to_output_lang(output_list):\n",
        "  result=[]\n",
        "  for token_index in output_list:\n",
        "    token=output_lang.index2word[token_index]\n",
        "    result.append(token)\n",
        "  return result\n",
        "\n",
        "def evaluate_beam_search(encoder, decoder, input_seq, input_lang, output_lang, max_length=MAX_LENGTH, beam_size = 10):\n",
        "  with torch.no_grad():\n",
        "\n",
        "    input_seqs = [indexesFromSentence(input_lang, input_seq)]\n",
        "    #print (input_seqs)\n",
        "    input_lengths = [len(input_seq.split())]\n",
        "    input_batches = Variable(torch.LongTensor(input_seqs)).transpose(0, 1)\n",
        "\n",
        "    input_batches = input_batches.to(device)\n",
        "\n",
        "    # Set to not-training mode to disable dropout\n",
        "    encoder.train(False)\n",
        "    decoder.train(False)\n",
        "    # Run through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
        "\n",
        "\n",
        "\n",
        "    # Create starting vectors for decoder\n",
        "    #decoder_input = Variable(torch.LongTensor([SOS_token])) # SOS\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
        "\n",
        "    sequences=[[[SOS_token], 1.0]]\n",
        "    decoder_attentions = torch.zeros(max_length, max_length)\n",
        "    for di in range(max_length):\n",
        "\n",
        "      for sequence in sequences:\n",
        "        sequence_list, score = sequence\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        for word in sequence_list:\n",
        "          #print (\"word:\", word)\n",
        "          word = Variable(torch.LongTensor([word])).to(device)\n",
        "          decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "          word, decoder_hidden, encoder_outputs\n",
        "        )\n",
        "        \n",
        "        #decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
        "\n",
        "        #print (\"decoder_output.shape\", decoder_output.shape)\n",
        "        output_prob=F.softmax(decoder_output.data)\n",
        "        topv, topi = output_prob.topk(30000)\n",
        "        candidates=[]\n",
        "        for i in range (30000):\n",
        "          prob = topv[0][i].data.tolist()\n",
        "          toekn = [topi[0][i].data.tolist()]\n",
        "          candidates.append([sequence_list+toekn, prob*score])\n",
        "\n",
        "        sequences=sorted(candidates, key=lambda tup: tup[1], reverse=True)[0:beam_size]\n",
        "        sequences_word=[[to_output_lang(x[0]), x[1]] for x in sequences]\n",
        "  return sequences_word\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vO6C8Hgwiv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index else UNK_IDX for word in sentence.split(' ')] + [EOS_token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gDWrJ6PsA43",
        "colab_type": "code",
        "outputId": "433f435d-9123-4f22-a2e1-7ae465dce627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "cell_type": "code",
      "source": [
        "input_seq = \"我 讨厌 你\"\n",
        "input_lang = train_input_lang \n",
        "output_lang = train_output_lang \n",
        "max_length = 7\n",
        "beam_size = 3\n",
        "encoder.batch_size=1\n",
        "decoder.batch_size=1\n",
        "evaluate_beam_search(encoder, decoder, input_seq, input_lang, output_lang, max_length=5, beam_size = 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['<EOS>', 'they', 'they', 'they', 'they', 'a'], 1.3731330188136913e-05],\n",
              " [['<EOS>', 'they', 'they', 'they', 'they', '<PAD>'], 4.5850020912157005e-06],\n",
              " [['<EOS>', 'they', 'they', 'they', 'they', 'they'], 4.5684426458255714e-06]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "JXkVZRsd8K07",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_and_show_attention(input_sentence, input_lang, output_lang, target_sentence=None):\n",
        "    print('>', input_sentence)\n",
        "    \n",
        "    if target_sentence is not None:\n",
        "        print('=', target_sentence)\n",
        "    \n",
        "    sequences_word = evaluate_beam_search(encoder, decoder, input_sentence, input_lang, output_lang, max_length=10, beam_size = 4)\n",
        "    \n",
        "    output_sentence = ' '.join(sequences_word[0][0])\n",
        "\n",
        "    print('<', output_sentence)\n",
        "    \n",
        "#     show_attention(input_sentence, output_words, attentions)\n",
        "    \n",
        "#     # Show input, target, output text in visdom\n",
        "#     win = 'evaluted (%s)' % hostname\n",
        "#     text = '<p>&gt; %s</p><p>= %s</p><p>&lt; %s</p>' % (input_sentence, target_sentence, output_sentence)\n",
        "#     vis.text(text, win=win, opts={'title': win})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TE8sI5RTKct6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_randomly(pairs, input_lang, output_lang):\n",
        "    [input_sentence, target_sentence] = random.choice(pairs)\n",
        "    evaluate_and_show_attention(input_sentence, input_lang, output_lang, target_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KeAqmvA18mGL",
        "colab_type": "code",
        "outputId": "239a428b-31a5-433a-eccd-95838c7ca5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate_randomly(train_pairs, train_input_lang, train_output_lang)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> 他们 会 变成 这样 的 消费 消费者    和 你 我 一样   一无所知 无所 所知 的 消费 消费者\n",
            "= They d go back to being consumers clueless consumers like we are most of the time .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "< <SOS> of of of of of of of of of a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aCpeGthD8rCy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "from detok import detok\n",
        "import numpy as np\n",
        "\n",
        "def bleu(itos, translation_output, reference):\n",
        "    '''\n",
        "    Args:\n",
        "        arg.vocab.itos: a list the match indices to string.\n",
        "        translation_output: 2D tensor of tranlation output. shape: N x B\n",
        "        reference: 1D list of reference sentences (words, not indices). len(reference) = B\n",
        "    '''\n",
        "    EN_ind2word = np.array(itos)\n",
        "    detok_translation = detok(translation_output, EN_ind2word)\n",
        "    bleu_score = sacrebleu.raw_corpus_bleu(detok_translation, [reference], .01).score\n",
        "    \n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "def bleu_epoch(itos, translation_outputs, reference):\n",
        "    '''\n",
        "    Args:\n",
        "        trg.vocab.itos: a list the match indices to string.\n",
        "        translation_output: 2D tensor of tranlation output. shape: N x B\n",
        "        reference: 1D list of reference sentences (words, not indices). len(reference) = B\n",
        "    '''\n",
        "    EN_ind2word = np.array(itos)\n",
        "    detok_translation = []\n",
        "    for translation_output in translation_outputs:\n",
        "        detok_translation.extend(detok(translation_output, EN_ind2word))\n",
        "    bleu_score = sacrebleu.raw_corpus_bleu(detok_translation, [reference], .01).score\n",
        "    \n",
        "\n",
        "    return bleu_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KFgahQuvfyzn",
        "colab_type": "code",
        "outputId": "6172bf01-b7df-44ff-8079-cefd1ab5039f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "cell_type": "code",
      "source": [
        "def tokenize_13a(line):\n",
        "    \"\"\"\n",
        "    Tokenizes an input line using a relatively minimal tokenization that is however equivalent to mteval-v13a, used by WMT.\n",
        "    :param line: a segment to tokenize\n",
        "    :return: the tokenized line\n",
        "    \"\"\"\n",
        "\n",
        "    norm = line\n",
        "\n",
        "    # language-independent part:\n",
        "    norm = norm.replace('<skipped>', '')\n",
        "    norm = norm.replace('-\\n', '')\n",
        "    norm = norm.replace('\\n', ' ')\n",
        "    norm = norm.replace('&quot;', '\"')\n",
        "    norm = norm.replace('&amp;', '&')\n",
        "    norm = norm.replace('&lt;', '<')\n",
        "    norm = norm.replace('&gt;', '>')\n",
        "\n",
        "    # language-dependent part (assuming Western languages):\n",
        "    norm = \" {} \".format(norm)\n",
        "    norm = re.sub(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', ' \\\\1 ', norm)\n",
        "    norm = re.sub(r'([^0-9])([\\.,])', '\\\\1 \\\\2 ', norm)  # tokenize period and comma unless preceded by a digit\n",
        "    norm = re.sub(r'([\\.,])([^0-9])', ' \\\\1 \\\\2', norm)  # tokenize period and comma unless followed by a digit\n",
        "    norm = re.sub(r'([0-9])(-)', '\\\\1 \\\\2 ', norm)  # tokenize dash when preceded by a digit\n",
        "    norm = re.sub(r'\\s+', ' ', norm)  # one space only between words\n",
        "    norm = re.sub(r'^\\s+', '', norm)  # no leading space\n",
        "    norm = re.sub(r'\\s+$', '', norm)  # no trailing space\n",
        "\n",
        "    return norm\n",
        "TOKENIZERS = {\n",
        "    '13a': tokenize_13a,\n",
        "\n",
        "}\n",
        "DEFAULT_TOKENIZER = '13a'\n",
        "\n",
        "def corpus_bleu(sys_stream, ref_streams, smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n",
        "                tokenize=DEFAULT_TOKENIZER, use_effective_order=False) -> BLEU:\n",
        "    \"\"\"Produces BLEU scores along with its sufficient statistics from a source against one or more references.\n",
        "    :param sys_stream: The system stream (a sequence of segments)\n",
        "    :param ref_streams: A list of one or more reference streams (each a sequence of segments)\n",
        "    :param smooth: The smoothing method to use\n",
        "    :param smooth_floor: For 'floor' smoothing, the floor to use\n",
        "    :param force: Ignore data that looks already tokenized\n",
        "    :param lowercase: Lowercase the data\n",
        "    :param tokenize: The tokenizer to use\n",
        "    :return: a BLEU object containing everything you'd want\n",
        "    \"\"\"\n",
        "\n",
        "    # Add some robustness to the input arguments\n",
        "    if isinstance(sys_stream, str):\n",
        "        sys_stream = [sys_stream]\n",
        "    if isinstance(ref_streams, str):\n",
        "        ref_streams = [[ref_streams]]\n",
        "\n",
        "    sys_len = 0\n",
        "    ref_len = 0\n",
        "\n",
        "    correct = [0 for n in range(NGRAM_ORDER)]\n",
        "    total = [0 for n in range(NGRAM_ORDER)]\n",
        "\n",
        "    # look for already-tokenized sentences\n",
        "    tokenized_count = 0\n",
        "\n",
        "    fhs = [sys_stream] + ref_streams\n",
        "    for lines in zip_longest(*fhs):\n",
        "        if None in lines:\n",
        "            raise EOFError(\"Source and reference streams have different lengths!\")\n",
        "\n",
        "        if lowercase:\n",
        "            lines = [x.lower() for x in lines]\n",
        "\n",
        "        if not (force or tokenize == 'none') and lines[0].rstrip().endswith(' .'):\n",
        "            tokenized_count += 1\n",
        "\n",
        "            if tokenized_count == 100:\n",
        "                logging.warning('That\\'s 100 lines that end in a tokenized period (\\'.\\')')\n",
        "                logging.warning('It looks like you forgot to detokenize your test data, which may hurt your score.')\n",
        "                logging.warning('If you insist your data is detokenized, or don\\'t care, you can suppress this message with \\'--force\\'.')\n",
        "\n",
        "        output, *refs = [TOKENIZERS[tokenize](x.rstrip()) for x in lines]\n",
        "\n",
        "        ref_ngrams, closest_diff, closest_len = ref_stats(output, refs)\n",
        "\n",
        "        sys_len += len(output.split())\n",
        "        ref_len += closest_len\n",
        "\n",
        "        sys_ngrams = extract_ngrams(output)\n",
        "        for ngram in sys_ngrams.keys():\n",
        "            n = len(ngram.split())\n",
        "            correct[n-1] += min(sys_ngrams[ngram], ref_ngrams.get(ngram, 0))\n",
        "            total[n-1] += sys_ngrams[ngram]\n",
        "\n",
        "    return compute_bleu(correct, total, sys_len, ref_len, smooth, smooth_floor, use_effective_order)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-5d62eba7cd67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m def corpus_bleu(sys_stream, ref_streams, smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n\u001b[0;32m---> 37\u001b[0;31m                 tokenize=DEFAULT_TOKENIZER, use_effective_order=False) -> BLEU:\n\u001b[0m\u001b[1;32m     38\u001b[0m     \"\"\"Produces BLEU scores along with its sufficient statistics from a source against one or more references.\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0msys_stream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msystem\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0msequence\u001b[0m \u001b[0mof\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BLEU' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "A6RvLRZVf00V",
        "colab_type": "code",
        "outputId": "b87d8a6c-8789-4a38-95ac-30c75b715792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#The BLEU score consists of two parts, modified precision and brevity penalty. Details can be seen in the paper. You can use the nltk.align.bleu_score module inside the NLTK. One code example can be seen as below:\n",
        "\n",
        "import nltk\n",
        "\n",
        "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
        "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
        "#there may be several references\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "print (BLEUscore)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4548019047027907\n",
            "0.816496580927726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XiDbzWMwhmuM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}