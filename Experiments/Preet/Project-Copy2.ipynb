{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import functional\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "MAX_LENGTH = 5 #temp\n",
    "\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "PAD_IDX = 0 \n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_IDX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(torch.LongTensor(length))\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = functional.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "def removePunctuation(s):\n",
    "\n",
    "    to_remove = ('&lt;', '&gt;', '&amp;', '&apos;', '&quot;')\n",
    "    table = str.maketrans(dict.fromkeys('.!?:,'))\n",
    "    s = s.translate(table)\n",
    "    for i in to_remove:\n",
    "        s=s.replace(i,'')   \n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from collections import Counter, namedtuple\n",
    "from itertools import zip_longest\n",
    "\n",
    "def tokenize_13a(line):\n",
    "    \"\"\"\n",
    "    Tokenizes an input line using a relatively minimal tokenization that is however equivalent to mteval-v13a, used by WMT.\n",
    "    :param line: a segment to tokenize\n",
    "    :return: the tokenized line\n",
    "    \"\"\"\n",
    "\n",
    "    norm = line\n",
    "\n",
    "    # language-independent part:\n",
    "    norm = norm.replace('<skipped>', '')\n",
    "    norm = norm.replace('-\\n', '')\n",
    "    norm = norm.replace('\\n', ' ')\n",
    "    norm = norm.replace('&quot;', '\"')\n",
    "    norm = norm.replace('&amp;', '&')\n",
    "    norm = norm.replace('&lt;', '<')\n",
    "    norm = norm.replace('&gt;', '>')\n",
    "\n",
    "    # language-dependent part (assuming Western languages):\n",
    "    norm = \" {} \".format(norm)\n",
    "    norm = re.sub(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', ' \\\\1 ', norm)\n",
    "    norm = re.sub(r'([^0-9])([\\.,])', '\\\\1 \\\\2 ', norm)  # tokenize period and comma unless preceded by a digit\n",
    "    norm = re.sub(r'([\\.,])([^0-9])', ' \\\\1 \\\\2', norm)  # tokenize period and comma unless followed by a digit\n",
    "    norm = re.sub(r'([0-9])(-)', '\\\\1 \\\\2 ', norm)  # tokenize dash when preceded by a digit\n",
    "    norm = re.sub(r'\\s+', ' ', norm)  # one space only between words\n",
    "    norm = re.sub(r'^\\s+', '', norm)  # no leading space\n",
    "    norm = re.sub(r'\\s+$', '', norm)  # no trailing space\n",
    "\n",
    "    return norm\n",
    "\n",
    "def corpus_bleu(sys_stream, ref_streams, smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n",
    "                 use_effective_order=False):\n",
    "    \"\"\"Produces BLEU scores along with its sufficient statistics from a source against one or more references.\n",
    "    :param sys_stream: The system stream (a sequence of segments)\n",
    "    :param ref_streams: A list of one or more reference streams (each a sequence of segments)\n",
    "    :param smooth: The smoothing method to use\n",
    "    :param smooth_floor: For 'floor' smoothing, the floor to use\n",
    "    :param force: Ignore data that looks already tokenized\n",
    "    :param lowercase: Lowercase the data\n",
    "    :param tokenize: The tokenizer to use\n",
    "    :return: a BLEU object containing everything you'd want\n",
    "    \"\"\"\n",
    "\n",
    "    # Add some robustness to the input arguments\n",
    "    if isinstance(sys_stream, str):\n",
    "        sys_stream = [sys_stream]\n",
    "    if isinstance(ref_streams, str):\n",
    "        ref_streams = [[ref_streams]]\n",
    "\n",
    "    sys_len = 0\n",
    "    ref_len = 0\n",
    "\n",
    "    correct = [0 for n in range(NGRAM_ORDER)]\n",
    "    total = [0 for n in range(NGRAM_ORDER)]\n",
    "    \n",
    "\n",
    "    # look for already-tokenized sentences\n",
    "    tokenized_count = 0\n",
    "\n",
    "    fhs = [sys_stream] + ref_streams\n",
    "    for lines in zip_longest(*fhs):\n",
    "        if None in lines:\n",
    "            raise EOFError(\"Source and reference streams have different lengths!\")\n",
    "\n",
    "        if lowercase:\n",
    "            lines = [x.lower() for x in lines]\n",
    "            \n",
    "        tokenize= 'tokenize_13a'    \n",
    "\n",
    "        if not (force or tokenize == 'none') and lines[0].rstrip().endswith(' .'):\n",
    "            tokenized_count += 1\n",
    "\n",
    "            if tokenized_count == 100:\n",
    "                logging.warning('That\\'s 100 lines that end in a tokenized period (\\'.\\')')\n",
    "                logging.warning('It looks like you forgot to detokenize your test data, which may hurt your score.')\n",
    "                logging.warning('If you insist your data is detokenized, or don\\'t care, you can suppress this message with \\'--force\\'.')\n",
    "\n",
    "        output, *refs = [tokenize_13a(x.rstrip()) for x in lines]\n",
    "        \n",
    "\n",
    "        ref_ngrams, closest_diff, closest_len = ref_stats(output, refs)\n",
    "        \n",
    "\n",
    "        sys_len += len(output.split())\n",
    "        ref_len += closest_len\n",
    "\n",
    "        sys_ngrams = extract_ngrams(output)\n",
    "        for ngram in sys_ngrams.keys():\n",
    "            n = len(ngram.split())\n",
    "            correct[n-1] += min(sys_ngrams[ngram], ref_ngrams.get(ngram, 0))\n",
    "            total[n-1] += sys_ngrams[ngram]\n",
    "            \n",
    "\n",
    "    return compute_bleu(correct, total, sys_len, ref_len, smooth, smooth_floor, use_effective_order)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_ORDER = 4\n",
    "  \n",
    "def compute_bleu(correct: List[int], total: List[int], sys_len: int, ref_len: int, smooth = 'none', smooth_floor = 0.01,\n",
    "                 use_effective_order = False):\n",
    "    \"\"\"Computes BLEU score from its sufficient statistics. Adds smoothing.\n",
    "    :param correct: List of counts of correct ngrams, 1 <= n <= NGRAM_ORDER\n",
    "    :param total: List of counts of total ngrams, 1 <= n <= NGRAM_ORDER\n",
    "    :param sys_len: The cumulative system length\n",
    "    :param ref_len: The cumulative reference length\n",
    "    :param smooth: The smoothing method to use\n",
    "    :param smooth_floor: The smoothing value added, if smooth method 'floor' is used\n",
    "    :param use_effective_order: Use effective order.\n",
    "    :return: A BLEU object with the score (100-based) and other statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    precisions = [0 for x in range(NGRAM_ORDER)]\n",
    "\n",
    "    smooth_mteval = 1.\n",
    "    effective_order = NGRAM_ORDER\n",
    "    for n in range(NGRAM_ORDER):\n",
    "        if total[n] == 0:\n",
    "            break\n",
    "\n",
    "        if use_effective_order:\n",
    "            effective_order = n + 1\n",
    "\n",
    "        if correct[n] == 0:\n",
    "            if smooth == 'exp':\n",
    "                smooth_mteval *= 2\n",
    "                precisions[n] = 100. / (smooth_mteval * total[n])\n",
    "            elif smooth == 'floor':\n",
    "                precisions[n] = 100. * smooth_floor / total[n]\n",
    "        else:\n",
    "            precisions[n] = 100. * correct[n] / total[n]\n",
    "\n",
    "    # If the system guesses no i-grams, 1 <= i <= NGRAM_ORDER, the BLEU score is 0 (technically undefined).\n",
    "    # This is a problem for sentence-level BLEU or a corpus of short sentences, where systems will get no credit\n",
    "    # if sentence lengths fall under the NGRAM_ORDER threshold. This fix scales NGRAM_ORDER to the observed\n",
    "    # maximum order. It is only available through the API and off by default\n",
    "\n",
    "    brevity_penalty = 1.0\n",
    "    if sys_len < ref_len:\n",
    "        brevity_penalty = math.exp(1 - ref_len / sys_len) if sys_len > 0 else 0.0\n",
    "        \n",
    "\n",
    "    bleu = brevity_penalty * math.exp(sum(map(my_log, precisions[:effective_order])) / effective_order)\n",
    "\n",
    "    return bleu \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def ref_stats(output, refs):\n",
    "    ngrams = Counter()\n",
    "    closest_diff = None\n",
    "    closest_len = None\n",
    "    for ref in refs:\n",
    "        tokens = ref.split()\n",
    "        reflen = len(tokens)\n",
    "        diff = abs(len(output.split()) - reflen)\n",
    "        if closest_diff is None or diff < closest_diff:\n",
    "            closest_diff = diff\n",
    "            closest_len = reflen\n",
    "        elif diff == closest_diff:\n",
    "            if reflen < closest_len:\n",
    "                closest_len = reflen\n",
    "\n",
    "        ngrams_ref = extract_ngrams(ref)\n",
    "        for ngram in ngrams_ref.keys():\n",
    "            ngrams[ngram] = max(ngrams[ngram], ngrams_ref[ngram])\n",
    "\n",
    "    return ngrams, closest_diff, closest_len\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def extract_ngrams(line, min_order=1, max_order=NGRAM_ORDER) -> Counter:\n",
    "    \"\"\"Extracts all the ngrams (1 <= n <= NGRAM_ORDER) from a sequence of tokens.\n",
    "    :param line: a segment containing a sequence of words\n",
    "    :param max_order: collect n-grams from 1<=n<=max\n",
    "    :return: a dictionary containing ngrams and counts\n",
    "    \"\"\"\n",
    "\n",
    "    ngrams = Counter()\n",
    "    tokens = line.split()\n",
    "    for n in range(min_order, max_order + 1):\n",
    "        for i in range(0, len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i: i + n])\n",
    "            ngrams[ngram] += 1\n",
    "\n",
    "    return ngrams  \n",
    "\n",
    "def my_log(num):\n",
    "    \"\"\"\n",
    "    Floors the log function\n",
    "    :param num: the number\n",
    "    :return: log(num) floored to a very low number\n",
    "    \"\"\"\n",
    "\n",
    "    if num == 0.0:\n",
    "        return -9999999999\n",
    "    return math.log(num)\n",
    "  \n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2index = {\"PAD\" : 0, \"<SOS>\" : 1, \"EOS\" : 2, \"UNK\" : 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"<SOS>\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.n_words = 4  # Count SOS and EOS and Pad\n",
    "        self.all_words = []\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        'Add all words from all sentences'\n",
    "        for word in sentence.split(' '):\n",
    "            if word.strip(): #if not empty space\n",
    "                self.all_words.append(word)\n",
    "                \n",
    "                \n",
    "    def build_vocab(self, vocab_size=MAX_VOCAB_SIZE):\n",
    "        'Build vocabulary of vocab_size most common words'\n",
    "        \n",
    "        token_counter = Counter(self.all_words)\n",
    "        vocab, count = zip(*token_counter.most_common(vocab_size)) #* unzips the tuples\n",
    "        for word in vocab:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def remove_blanks(pair):\n",
    "    '''Remove empty lines'''\n",
    "    if len(pair[0]) == 0 and len(pair[1]) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def set_max_length(pair, max_length=MAX_LENGTH):\n",
    "    if len(pair[0].split(' ')) > max_length or len(pair[1].split(' '))>max_length:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def readLangs(filename1, filename2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    with open(filename1, encoding='utf-8') as f:\n",
    "        lines1 = f.read().strip().split('\\n')\n",
    "        \n",
    "    with open(filename2, encoding='utf-8') as f:\n",
    "        lines2 = f.read().strip().split('\\n')   \n",
    "        \n",
    "    # Remove punctuation\n",
    "    lines1 = [removePunctuation(l) for l in lines1]\n",
    "    lines2 = [removePunctuation(l) for l in lines2]\n",
    "              \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse: #change from english->french to french->english for example\n",
    "        pairs =list(zip(lines2, lines1))\n",
    "        input_lang = Lang(filename2[-2:]) #take last two letters\n",
    "        output_lang = Lang(filename1[-2:])\n",
    "    else:\n",
    "        pairs =list(zip(lines1, lines2))\n",
    "        input_lang = Lang(filename1[-2:])\n",
    "        output_lang = Lang(filename2[-2:])\n",
    "            \n",
    "        \n",
    "\n",
    "    pairs = list(filter(remove_blanks, pairs))  \n",
    "    pairs = list(filter(set_max_length, pairs))\n",
    "\n",
    "    return input_lang, output_lang, pairs \n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, num_sent=None, reverse=False):\n",
    "    \n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    \n",
    "    pairs = pairs[:num_sent]\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    input_lang.build_vocab()\n",
    "    output_lang.build_vocab()\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, word2id_lang1, word2id_lang2):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1, self.data_list2 = zip(*data_tuple)\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        self.word2id1 = word2id_lang1\n",
    "        self.word2id2 = word2id_lang2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        input_sentence = [self.word2id1[c] if c in self.word2id1.keys() \n",
    "                         else UNK_IDX for c in self.data_list1[key].split()][:MAX_LENGTH-1]\n",
    "        input_sentence.append(EOS_token)\n",
    "                                                                   \n",
    "        output_sentence = [self.word2id2[c] if c in self.word2id2.keys() \n",
    "                          else UNK_IDX for c in self.data_list2[key].split()][:MAX_LENGTH-1]\n",
    "        output_sentence.append(EOS_token)\n",
    "\n",
    "        return [input_sentence, output_sentence, len(input_sentence), len(output_sentence)]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "     \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        x1 = datum[0]\n",
    "        x2 = datum[1]\n",
    "        len1 = datum[2]\n",
    "        len2 = datum[3]\n",
    "        \n",
    "        length_list1.append(len1)\n",
    "        length_list2.append(len2)\n",
    "        #Pad first sentences\n",
    "        padded_vec1 = np.pad(np.array(x1),\n",
    "                                pad_width=((0,MAX_LENGTH-len1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "        #Pad second sentences\n",
    "        padded_vec2 = np.pad(np.array(x2),\n",
    "                        pad_width=((0,MAX_LENGTH-len2)),\n",
    "                        mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    data_list1 = np.array(data_list1)\n",
    "    data_list2 = np.array(data_list2)\n",
    "    length_list1 = np.array(length_list1)\n",
    "    lenth_list2 = np.array(length_list2)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list1)), \n",
    "            torch.from_numpy(np.array(data_list2)),\n",
    "            torch.LongTensor(length_list1), \n",
    "            torch.LongTensor(length_list2)]\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, dropout=0):\n",
    "        '''Bidirectional RNN'''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Embedding input: max_length x batch_size\n",
    "        # Embedding output: max_length x batch_size x hidden size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0) #vocab size x hidden size\n",
    "        \n",
    "        # Input: (max_length x batch_size x hidden_size)\n",
    "        # Output: hidden - 2 x batch_size x hidden_size\n",
    "        # Output: outputs max_length x batch_size x hidden_size*2\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden\n",
    "    \n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_weights = Variable(torch.zeros(this_batch_size, max_len)).to(device) # B x S\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_weights[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_weights, dim=1).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "            \n",
    "        weights = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "        weights = self.v.mm(weights.transpose(0,1))\n",
    "        return weights\n",
    "    \n",
    "\n",
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = MAX_LENGTH\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).unsqueeze(0) #so that we have 1 x batch x hidden\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Implement attention\n",
    "        attn_weights = self.attn(last_hidden, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)).transpose(0,1) # 1 x batch x hidden\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights \n",
    "        return output, hidden, attn_weights\n",
    "        \n",
    "    \n",
    "def train(inputs, input_lengths, targets, target_lengths, \n",
    "          encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH,\n",
    "         teacher_forcing_ratio=0.5, clip = 5):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 #\n",
    "    batch_size = inputs.size()[1]\n",
    "    #print('input size', inputs.size())\n",
    "    #print('batch size', batch_size)\n",
    "    max_targ_len = max_length\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(inputs, input_lengths, None)\n",
    "\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = torch.LongTensor([SOS_token] * batch_size).to(device)\n",
    "    decoder_hidden = encoder_hidden[:1] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    #print('time 1 size', decoder_input.size())\n",
    "    #print('time 1 hidden size', decoder_hidden.size())\n",
    "    \n",
    "    #randomly use teacher forcing or not\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_targ_len, batch_size, output_lang.n_words))\n",
    "    \n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_targ_len):\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            decoder_input = targets[t]\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_targ_len):\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "\n",
    "    loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    targets.transpose(0, 1).contiguous(),\n",
    "    target_lengths)\n",
    "        \n",
    "    loss.backward()\n",
    "        \n",
    "    # Clip gradient norms\n",
    "    clip = clip\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    \n",
    "    \n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def trainIters(loader, encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01,\n",
    "              teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    start = time.time()\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    plot_losses = []\n",
    "\n",
    "    counter = 0\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch < n_iters:\n",
    "        epoch += 1\n",
    "\n",
    "        # Get training data for this cycle\n",
    "        for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # Run the train function\n",
    "            loss = train(\n",
    "                source.long().transpose(0,1), lengths1, target.long().transpose(0,1), lengths2,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio=teacher_forcing_ratio\n",
    "            )\n",
    "\n",
    "            # Keep track of loss\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_iters), epoch, \n",
    "                                                       epoch / n_iters * 100, print_loss_avg)\n",
    "                print(print_summary)\n",
    "\n",
    "\n",
    "            if counter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lengths, translated, search='greedy', max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence.transpose(0,1)\n",
    "        input_length = sentence.size()[0]\n",
    "        \n",
    "        # encode the source lanugage\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor, input_lengths, None)\n",
    "\n",
    "        decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden[:1] # Use last (forward) hidden state from encoder \n",
    "        # output of this function\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)\n",
    "            \n",
    "            # hint: print out decoder_output and decoder_attention\n",
    "            # TODO: add your code here to populate decoded_words and decoder_attentions\n",
    "            # TODO: do this in 2 ways discussed in class: greedy & beam_search\n",
    "            \n",
    "            # GREEDY\n",
    "            topv, topi = decoder_output.data.topk(1) \n",
    "\n",
    "            if topi.item() == EOS_token:\n",
    "                #decoded_words.append('<EOS>')\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                if topi.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
    "                    decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            \n",
    "            decoder_input = topi[0].detach()\n",
    "        \n",
    "        translation = []\n",
    "        for i in translated: #expected translation\n",
    "            if i.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
    "                translation.append(output_lang.index2word[i.item()])\n",
    "\n",
    "        return decoded_words, translation\n",
    "    \n",
    "    \n",
    "def evaluate_batch(loader, encoder, decoder):\n",
    "    \n",
    "    decoded_sentences = []\n",
    "    actual_sentences = []\n",
    "    \n",
    "    for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
    "        #iterate over batch\n",
    "        \n",
    "        for n in range(len(source)):\n",
    "            # Go sentence by sentence\n",
    "            \n",
    "            decoded, actual = evaluate(encoder, decoder, source[n].unsqueeze(0), lengths1[n], target[n])\n",
    "            decoded_sentences.append(decoded)\n",
    "            actual_sentences.append(actual)\n",
    "            \n",
    "    return decoded_sentences, actual_sentences\n",
    "\n",
    "\n",
    "def evaluate_bleu(translation_list, reference_list):\n",
    "     \n",
    "    translations = ' '.join(r for v in translation_list for r in v)\n",
    "    references = ' '.join(r for v in reference_list for r in v)\n",
    "    \n",
    "    return corpus_bleu(translations, references)\n",
    "\n",
    "#Plot results\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))    \n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    \"\"\"\n",
    "    Function that takes in attention and visualize the attention.\n",
    "    @param - input_sentence: string the represent a list of words from source language\n",
    "    @param - output_words: the gold translation in target language\n",
    "    @param - attentions: a numpy array\n",
    "    \"\"\"\n",
    "    # Set up figure with colorbar    \n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # TODO: Add your code here to visualize the attention\n",
    "    # look at documentation for imshow https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.matshow.html\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder, decoder, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_loc = 'iwslt-vi-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 100 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 233\n",
      "en 216\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(en_loc+'/train.tok.vi', en_loc+'/train.tok.en', num_sent=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 61 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 130\n",
      "en 114\n"
     ]
    }
   ],
   "source": [
    "input_lang_v, output_lang_v, pairs_v = prepareData(en_loc+'/dev.tok.vi', en_loc+'/dev.tok.en', num_sent=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 11s (- 28m 53s) (13 0%) 5.1024\n",
      "0m 21s (- 27m 53s) (25 1%) 4.5195\n",
      "0m 31s (- 26m 54s) (38 1%) 4.3315\n",
      "0m 41s (- 26m 54s) (50 2%) 4.1953\n",
      "0m 51s (- 26m 23s) (63 3%) 4.0996\n",
      "1m 1s (- 26m 15s) (75 3%) 4.0166\n",
      "1m 11s (- 25m 48s) (88 4%) 3.9305\n",
      "1m 21s (- 25m 43s) (100 5%) 3.8440\n",
      "1m 31s (- 25m 24s) (113 5%) 3.8049\n",
      "1m 41s (- 25m 18s) (125 6%) 3.7826\n",
      "1m 51s (- 25m 3s) (138 6%) 3.7388\n",
      "2m 1s (- 24m 55s) (150 7%) 3.6441\n",
      "2m 11s (- 24m 41s) (163 8%) 3.5924\n",
      "2m 21s (- 24m 34s) (175 8%) 3.5692\n",
      "2m 31s (- 24m 20s) (188 9%) 3.5482\n",
      "2m 41s (- 24m 12s) (200 10%) 3.5384\n",
      "2m 51s (- 23m 58s) (213 10%) 3.4665\n",
      "3m 1s (- 23m 51s) (225 11%) 3.4515\n",
      "3m 11s (- 23m 37s) (238 11%) 3.4475\n",
      "3m 21s (- 23m 27s) (250 12%) 3.4053\n",
      "3m 31s (- 23m 14s) (263 13%) 3.3757\n",
      "3m 41s (- 23m 6s) (275 13%) 3.3071\n",
      "3m 51s (- 22m 54s) (288 14%) 3.2025\n",
      "4m 1s (- 22m 45s) (300 15%) 3.2501\n",
      "4m 11s (- 22m 34s) (313 15%) 3.2448\n",
      "4m 21s (- 22m 26s) (325 16%) 3.2124\n",
      "4m 31s (- 22m 15s) (338 16%) 3.1490\n",
      "4m 41s (- 22m 7s) (350 17%) 3.1654\n",
      "4m 51s (- 21m 55s) (363 18%) 3.0558\n",
      "5m 1s (- 21m 46s) (375 18%) 3.0515\n",
      "5m 11s (- 21m 34s) (388 19%) 3.0139\n",
      "5m 21s (- 21m 25s) (400 20%) 2.9923\n",
      "5m 31s (- 21m 12s) (413 20%) 2.9657\n",
      "5m 41s (- 21m 4s) (425 21%) 2.9316\n",
      "5m 51s (- 20m 53s) (438 21%) 2.9996\n",
      "6m 1s (- 20m 45s) (450 22%) 2.9171\n",
      "6m 11s (- 20m 34s) (463 23%) 2.9026\n",
      "6m 21s (- 20m 24s) (475 23%) 2.9233\n",
      "6m 31s (- 20m 12s) (488 24%) 2.8706\n",
      "6m 41s (- 20m 3s) (500 25%) 2.8606\n",
      "6m 51s (- 19m 52s) (513 25%) 2.7963\n",
      "7m 1s (- 19m 43s) (525 26%) 2.7522\n",
      "7m 11s (- 19m 32s) (538 26%) 2.7105\n",
      "7m 21s (- 19m 23s) (550 27%) 2.6594\n",
      "7m 31s (- 19m 11s) (563 28%) 2.6660\n",
      "7m 41s (- 19m 2s) (575 28%) 2.6629\n",
      "7m 51s (- 18m 52s) (588 29%) 2.6039\n",
      "8m 1s (- 18m 43s) (600 30%) 2.5422\n",
      "8m 11s (- 18m 32s) (613 30%) 2.6105\n",
      "8m 21s (- 18m 23s) (625 31%) 2.5949\n",
      "8m 31s (- 18m 11s) (638 31%) 2.5112\n",
      "8m 41s (- 18m 2s) (650 32%) 2.4179\n",
      "8m 51s (- 17m 51s) (663 33%) 2.4290\n",
      "9m 1s (- 17m 42s) (675 33%) 2.4977\n",
      "9m 11s (- 17m 31s) (688 34%) 2.4096\n",
      "9m 21s (- 17m 21s) (700 35%) 2.4040\n",
      "9m 30s (- 17m 10s) (713 35%) 2.3629\n",
      "9m 40s (- 17m 1s) (725 36%) 2.3185\n",
      "9m 50s (- 16m 50s) (738 36%) 2.4016\n",
      "10m 0s (- 16m 40s) (750 37%) 2.2625\n",
      "10m 10s (- 16m 29s) (763 38%) 2.2110\n",
      "10m 20s (- 16m 20s) (775 38%) 2.2391\n",
      "10m 30s (- 16m 9s) (788 39%) 2.2570\n",
      "10m 40s (- 16m 0s) (800 40%) 2.1304\n",
      "10m 50s (- 15m 49s) (813 40%) 2.1458\n",
      "10m 59s (- 15m 39s) (825 41%) 2.0703\n",
      "11m 9s (- 15m 28s) (838 41%) 2.1380\n",
      "11m 19s (- 15m 19s) (850 42%) 2.0055\n",
      "11m 29s (- 15m 8s) (863 43%) 2.1129\n",
      "11m 39s (- 14m 59s) (875 43%) 2.0401\n",
      "11m 49s (- 14m 48s) (888 44%) 1.9343\n",
      "11m 59s (- 14m 38s) (900 45%) 1.9488\n",
      "12m 8s (- 14m 27s) (913 45%) 1.9079\n",
      "12m 18s (- 14m 18s) (925 46%) 1.9311\n",
      "12m 28s (- 14m 7s) (938 46%) 1.8857\n",
      "12m 38s (- 13m 58s) (950 47%) 1.8055\n",
      "12m 48s (- 13m 47s) (963 48%) 1.8360\n",
      "12m 58s (- 13m 38s) (975 48%) 1.7103\n",
      "13m 8s (- 13m 27s) (988 49%) 1.6988\n",
      "13m 17s (- 13m 17s) (1000 50%) 1.6964\n",
      "13m 27s (- 13m 7s) (1013 50%) 1.6511\n",
      "13m 37s (- 12m 57s) (1025 51%) 1.6065\n",
      "13m 47s (- 12m 47s) (1038 51%) 1.6453\n",
      "13m 57s (- 12m 37s) (1050 52%) 1.6232\n",
      "14m 7s (- 12m 27s) (1063 53%) 1.6306\n",
      "14m 17s (- 12m 17s) (1075 53%) 1.4423\n",
      "14m 27s (- 12m 7s) (1088 54%) 1.5176\n",
      "14m 37s (- 11m 57s) (1100 55%) 1.4502\n",
      "14m 47s (- 11m 47s) (1113 55%) 1.4075\n",
      "14m 57s (- 11m 37s) (1125 56%) 1.3510\n",
      "15m 7s (- 11m 27s) (1138 56%) 1.3984\n",
      "15m 17s (- 11m 17s) (1150 57%) 1.3771\n",
      "15m 27s (- 11m 7s) (1163 58%) 1.2574\n",
      "15m 37s (- 10m 57s) (1175 58%) 1.3372\n",
      "15m 47s (- 10m 47s) (1188 59%) 1.3017\n",
      "15m 56s (- 10m 37s) (1200 60%) 1.2181\n",
      "16m 6s (- 10m 27s) (1213 60%) 1.2170\n",
      "16m 16s (- 10m 17s) (1225 61%) 1.2083\n",
      "16m 26s (- 10m 7s) (1238 61%) 1.1078\n",
      "16m 36s (- 9m 57s) (1250 62%) 1.1225\n",
      "16m 46s (- 9m 47s) (1263 63%) 1.1600\n",
      "16m 56s (- 9m 37s) (1275 63%) 1.1260\n",
      "17m 6s (- 9m 27s) (1288 64%) 1.0896\n",
      "17m 15s (- 9m 17s) (1300 65%) 1.0390\n",
      "17m 25s (- 9m 7s) (1313 65%) 1.0154\n",
      "17m 35s (- 8m 57s) (1325 66%) 1.0044\n",
      "17m 45s (- 8m 47s) (1338 66%) 0.9463\n",
      "17m 55s (- 8m 37s) (1350 67%) 0.9819\n",
      "18m 5s (- 8m 27s) (1363 68%) 0.9502\n",
      "18m 15s (- 8m 17s) (1375 68%) 0.8813\n",
      "18m 25s (- 8m 7s) (1388 69%) 0.8401\n",
      "18m 35s (- 7m 57s) (1400 70%) 0.8439\n",
      "18m 45s (- 7m 47s) (1413 70%) 0.8200\n",
      "18m 54s (- 7m 37s) (1425 71%) 0.8091\n",
      "19m 4s (- 7m 27s) (1438 71%) 0.7679\n",
      "19m 14s (- 7m 17s) (1450 72%) 0.8206\n",
      "19m 24s (- 7m 7s) (1463 73%) 0.7419\n",
      "19m 34s (- 6m 58s) (1475 73%) 0.7674\n",
      "19m 44s (- 6m 47s) (1488 74%) 0.7234\n",
      "19m 54s (- 6m 38s) (1500 75%) 0.6471\n",
      "20m 4s (- 6m 27s) (1513 75%) 0.6752\n",
      "20m 14s (- 6m 18s) (1525 76%) 0.6516\n",
      "20m 24s (- 6m 7s) (1538 76%) 0.6025\n",
      "20m 33s (- 5m 58s) (1550 77%) 0.5939\n",
      "20m 44s (- 5m 47s) (1563 78%) 0.5735\n",
      "20m 53s (- 5m 38s) (1575 78%) 0.6178\n",
      "21m 3s (- 5m 27s) (1588 79%) 0.5712\n",
      "21m 13s (- 5m 18s) (1600 80%) 0.5379\n",
      "21m 23s (- 5m 7s) (1613 80%) 0.5261\n",
      "21m 33s (- 4m 58s) (1625 81%) 0.5198\n",
      "21m 43s (- 4m 48s) (1638 81%) 0.5135\n",
      "21m 53s (- 4m 38s) (1650 82%) 0.4774\n",
      "22m 3s (- 4m 28s) (1663 83%) 0.4841\n",
      "22m 13s (- 4m 18s) (1675 83%) 0.4644\n",
      "22m 23s (- 4m 8s) (1688 84%) 0.4353\n",
      "22m 33s (- 3m 58s) (1700 85%) 0.4354\n",
      "22m 43s (- 3m 48s) (1713 85%) 0.4287\n",
      "22m 53s (- 3m 38s) (1725 86%) 0.4234\n",
      "23m 3s (- 3m 28s) (1738 86%) 0.4013\n",
      "23m 13s (- 3m 19s) (1750 87%) 0.3982\n",
      "23m 23s (- 3m 8s) (1763 88%) 0.3884\n",
      "23m 33s (- 2m 59s) (1775 88%) 0.4029\n",
      "23m 43s (- 2m 48s) (1788 89%) 0.3792\n",
      "23m 53s (- 2m 39s) (1800 90%) 0.3803\n",
      "24m 3s (- 2m 28s) (1813 90%) 0.3590\n",
      "24m 12s (- 2m 19s) (1825 91%) 0.3585\n",
      "24m 23s (- 2m 8s) (1838 91%) 0.3394\n",
      "24m 33s (- 1m 59s) (1850 92%) 0.3305\n",
      "24m 43s (- 1m 49s) (1863 93%) 0.3547\n",
      "24m 53s (- 1m 39s) (1875 93%) 0.3247\n",
      "25m 3s (- 1m 29s) (1888 94%) 0.3260\n",
      "25m 12s (- 1m 19s) (1900 95%) 0.3084\n",
      "25m 23s (- 1m 9s) (1913 95%) 0.3159\n",
      "25m 32s (- 0m 59s) (1925 96%) 0.2939\n",
      "25m 43s (- 0m 49s) (1938 96%) 0.2876\n",
      "25m 53s (- 0m 39s) (1950 97%) 0.2921\n",
      "26m 2s (- 0m 29s) (1963 98%) 0.2862\n",
      "26m 12s (- 0m 19s) (1975 98%) 0.2825\n",
      "26m 22s (- 0m 9s) (1988 99%) 0.2717\n",
      "26m 32s (- 0m 0s) (2000 100%) 0.2695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE=32\n",
    "hidden_size=256\n",
    "\n",
    "train_dataset = VocabDataset(pairs, input_lang.word2index, output_lang.word2index)\n",
    "# 1 batch input dimension: num_sentences x max sentence length\n",
    "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "encoder = EncoderRNN(hidden_size = hidden_size, vocab_size = input_lang.n_words )\n",
    "decoder = BahdanauAttnDecoderRNN(hidden_size, output_lang.n_words)\n",
    "\n",
    "plot_losses = trainIters(train_loader, encoder, decoder, n_iters=2000, \n",
    "                         print_every=50, plot_every=100, learning_rate=0.01, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Expected: ['He', 'was', 'looking', 'at']\n",
      "Actual: ['He', 'was', 'looking', 'at']\n",
      "\n",
      "\n",
      "Expected: ['Don', 't', 'start', 'wrestling']\n",
      "Actual: ['Don', 't', 'start', 'wrestling']\n",
      "\n",
      "\n",
      "Expected: ['Umar', 'was', 'paralyzed', 'for']\n",
      "Actual: ['Umar', 'was', 'paralyzed', 'for']\n",
      "\n",
      "\n",
      "Expected: ['This', 'is', 'Chet', 'Baker']\n",
      "Actual: ['This', 'is', 'Chet', 'Baker']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you']\n",
      "Actual: ['Thank', 'you']\n",
      "\n",
      "\n",
      "Expected: ['Francesca', 'Fedeli', 'Ciao']\n",
      "Actual: ['Francesca', 'Fedeli', 'Ciao']\n",
      "\n",
      "\n",
      "Expected: ['Cows', 'with', 'a', 'view']\n",
      "Actual: ['Cows', 'with', 'a', 'view']\n",
      "\n",
      "\n",
      "Expected: ['Here', 'is', 'the', 'thing']\n",
      "Actual: ['Here', 'is', 'the', 'thing']\n",
      "\n",
      "\n",
      "Expected: ['So', 'congratulations']\n",
      "Actual: ['So', 'congratulations']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you', 'very', 'much']\n",
      "Actual: ['Thank', 'you', 'very', 'much']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you', 'very', 'much']\n",
      "Actual: ['Thank', 'you', 'very', 'much']\n",
      "\n",
      "\n",
      "Expected: ['Something', 'crazy', 'also', 'happened']\n",
      "Actual: ['Something', 'crazy', 'also', 'happened']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you']\n",
      "Actual: ['Thank', 'you']\n",
      "\n",
      "\n",
      "Expected: ['Please', 'keep', 'your', 'seat']\n",
      "Actual: ['Please', 'keep', 'your', 'seat']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you']\n",
      "Actual: ['Thank', 'you']\n",
      "\n",
      "\n",
      "Expected: ['Oh', 'my', 'God']\n",
      "Actual: ['Oh', 'my', 'God']\n",
      "\n",
      "\n",
      "Expected: ['All', 'right']\n",
      "Actual: ['Okay']\n",
      "\n",
      "\n",
      "Expected: ['Grab', 'that', 'thumb']\n",
      "Actual: ['Grab', 'that', 'thumb']\n",
      "\n",
      "\n",
      "Expected: ['A', 'very', 'different', 'feeling']\n",
      "Actual: ['A', 'very', 'different', 'feeling']\n",
      "\n",
      "\n",
      "Expected: ['It', 's', 'a', 'massive']\n",
      "Actual: ['It', 's', 'a', 'massive']\n",
      "\n",
      "\n",
      "Expected: ['Charles', 'Moore', 'Thank', 'you']\n",
      "Actual: ['Charles', 'Moore', 'Thank', 'you']\n",
      "\n",
      "\n",
      "Expected: ['We', 'like', 'to', 'build']\n",
      "Actual: ['We', 'like', 'to', 'build']\n",
      "\n",
      "\n",
      "Expected: ['Okay']\n",
      "Actual: ['Okay']\n",
      "\n",
      "\n",
      "Expected: ['Yes', 'Awesome', 'Okay']\n",
      "Actual: ['Yes', 'Awesome', 'Okay']\n",
      "\n",
      "\n",
      "Expected: ['It', 's', 'diddly-point-squat']\n",
      "Actual: ['It', 's', 'diddly-point-squat']\n",
      "\n",
      "\n",
      "Expected: ['Not', 'anymore']\n",
      "Actual: ['Not', 'anymore']\n",
      "\n",
      "\n",
      "Expected: ['Hundreds', 'of', 'people', 'were']\n",
      "Actual: ['Hundreds', 'of', 'people', 'were']\n",
      "\n",
      "\n",
      "Expected: ['So', 'that', 'became', 'a']\n",
      "Actual: ['So', 'that', 'became', 'a']\n",
      "\n",
      "\n",
      "Expected: ['Think', 'about', 'the', 'following']\n",
      "Actual: ['Think', 'about', 'the', 'following']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you', 'very', 'much']\n",
      "Actual: ['Thank', 'you', 'very', 'much']\n",
      "\n",
      "\n",
      "Expected: ['So', 'life', 'had', 'to']\n",
      "Actual: ['So', 'life', 'had', 'to']\n",
      "\n",
      "\n",
      "Expected: ['The', 'press', 'started', 'calling']\n",
      "Actual: ['The', 'press', 'started', 'calling']\n",
      "\n",
      "\n",
      "Expected: ['And', 'it', 'was', 'also']\n",
      "Actual: ['And', 'it', 'was', 'also']\n",
      "\n",
      "\n",
      "Expected: ['Perfect']\n",
      "Actual: ['Perfect']\n",
      "\n",
      "\n",
      "Expected: ['Of', 'course', 'they', 'would']\n",
      "Actual: ['Of', 'course', 'they', 'would']\n",
      "\n",
      "\n",
      "Expected: ['We', 'make', 'things', 'grow']\n",
      "Actual: ['We', 'make', 'things', 'grow']\n",
      "\n",
      "\n",
      "Expected: ['It', 's', 'happening']\n",
      "Actual: ['Okay']\n",
      "\n",
      "\n",
      "Expected: ['It', 's', 'smaller', 'than']\n",
      "Actual: ['It', 's', 'smaller', 'than']\n",
      "\n",
      "\n",
      "Expected: ['Okay']\n",
      "Actual: ['Okay']\n",
      "\n",
      "\n",
      "Expected: ['And', 'you', 'become', 'friends']\n",
      "Actual: ['And', 'you', 'become', 'friends']\n",
      "\n",
      "\n",
      "Expected: ['Biohackers', 'work', 'alone']\n",
      "Actual: ['Biohackers', 'work', 'alone']\n",
      "\n",
      "\n",
      "Expected: ['Let', 's', 'talk', 'trash']\n",
      "Actual: ['Let', 's', 'talk', 'trash']\n",
      "\n",
      "\n",
      "Expected: ['There', 's', 'absolutely', 'no']\n",
      "Actual: ['There', 's', 'absolutely', 'no']\n",
      "\n",
      "\n",
      "Expected: ['All', 'right']\n",
      "Actual: ['Okay']\n",
      "\n",
      "\n",
      "Expected: ['Just', 'raise', 'that', 'Exactly']\n",
      "Actual: ['Just', 'raise', 'that', 'Exactly']\n",
      "\n",
      "\n",
      "Expected: ['Porous', 'nonporous']\n",
      "Actual: ['Porous', 'nonporous']\n",
      "\n",
      "\n",
      "Expected: ['We', 'adapt']\n",
      "Actual: ['We', 'adapt']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you', 'very', 'much']\n",
      "Actual: ['Thank', 'you', 'very', 'much']\n",
      "\n",
      "\n",
      "Expected: ['These', 'are', 'children', 'like']\n",
      "Actual: ['These', 'are', 'children', 'like']\n",
      "\n",
      "\n",
      "Expected: ['Don', 't', 'cry']\n",
      "Actual: ['Don', 't', 'cry']\n",
      "\n",
      "\n",
      "Expected: ['This', 'is', 'La', 'Scala']\n",
      "Actual: ['This', 'is', 'La', 'Scala']\n",
      "\n",
      "\n",
      "Expected: ['It', 's', 'really', 'cool']\n",
      "Actual: ['It', 's', 'really', 'cool']\n",
      "\n",
      "\n",
      "Expected: ['Okay', 'Africa']\n",
      "Actual: ['Okay', 'Africa']\n",
      "\n",
      "\n",
      "Expected: ['This', 'is', 'a', 'great']\n",
      "Actual: ['This', 'is', 'a', 'great']\n",
      "\n",
      "\n",
      "Expected: ['You', 'can', 'see', 'the']\n",
      "Actual: ['You', 'can', 'see', 'the']\n",
      "\n",
      "\n",
      "Expected: ['I', 'lost', 'my', 'face']\n",
      "Actual: ['I', 'lost', 'my', 'face']\n",
      "\n",
      "\n",
      "Expected: ['This', 'is', 'the', 'enemy']\n",
      "Actual: ['This', 'is', 'the', 'enemy']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you']\n",
      "Actual: ['Thank', 'you']\n",
      "\n",
      "\n",
      "Expected: ['Umar', 'also', 'has', 'polio']\n",
      "Actual: ['Umar', 'also', 'has', 'polio']\n",
      "\n",
      "\n",
      "Expected: []\n",
      "Actual: []\n",
      "\n",
      "\n",
      "Expected: ['It', 'was', 'terribly', 'dangerous']\n",
      "Actual: ['It', 'was', 'terribly', 'dangerous']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you', 'very', 'much']\n",
      "Actual: ['Thank', 'you']\n",
      "\n",
      "\n",
      "Expected: ['But', 'Wagner', 'made', 'an']\n",
      "Actual: ['But', 'Wagner', 'made', 'an']\n",
      "\n",
      "\n",
      "Expected: ['This', 'is', 'Mahler']\n",
      "Actual: ['This', 'is', 'Mahler']\n",
      "\n",
      "\n",
      "Expected: ['It', 's', 'adaptive']\n",
      "Actual: ['It', 's', 'adaptive']\n",
      "\n",
      "\n",
      "Expected: ['It', 'could', 'be', 'anti-bacterial']\n",
      "Actual: ['It', 'could', 'be', 'anti-bacterial']\n",
      "\n",
      "\n",
      "Expected: ['Actually', 'it', 's', 'very']\n",
      "Actual: ['Actually', 'it', 's', 'very']\n",
      "\n",
      "\n",
      "Expected: ['We', 'sing']\n",
      "Actual: ['We', 'sing']\n",
      "\n",
      "\n",
      "Expected: ['The', 'outcome', 'immediate']\n",
      "Actual: ['The', 'outcome', 'immediate']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you', 'very', 'much']\n",
      "Actual: ['Thank', 'you', 'very', 'much']\n",
      "\n",
      "\n",
      "Expected: ['This', 'is', 'the', 'picture']\n",
      "Actual: ['This', 'is', 'the', 'picture']\n",
      "\n",
      "\n",
      "Expected: ['The', 'effect', 'is', 'just']\n",
      "Actual: ['The', 'effect', 'is', 'just']\n",
      "\n",
      "\n",
      "Expected: ['This', 'disease', 'was', 'terrifying']\n",
      "Actual: ['This', 'disease', 'was', 'terrifying']\n",
      "\n",
      "\n",
      "Expected: ['but', 'mostly', 'bottle', 'caps']\n",
      "Actual: ['but', 'mostly', 'bottle', 'caps']\n",
      "\n",
      "\n",
      "Expected: ['Wooo']\n",
      "Actual: ['Wooo']\n",
      "\n",
      "\n",
      "Expected: ['Steaming', 'piles', 'of', 'humus']\n",
      "Actual: ['Steaming', 'piles', 'of', 'humus']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you', 'very', 'much']\n",
      "Actual: ['Thank', 'you', 'very', 'much']\n",
      "\n",
      "\n",
      "Expected: ['Thank', 'you', 'guys']\n",
      "Actual: ['Thank', 'you', 'guys']\n",
      "\n",
      "\n",
      "Expected: ['No', 'you', 'can', 't']\n",
      "Actual: ['No', 'you', 'can', 't']\n",
      "\n",
      "\n",
      "Expected: ['And', 'what', 'went', 'wrong']\n",
      "Actual: ['And', 'what', 'went', 'wrong']\n",
      "\n",
      "\n",
      "Expected: ['The', 'nervous', 'system', 'has']\n",
      "Actual: ['The', 'nervous', 'system', 'has']\n",
      "\n",
      "\n",
      "Expected: ['Mario']\n",
      "Actual: ['Mario']\n",
      "\n",
      "\n",
      "Expected: ['Crazy', 'yes']\n",
      "Actual: ['Crazy', 'yes']\n",
      "\n",
      "\n",
      "Expected: ['And', 'everybody', 's', 'crying']\n",
      "Actual: ['And', 'everybody', 's', 'crying']\n",
      "\n",
      "\n",
      "Expected: ['The', 'same', 'thing', 'happens']\n",
      "Actual: ['The', 'same', 'thing', 'happens']\n",
      "\n",
      "\n",
      "Expected: ['Now', 'it', 's', 'anonymous']\n",
      "Actual: ['Now', 'it', 's', 'anonymous']\n",
      "\n",
      "\n",
      "Expected: ['Okay', 'here', 'it', 'is']\n",
      "Actual: ['Okay', 'here', 'it', 'is']\n",
      "\n",
      "\n",
      "Expected: ['What']\n",
      "Actual: ['What']\n",
      "\n",
      "\n",
      "Expected: ['We', 'were', 'his', 'mirror']\n",
      "Actual: ['We', 'were', 'his', 'mirror']\n",
      "\n",
      "\n",
      "Expected: ['I', 'start', 'it', 'myself']\n",
      "Actual: ['I', 'start', 'it', 'myself']\n",
      "\n",
      "\n",
      "Expected: ['This', 'is', 'Carnegie', 'Hall']\n",
      "Actual: ['This', 'is', 'Carnegie', 'Hall']\n",
      "\n",
      "\n",
      "Expected: ['So', 'curiosity']\n",
      "Actual: ['So', 'curiosity']\n",
      "\n",
      "\n",
      "Expected: ['And', 'this']\n",
      "Actual: ['And', 'this']\n",
      "\n",
      "\n",
      "Expected: ['Oh', 'hah']\n",
      "Actual: ['Oh', 'hah']\n",
      "\n",
      "\n",
      "Expected: ['Was', 'the', 'tale', 'told']\n",
      "Actual: ['Was', 'the', 'tale', 'told']\n",
      "\n",
      "\n",
      "Expected: ['That', 's', 'the', 'stigma']\n",
      "Actual: ['That', 's', 'the', 'stigma']\n",
      "\n",
      "\n",
      "Expected: ['That', 'was', 'whistling']\n",
      "Actual: ['That', 'was', 'whistling']\n",
      "\n",
      "\n",
      "Expected: ['I', 'thought', 'so']\n",
      "Actual: ['I', 'thought', 'so']\n",
      "\n",
      "\n",
      "Expected: ['We', 'reverse', 'engineer', 'lab']\n",
      "Actual: ['We', 'reverse', 'engineer', 'lab']\n",
      "\n",
      "\n",
      "Expected: ['I', 'was', 'burned', 'very']\n",
      "Actual: ['I', 'was', 'burned', 'very']\n"
     ]
    }
   ],
   "source": [
    "decoded, actual = evaluate_batch(train_loader, encoder, decoder)\n",
    "\n",
    "for i in zip(decoded, actual):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print('\\n')\n",
    "    print('Expected:', i[1])\n",
    "    print('Actual:' ,i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.43900451104247"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(decoded, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
