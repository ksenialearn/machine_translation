{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "encoder_decoder1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "sP4TOAqdM1ia",
        "colab_type": "code",
        "outputId": "77efae4d-4aeb-43d0-a97b-f667b21fba6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x57464000 @  0x7f482d19d2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0uWjT6lM7eL",
        "colab_type": "code",
        "outputId": "c9873deb-3989-4894-ab53-fa874ac7c7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AyXb-dQXM3dy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import functional\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "MAX_LENGTH = 40 #temp\n",
        "\n",
        "MAX_VOCAB_SIZE = 1000000\n",
        "\n",
        "PAD_IDX = 0 \n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "UNK_IDX = 3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtj5cLvUN0We",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_loc = 'gdrive/My Drive/iwslt-vi-en'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dU8LsH2D_GMc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    batch_size = sequence_length.size(0)\n",
        "    seq_range = torch.range(0, max_len - 1).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
        "    seq_range_expand = Variable(seq_range_expand)\n",
        "    if sequence_length.is_cuda:\n",
        "        seq_range_expand = seq_range_expand.cuda()\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    return seq_range_expand < seq_length_expand\n",
        "\n",
        "\n",
        "def masked_cross_entropy(logits, target, length):\n",
        "    length = Variable(torch.LongTensor(length))\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "    \"\"\"\n",
        "\n",
        "    # logits_flat: (batch * max_len, num_classes)\n",
        "    logits_flat = logits.view(-1, logits.size(-1)).to(device)\n",
        "    # log_probs_flat: (batch * max_len, num_classes)\n",
        "    log_probs_flat = functional.log_softmax(logits_flat)\n",
        "    # target_flat: (batch * max_len, 1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    # losses_flat: (batch * max_len, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    # losses: (batch, max_len)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    # mask: (batch, max_len)\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1)).to(device)\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / length.float().sum().to(device)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Remove punctuation\n",
        "def removePunctuation(s):\n",
        "\n",
        "    to_remove = ('&lt;', '&gt;', '&amp;', '&apos;', '&quot;')\n",
        "    table = str.maketrans(dict.fromkeys('.!?:,'))\n",
        "    s = s.translate(table)\n",
        "    for i in to_remove:\n",
        "        s=s.replace(i,'')   \n",
        "    s = s.strip()\n",
        "    \n",
        "    return s\n",
        "\n",
        "\n",
        "from typing import List\n",
        "from collections import Counter, namedtuple\n",
        "from itertools import zip_longest\n",
        "\n",
        "def tokenize_13a(line):\n",
        "    \"\"\"\n",
        "    Tokenizes an input line using a relatively minimal tokenization that is however equivalent to mteval-v13a, used by WMT.\n",
        "    :param line: a segment to tokenize\n",
        "    :return: the tokenized line\n",
        "    \"\"\"\n",
        "\n",
        "    norm = line\n",
        "\n",
        "    # language-independent part:\n",
        "    norm = norm.replace('<skipped>', '')\n",
        "    norm = norm.replace('-\\n', '')\n",
        "    norm = norm.replace('\\n', ' ')\n",
        "    norm = norm.replace('&quot;', '\"')\n",
        "    norm = norm.replace('&amp;', '&')\n",
        "    norm = norm.replace('&lt;', '<')\n",
        "    norm = norm.replace('&gt;', '>')\n",
        "\n",
        "    # language-dependent part (assuming Western languages):\n",
        "    norm = \" {} \".format(norm)\n",
        "    norm = re.sub(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', ' \\\\1 ', norm)\n",
        "    norm = re.sub(r'([^0-9])([\\.,])', '\\\\1 \\\\2 ', norm)  # tokenize period and comma unless preceded by a digit\n",
        "    norm = re.sub(r'([\\.,])([^0-9])', ' \\\\1 \\\\2', norm)  # tokenize period and comma unless followed by a digit\n",
        "    norm = re.sub(r'([0-9])(-)', '\\\\1 \\\\2 ', norm)  # tokenize dash when preceded by a digit\n",
        "    norm = re.sub(r'\\s+', ' ', norm)  # one space only between words\n",
        "    norm = re.sub(r'^\\s+', '', norm)  # no leading space\n",
        "    norm = re.sub(r'\\s+$', '', norm)  # no trailing space\n",
        "\n",
        "    return norm\n",
        "\n",
        "def corpus_bleu(sys_stream, ref_streams, smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n",
        "                 use_effective_order=False):\n",
        "    \"\"\"Produces BLEU scores along with its sufficient statistics from a source against one or more references.\n",
        "    :param sys_stream: The system stream (a sequence of segments)\n",
        "    :param ref_streams: A list of one or more reference streams (each a sequence of segments)\n",
        "    :param smooth: The smoothing method to use\n",
        "    :param smooth_floor: For 'floor' smoothing, the floor to use\n",
        "    :param force: Ignore data that looks already tokenized\n",
        "    :param lowercase: Lowercase the data\n",
        "    :param tokenize: The tokenizer to use\n",
        "    :return: a BLEU object containing everything you'd want\n",
        "    \"\"\"\n",
        "\n",
        "    # Add some robustness to the input arguments\n",
        "    if isinstance(sys_stream, str):\n",
        "        sys_stream = [sys_stream]\n",
        "    if isinstance(ref_streams, str):\n",
        "        ref_streams = [[ref_streams]]\n",
        "\n",
        "    sys_len = 0\n",
        "    ref_len = 0\n",
        "\n",
        "    correct = [0 for n in range(NGRAM_ORDER)]\n",
        "    total = [0 for n in range(NGRAM_ORDER)]\n",
        "    \n",
        "\n",
        "    # look for already-tokenized sentences\n",
        "    tokenized_count = 0\n",
        "\n",
        "    fhs = [sys_stream] + ref_streams\n",
        "    for lines in zip_longest(*fhs):\n",
        "        if None in lines:\n",
        "            raise EOFError(\"Source and reference streams have different lengths!\")\n",
        "\n",
        "        if lowercase:\n",
        "            lines = [x.lower() for x in lines]\n",
        "            \n",
        "        tokenize= 'tokenize_13a'    \n",
        "\n",
        "        if not (force or tokenize == 'none') and lines[0].rstrip().endswith(' .'):\n",
        "            tokenized_count += 1\n",
        "\n",
        "            if tokenized_count == 100:\n",
        "                logging.warning('That\\'s 100 lines that end in a tokenized period (\\'.\\')')\n",
        "                logging.warning('It looks like you forgot to detokenize your test data, which may hurt your score.')\n",
        "                logging.warning('If you insist your data is detokenized, or don\\'t care, you can suppress this message with \\'--force\\'.')\n",
        "\n",
        "        output, *refs = [tokenize_13a(x.rstrip()) for x in lines]\n",
        "        \n",
        "\n",
        "        ref_ngrams, closest_diff, closest_len = ref_stats(output, refs)\n",
        "        \n",
        "\n",
        "        sys_len += len(output.split())\n",
        "        ref_len += closest_len\n",
        "\n",
        "        sys_ngrams = extract_ngrams(output)\n",
        "        for ngram in sys_ngrams.keys():\n",
        "            n = len(ngram.split())\n",
        "            correct[n-1] += min(sys_ngrams[ngram], ref_ngrams.get(ngram, 0))\n",
        "            total[n-1] += sys_ngrams[ngram]\n",
        "            \n",
        "\n",
        "    return compute_bleu(correct, total, sys_len, ref_len, smooth, smooth_floor, use_effective_order)\n",
        "  \n",
        "  \n",
        "# n-gram order. Don't change this.\n",
        "NGRAM_ORDER = 4\n",
        "  \n",
        "def compute_bleu(correct: List[int], total: List[int], sys_len: int, ref_len: int, smooth = 'none', smooth_floor = 0.01,\n",
        "                 use_effective_order = False):\n",
        "    \"\"\"Computes BLEU score from its sufficient statistics. Adds smoothing.\n",
        "    :param correct: List of counts of correct ngrams, 1 <= n <= NGRAM_ORDER\n",
        "    :param total: List of counts of total ngrams, 1 <= n <= NGRAM_ORDER\n",
        "    :param sys_len: The cumulative system length\n",
        "    :param ref_len: The cumulative reference length\n",
        "    :param smooth: The smoothing method to use\n",
        "    :param smooth_floor: The smoothing value added, if smooth method 'floor' is used\n",
        "    :param use_effective_order: Use effective order.\n",
        "    :return: A BLEU object with the score (100-based) and other statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    precisions = [0 for x in range(NGRAM_ORDER)]\n",
        "\n",
        "    smooth_mteval = 1.\n",
        "    effective_order = NGRAM_ORDER\n",
        "    for n in range(NGRAM_ORDER):\n",
        "        if total[n] == 0:\n",
        "            break\n",
        "\n",
        "        if use_effective_order:\n",
        "            effective_order = n + 1\n",
        "\n",
        "        if correct[n] == 0:\n",
        "            if smooth == 'exp':\n",
        "                smooth_mteval *= 2\n",
        "                precisions[n] = 100. / (smooth_mteval * total[n])\n",
        "            elif smooth == 'floor':\n",
        "                precisions[n] = 100. * smooth_floor / total[n]\n",
        "        else:\n",
        "            precisions[n] = 100. * correct[n] / total[n]\n",
        "\n",
        "    # If the system guesses no i-grams, 1 <= i <= NGRAM_ORDER, the BLEU score is 0 (technically undefined).\n",
        "    # This is a problem for sentence-level BLEU or a corpus of short sentences, where systems will get no credit\n",
        "    # if sentence lengths fall under the NGRAM_ORDER threshold. This fix scales NGRAM_ORDER to the observed\n",
        "    # maximum order. It is only available through the API and off by default\n",
        "\n",
        "    brevity_penalty = 1.0\n",
        "    if sys_len < ref_len:\n",
        "        brevity_penalty = math.exp(1 - ref_len / sys_len) if sys_len > 0 else 0.0\n",
        "        \n",
        "\n",
        "    bleu = brevity_penalty * math.exp(sum(map(my_log, precisions[:effective_order])) / effective_order)\n",
        "\n",
        "    return bleu \n",
        "  \n",
        "  \n",
        "def ref_stats(output, refs):\n",
        "    ngrams = Counter()\n",
        "    closest_diff = None\n",
        "    closest_len = None\n",
        "    for ref in refs:\n",
        "        tokens = ref.split()\n",
        "        reflen = len(tokens)\n",
        "        diff = abs(len(output.split()) - reflen)\n",
        "        if closest_diff is None or diff < closest_diff:\n",
        "            closest_diff = diff\n",
        "            closest_len = reflen\n",
        "        elif diff == closest_diff:\n",
        "            if reflen < closest_len:\n",
        "                closest_len = reflen\n",
        "\n",
        "        ngrams_ref = extract_ngrams(ref)\n",
        "        for ngram in ngrams_ref.keys():\n",
        "            ngrams[ngram] = max(ngrams[ngram], ngrams_ref[ngram])\n",
        "\n",
        "    return ngrams, closest_diff, closest_len\n",
        "  \n",
        "  \n",
        "def extract_ngrams(line, min_order=1, max_order=NGRAM_ORDER) -> Counter:\n",
        "    \"\"\"Extracts all the ngrams (1 <= n <= NGRAM_ORDER) from a sequence of tokens.\n",
        "    :param line: a segment containing a sequence of words\n",
        "    :param max_order: collect n-grams from 1<=n<=max\n",
        "    :return: a dictionary containing ngrams and counts\n",
        "    \"\"\"\n",
        "\n",
        "    ngrams = Counter()\n",
        "    tokens = line.split()\n",
        "    for n in range(min_order, max_order + 1):\n",
        "        for i in range(0, len(tokens) - n + 1):\n",
        "            ngram = ' '.join(tokens[i: i + n])\n",
        "            ngrams[ngram] += 1\n",
        "\n",
        "    return ngrams  \n",
        "\n",
        "def my_log(num):\n",
        "    \"\"\"\n",
        "    Floors the log function\n",
        "    :param num: the number\n",
        "    :return: log(num) floored to a very low number\n",
        "    \"\"\"\n",
        "\n",
        "    if num == 0.0:\n",
        "        return -9999999999\n",
        "    return math.log(num)\n",
        "  \n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2index = {\"PAD\" : 0, \"<SOS>\" : 1, \"<EOS>\" : 2, \"UNK\" : 3}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"PAD\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"UNK\"}\n",
        "        self.n_words = 4  # Count SOS and EOS and Pad\n",
        "        self.all_words = []\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        'Add all words from all sentences'\n",
        "        for word in sentence.split(' '):\n",
        "            if word.strip(): #if not empty space\n",
        "                self.all_words.append(word)\n",
        "                \n",
        "                \n",
        "    def build_vocab(self, vocab_size=MAX_VOCAB_SIZE):\n",
        "        'Build vocabulary of vocab_size most common words'\n",
        "        \n",
        "        token_counter = Counter(self.all_words)\n",
        "        vocab, count = zip(*token_counter.most_common(vocab_size)) #* unzips the tuples\n",
        "        for word in vocab:\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "def remove_blanks(pair):\n",
        "    '''Remove empty lines'''\n",
        "    if len(pair[0]) == 0 and len(pair[1]) == 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def set_max_length(pair, max_length=MAX_LENGTH):\n",
        "    if len(pair[0].split(' ')) > max_length or len(pair[1].split(' '))>max_length:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def readLangs(filename1, filename2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    with open(filename1, encoding='utf-8') as f:\n",
        "        lines1 = f.read().strip().split('\\n')\n",
        "        \n",
        "    with open(filename2, encoding='utf-8') as f:\n",
        "        lines2 = f.read().strip().split('\\n')   \n",
        "        \n",
        "    # Remove punctuation\n",
        "    lines1 = [removePunctuation(l) for l in lines1]\n",
        "    lines2 = [removePunctuation(l) for l in lines2]\n",
        "              \n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse: #change from english->french to french->english for example\n",
        "        pairs =list(zip(lines2, lines1))\n",
        "        input_lang = Lang(filename2[-2:]) #take last two letters\n",
        "        output_lang = Lang(filename1[-2:])\n",
        "    else:\n",
        "        pairs =list(zip(lines1, lines2))\n",
        "        input_lang = Lang(filename1[-2:])\n",
        "        output_lang = Lang(filename2[-2:])\n",
        "            \n",
        "        \n",
        "\n",
        "    pairs = list(filter(remove_blanks, pairs))  \n",
        "    pairs = list(filter(set_max_length, pairs))\n",
        "\n",
        "    return input_lang, output_lang, pairs \n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, num_sent=None, reverse=False):\n",
        "    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    \n",
        "    pairs = pairs[:num_sent]\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    \n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "        \n",
        "    input_lang.build_vocab()\n",
        "    output_lang.build_vocab()\n",
        "        \n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "class VocabDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_tuple, word2id_lang1, word2id_lang2):\n",
        "        \"\"\"\n",
        "        @param data_list: list of character\n",
        "        @param target_list: list of targets\n",
        "\n",
        "        \"\"\"\n",
        "        self.data_list1, self.data_list2 = zip(*data_tuple)\n",
        "        assert (len(self.data_list1) == len(self.data_list2))\n",
        "        self.word2id1 = word2id_lang1\n",
        "        self.word2id2 = word2id_lang2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list1)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        input_sentence = [self.word2id1[c] if c in self.word2id1.keys() \n",
        "                         else UNK_IDX for c in self.data_list1[key].split()][:MAX_LENGTH-1]\n",
        "        input_sentence.append(EOS_token)\n",
        "                                                                   \n",
        "        output_sentence = [self.word2id2[c] if c in self.word2id2.keys() \n",
        "                          else UNK_IDX for c in self.data_list2[key].split()][:MAX_LENGTH-1]\n",
        "        output_sentence.append(EOS_token)\n",
        "\n",
        "        return [input_sentence, output_sentence, len(input_sentence), len(output_sentence)]\n",
        "\n",
        "def vocab_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all\n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    data_list1 = []\n",
        "    data_list2 = []\n",
        "    length_list1 = []\n",
        "    length_list2 = []\n",
        "     \n",
        "    # padding\n",
        "    for datum in batch:\n",
        "        x1 = datum[0]\n",
        "        x2 = datum[1]\n",
        "        len1 = datum[2]\n",
        "        len2 = datum[3]\n",
        "        \n",
        "        length_list1.append(len1)\n",
        "        length_list2.append(len2)\n",
        "        #Pad first sentences\n",
        "        padded_vec1 = np.pad(np.array(x1),\n",
        "                                pad_width=((0,MAX_LENGTH-len1)),\n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        data_list1.append(padded_vec1)\n",
        "        \n",
        "        #Pad second sentences\n",
        "        padded_vec2 = np.pad(np.array(x2),\n",
        "                        pad_width=((0,MAX_LENGTH-len2)),\n",
        "                        mode=\"constant\", constant_values=0)\n",
        "        data_list2.append(padded_vec2)\n",
        "        \n",
        "    data_list1 = np.array(data_list1)\n",
        "    data_list2 = np.array(data_list2)\n",
        "    length_list1 = np.array(length_list1)\n",
        "    lenth_list2 = np.array(length_list2)\n",
        "    \n",
        "    return [torch.from_numpy(np.array(data_list1)), \n",
        "            torch.from_numpy(np.array(data_list2)),\n",
        "            torch.LongTensor(length_list1), \n",
        "            torch.LongTensor(length_list2)]\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, dropout=0):\n",
        "        '''Bidirectional RNN'''\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Embedding input: max_length x batch_size\n",
        "        # Embedding output: max_length x batch_size x hidden size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0) #vocab size x hidden size\n",
        "        \n",
        "        # Input: (max_length x batch_size x hidden_size)\n",
        "        # Output: hidden - 2 x batch_size x hidden_size\n",
        "        # Output: outputs max_length x batch_size x hidden_size*2\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, dropout=self.dropout, bidirectional=False)\n",
        "        \n",
        "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
        "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
        "        embedded = self.embedding(input_seqs)\n",
        "        outputs, hidden = self.gru(embedded, hidden)\n",
        "        #outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
        "        return outputs, hidden\n",
        "    \n",
        "    \n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
        "        \n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inp, hidden):\n",
        "        embedded = self.embedding(inp).unsqueeze(0) #so that we have 1 x batch x hidden\n",
        "        #print('embedded', embedded.size())\n",
        "        output = F.relu(embedded)\n",
        "        #print('after relu', output.size())\n",
        "        #print('hidden size', hidden.size())\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "    \n",
        "    \n",
        "def train(inputs, input_lengths, targets, target_lengths, \n",
        "          encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH,\n",
        "         teacher_forcing_ratio=0.5):\n",
        "    \n",
        "    # Zero gradients of both optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss = 0 #\n",
        "    batch_size = inputs.size()[1]\n",
        "    #print('input size', inputs.size())\n",
        "    #print('batch size', batch_size)\n",
        "    max_targ_len = max_length\n",
        "\n",
        "    # Run words through encoder\n",
        "    _, encoder_hidden = encoder(inputs, input_lengths, None)\n",
        "\n",
        "    \n",
        "    # Prepare input and output variables\n",
        "    decoder_input = torch.LongTensor([SOS_token] * batch_size).to(device)\n",
        "    decoder_hidden = encoder_hidden#[:1] # Use last (forward) hidden state from encoder\n",
        "    \n",
        "    #print('time 1 size', decoder_input.size())\n",
        "    #print('time 1 hidden size', decoder_hidden.size())\n",
        "    \n",
        "    #randomly use teacher forcing or not\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
        "    all_decoder_outputs = Variable(torch.zeros(max_targ_len, batch_size, output_lang.n_words))\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_targ_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            \n",
        "            all_decoder_outputs[t] = decoder_output\n",
        "            decoder_input = targets[t]\n",
        "            \n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(max_targ_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            \n",
        "            all_decoder_outputs[di] = decoder_output\n",
        "\n",
        "                \n",
        "    loss = masked_cross_entropy(\n",
        "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
        "    targets.transpose(0, 1).contiguous(),\n",
        "    target_lengths)\n",
        "        \n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters with optimizers\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item()\n",
        "\n",
        "def trainIters(loader, encoder, decoder, n_iters, print_every=1000, plot_every=100, validate_every=1,\n",
        "               learning_rate=0.01,\n",
        "              teacher_forcing_ratio=0.5):\n",
        "    \n",
        "    start = time.time()\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "    plot_losses = []\n",
        "    val_bleu = []\n",
        "\n",
        "    counter = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < n_iters:\n",
        "        epoch += 1\n",
        "\n",
        "        # Get training data for this cycle\n",
        "        for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
        "\n",
        "            counter += 1\n",
        "\n",
        "            # Run the train function\n",
        "            loss = train(\n",
        "                source.long().transpose(0,1).to(device), lengths1, target.long().transpose(0,1).to(device), lengths2,\n",
        "                encoder, decoder,\n",
        "                encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio=teacher_forcing_ratio\n",
        "            )\n",
        "\n",
        "            # Keep track of loss\n",
        "            print_loss_total += loss\n",
        "            plot_loss_total += loss\n",
        "\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_iters), epoch, \n",
        "                                                       epoch / n_iters * 100, print_loss_avg)\n",
        "                print(print_summary)\n",
        "\n",
        "\n",
        "            if counter % plot_every == 0:\n",
        "                plot_loss_avg = plot_loss_total / plot_every\n",
        "                plot_losses.append(plot_loss_avg)\n",
        "                plot_loss_total = 0\n",
        "                \n",
        "                torch.save(encoder.state_dict(), en_loc + '/encoder.pt')\n",
        "                torch.save(decoder.state_dict(), en_loc + '/decoder.pt')\n",
        "\n",
        "                with open(en_loc + '/loss.p', 'wb') as fp:\n",
        "                    pickle.dump(plot_losses, fp)\n",
        "                    \n",
        "                with open(en_loc + '/bleu.p', 'wb') as fp:\n",
        "                    pickle.dump(val_bleu, fp)    \n",
        "                \n",
        "                \n",
        "            if counter % validate_every == 0:\n",
        "                bleu = validate(encoder, decoder, val_loader)\n",
        "                print('Bleu ', bleu)\n",
        "                \n",
        "          \n",
        "\n",
        "    showPlot(plot_losses)\n",
        "    return plot_losses, val_bleu\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lengths, translated, search='greedy', max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Function that generate translation.\n",
        "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
        "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
        "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
        "    And collect the attention for each output words.\n",
        "    @param encoder: the encoder network\n",
        "    @param decoder: the decoder network\n",
        "    @param sentence: string, a sentence in source language to be translated\n",
        "    @param max_length: the max # of words that the decoder can return\n",
        "    @output decoded_words: a list of words in target language\n",
        "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
        "    \"\"\"    \n",
        "    # process input sentence\n",
        "    with torch.no_grad():\n",
        "        input_tensor = sentence.transpose(0,1).to(device)\n",
        "        input_length = sentence.size()[0]\n",
        "        \n",
        "        # encode the source lanugage\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor, input_lengths, None)\n",
        "\n",
        "        decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden[:1] # Use last (forward) hidden state from encoder \n",
        "        # output of this function\n",
        "        decoded_words = ''\n",
        "\n",
        "        for di in range(max_length):\n",
        "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            \n",
        "            # hint: print out decoder_output and decoder_attention\n",
        "            # TODO: add your code here to populate decoded_words and decoder_attentions\n",
        "            # TODO: do this in 2 ways discussed in class: greedy & beam_search\n",
        "            \n",
        "            # GREEDY\n",
        "            topv, topi = decoder_output.data.topk(1) \n",
        "\n",
        "            if topi.item() == EOS_token:\n",
        "                #decoded_words.append('<EOS>')\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                if topi.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
        "                    decoded_words = decoded_words + ' ' + output_lang.index2word[topi.item()]\n",
        "            \n",
        "            decoder_input = topi[0].detach()\n",
        "        \n",
        "        translation = ''\n",
        "        for i in translated: #expected translation\n",
        "            if i.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
        "                translation = translation + ' ' + output_lang_v.index2word[i.item()]\n",
        "\n",
        "        return decoded_words, translation\n",
        "    \n",
        "    \n",
        "def evaluate_batch(loader, encoder, decoder):\n",
        "    \n",
        "    decoded_sentences = []\n",
        "    actual_sentences = []\n",
        "    \n",
        "    for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
        "        #iterate over batch\n",
        "        \n",
        "        for n in range(len(source)):\n",
        "            # Go sentence by sentence\n",
        "            \n",
        "            decoded, actual = evaluate(encoder, decoder, source[n].unsqueeze(0), lengths1[n], target[n])\n",
        "            decoded_sentences.append(decoded)\n",
        "            actual_sentences.append(actual)\n",
        "            \n",
        "    return decoded_sentences, actual_sentences\n",
        "\n",
        "\n",
        "def validate(encoder, decoder, val_loader):\n",
        "    decoded_sentences, actual_sentences = evaluate_batch(val_loader, encoder, decoder)\n",
        "    bleu = evaluate_bleu(decoded_sentences, actual_sentences)\n",
        "    \n",
        "    \n",
        "    return bleu\n",
        "\n",
        "\n",
        "def evaluate_bleu(translation_list, reference_list):\n",
        "    \n",
        "    return corpus_bleu(translation_list, [reference_list])\n",
        "\n",
        "#Plot results\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    \n",
        "\n",
        "def as_minutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def time_since(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40LmMo7FNrde",
        "colab_type": "code",
        "outputId": "4ada27bb-1d11-42da-befc-6a18994ed68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData(en_loc+'/train.tok.vi', en_loc+'/train.tok.en', None, False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 121083 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "vi 37273\n",
            "en 47339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fFUUsUS_vCcT",
        "colab_type": "code",
        "outputId": "5c9653d7-1ac7-4c92-d1ab-66559f53d2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "input_lang_v, output_lang_v, pairs_v = prepareData(en_loc+'/dev.tok.vi', en_loc+'/dev.tok.en', num_sent=None)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 1143 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "vi 3347\n",
            "en 3287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z-16KwF45n4V",
        "colab_type": "code",
        "outputId": "36eb1579-4ce6-4571-e284-4a16bf00a110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "pairs[0:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Khoa_học đằng_sau một tiêu_đề về khí_hậu',\n",
              "  'Rachel Pike  The science behind a climate headline'),\n",
              " ('Tôi muốn cho các bạn biết về sự to_lớn của những nỗ_lực khoa_học đã góp_phần làm_nên các dòng tít bạn thường thấy trên báo',\n",
              "  'I d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper'),\n",
              " ('Có những dòng trông như thế_này khi bàn về biến_đổi khí_hậu  và như thế_này khi nói về chất_lượng không_khí hay khói bụi',\n",
              "  'Headlines that look like this when they have to do with climate change  and headlines that look like this when they have to do with air quality or smog'),\n",
              " ('Cả hai đều là một nhánh của cùng một lĩnh_vực trong ngành khoa_học khí_quyển',\n",
              "  'They are both two branches of the same field of atmospheric science'),\n",
              " ('Các tiêu_đề gần_đây trông như thế_này khi Ban Điều_hành Biến_đổi khí_hậu Liên_chính_phủ  gọi tắt là IPCC đưa ra_bài nghiên_cứu của họ về hệ_thống khí_quyển',\n",
              "  'Recently the headlines looked like this when the Intergovernmental Panel on Climate Change  or IPCC  put out their report on the state of understanding of the atmospheric system')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "e7chm6BdvKpj",
        "colab_type": "code",
        "outputId": "b4f70977-cfa2-4a2e-8657-3ac5910023ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "pairs_v[0:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Khi tôi còn nhỏ  Tôi nghĩ rằng BắcTriều Tiên là đất_nước tốt nhất trên thế_giới và tôi thường hát bài \" Chúng_ta chẳng có gì phải ghen_tị  \"',\n",
              "  'When I was little  I thought my country was the best on the planet  and I grew up singing a song called  Nothing To Envy'),\n",
              " ('Tôi đã rất tự_hào về đất_nước tôi', 'And I was very proud'),\n",
              " ('Ở trường  chúng_tôi dành rất nhiều thời_gian để học về cuộc_đời của chủ_tịch Kim II - Sung  nhưng lại không học nhiều về thế_giới bên_ngoài  ngoại_trừ việc Hoa_Kỳ  Hàn_Quốc và Nhật_Bản là kẻ_thù của chúng_tôi',\n",
              "  'In school  we spent a lot of time studying the history of Kim Il-Sung  but we never learned much about the outside world  except that America  South Korea  Japan are the enemies'),\n",
              " ('Mặc_dù tôi đã từng tự_hỏi không biết thế_giới bên_ngoài kia như thế_nào  nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộc_đời ở BắcTriều Tiên  cho tới khi tất_cả mọi thứ đột_nhiên thay_đổi',\n",
              "  'Although I often wondered about the outside world  I thought I would spend my entire life in North Korea  until everything suddenly changed'),\n",
              " ('Khi tôi lên 7  tôi chứng_kiến cảnh người_ta xử_bắn công_khai lần đầu_tiên trong đời  nhưng tôi vẫn nghĩ cuộc_sống của mình ở đây là hoàn_toàn bình_thường',\n",
              "  'When I was seven years old  I saw my first public execution  but I thought my life in North Korea was normal')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "NHwVSNAU31LU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size=265\n",
        "\n",
        "encoder = EncoderRNN(hidden_size = hidden_size, vocab_size = input_lang.n_words).to(device)\n",
        "encoder.load_state_dict(torch.load(en_loc + '/encoder.pt'))\n",
        "decoder = DecoderRNN(hidden_size = hidden_size, vocab_size = output_lang.n_words).to(device)\n",
        "decoder.load_state_dict(torch.load(en_loc + '/decoder.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vGjIUX7KOXaY",
        "colab_type": "code",
        "outputId": "c0e24145-8e76-455d-b207-170fbb51b0ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset = VocabDataset(pairs, input_lang.word2index, output_lang.word2index)\n",
        "\n",
        "BATCH_SIZE=32\n",
        "\n",
        "# 1 batch input dimension: num_sentences x max sentence length\n",
        "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_dataset = VocabDataset(pairs_v, input_lang_v.word2index, output_lang_v.word2index)\n",
        "# 1 batch input dimension: num_sentences x max sentence length\n",
        "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "plot_losses, bleu = trainIters(train_loader, encoder, decoder, n_iters=5, \n",
        "                         print_every=100\n",
        "                         , plot_every=100, validate_every = 500, learning_rate=0.001, teacher_forcing_ratio=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7m 34s (- 30m 18s) (1 20%) 6.5960\n",
            "15m 12s (- 60m 50s) (1 20%) 6.5861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ywpBaT2B3CGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "senc = EncoderRNN(hidden_size = hidden_size, vocab_size = input_lang.n_words)\n",
        "enc.load_state_dict(torch.load(en_loc + 'encoder.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSrjsTovObM8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoded, actual = evaluate_batch(train_loader, encoder, decoder)\n",
        "\n",
        "for i in zip(decoded, actual):\n",
        "    if i == 10:\n",
        "        break\n",
        "    print('\\n')\n",
        "    print('Expected:', i[1])\n",
        "    print('Actual:' ,i[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yOCJ9jhIbQUi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluate_bleu(decoded, actual)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNi9-FA77GXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}