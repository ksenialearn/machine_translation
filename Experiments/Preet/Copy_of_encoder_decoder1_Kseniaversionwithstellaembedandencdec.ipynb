{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of encoder_decoder1-Kseniaversionwithstellaembedandencdec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "sP4TOAqdM1ia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0uWjT6lM7eL",
        "colab_type": "code",
        "outputId": "3b647e9f-4095-464e-cbdf-67e695704e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AyXb-dQXM3dy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import functional\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "MAX_LENGTH = 40 #temp\n",
        "\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "\n",
        "PAD_IDX = 0 \n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "UNK_IDX = 3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtj5cLvUN0We",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_loc = 'gdrive/My Drive/iwslt-vi-en'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JA_3OsKzOvml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_emb_matrix(language):  \n",
        "  words_to_load = 50000\n",
        "  count=0\n",
        "  if language == 'english':\n",
        "    file = 'wiki-news-300d-1M.vec'\n",
        "  if language == 'vietnamese':\n",
        "    file = 'wiki.vi.vec'\n",
        "  with open('gdrive/My Drive/'+file) as f:\n",
        "    #remove the first line\n",
        "    firstLine = f.readline()\n",
        "    loaded_embeddings = np.zeros((words_to_load + 4, 300))\n",
        "    words2id = {}\n",
        "    idx2words = {}\n",
        "    for i, line in enumerate(f):\n",
        "      if count >= 50000: \n",
        "        break\n",
        "      s = line.split()\n",
        "      if len(s[1:])==300:\n",
        "        loaded_embeddings[count + 4 , :] = np.asarray(s[1:])\n",
        "        words2id[s[0]] = count + 4\n",
        "        idx2words[count + 4] = s[0]\n",
        "        count=count+1\n",
        "    words2id['<SOS>'] = SOS_token\n",
        "    words2id['<EOS>'] = EOS_token\n",
        "    words2id['<pad>'] = PAD_IDX\n",
        "    words2id['<unk>'] = UNK_IDX\n",
        "    idx2words[SOS_token] = '<SOS>'\n",
        "    idx2words[EOS_token] = '<EOD>'\n",
        "    idx2words[PAD_IDX] = '<pad>'\n",
        "    idx2words[UNK_IDX] = '<unk>'\n",
        "    return words2id,idx2words,loaded_embeddings\n",
        "\n",
        "def generate_weights_matrix(idx2words,loaded_embeddings):\n",
        "  matrix_len = len(idx2words)\n",
        "  weights_matrix = np.zeros((matrix_len, 300))\n",
        "  for key in idx2words.keys():\n",
        "    try: \n",
        "      weights_matrix[key] = loaded_embeddings[key]\n",
        "    except KeyError:\n",
        "      weights_matrix[key] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
        "  return weights_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_u3-V6kHoO0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "words2id_eng,idx2words_eng,loaded_embeddings_eng = load_emb_matrix('english')\n",
        "words2id_vi,idx2words_vi,loaded_embeddings_vi = load_emb_matrix('vietnamese')\n",
        "\n",
        "pkl.dump(words2id_eng, open(en_loc + '/words2id_eng.pkl', 'wb'))\n",
        "pkl.dump(idx2words_eng, open(en_loc +'/idx2words_eng.pkl', 'wb'))\n",
        "pkl.dump(loaded_embeddings_eng, open(en_loc+'/embedding_matrix_eng.pkl', 'wb'))\n",
        "\n",
        "pkl.dump(words2id_vi, open(en_loc + '/words2id_vi.pkl', 'wb'))\n",
        "pkl.dump(idx2words_vi, open(en_loc + '/idx2words_vi.pkl', 'wb'))\n",
        "pkl.dump(loaded_embeddings_vi, open(en_loc +'/embedding_matrix_vi.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FpZ-lE8Zv7jX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights_matrix_eng = generate_weights_matrix(idx2words_eng,loaded_embeddings_eng)\n",
        "pkl.dump(weights_matrix_eng, open(en_loc + '/weights_matrix_eng.pkl', 'wb'))\n",
        "weights_matrix_eng = torch.from_numpy(weights_matrix_eng).to(device)\n",
        "\n",
        "weights_matrix_vi = generate_weights_matrix(idx2words_vi,loaded_embeddings_vi)\n",
        "pkl.dump(weights_matrix_vi, open(en_loc + '/weights_matrix_vi.pkl', 'wb'))\n",
        "weights_matrix_vi = torch.from_numpy(weights_matrix_vi).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dU8LsH2D_GMc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    batch_size = sequence_length.size(0)\n",
        "    seq_range = torch.range(0, max_len - 1).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
        "    seq_range_expand = Variable(seq_range_expand)\n",
        "    if sequence_length.is_cuda:\n",
        "        seq_range_expand = seq_range_expand.cuda()\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    return seq_range_expand < seq_length_expand\n",
        "\n",
        "\n",
        "def masked_cross_entropy(logits, target, length):\n",
        "    length = Variable(torch.LongTensor(length))\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "    \"\"\"\n",
        "\n",
        "    # logits_flat: (batch * max_len, num_classes)\n",
        "    logits_flat = logits.view(-1, logits.size(-1)).to(device)\n",
        "    # log_probs_flat: (batch * max_len, num_classes)\n",
        "    log_probs_flat = functional.log_softmax(logits_flat)\n",
        "    # target_flat: (batch * max_len, 1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    # losses_flat: (batch * max_len, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    # losses: (batch, max_len)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    # mask: (batch, max_len)\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1)).to(device)\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / length.float().sum().to(device)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Remove punctuation\n",
        "def removePunctuation(s):\n",
        "\n",
        "    to_remove = ('&lt;', '&gt;', '&amp;', '&apos;', '&quot;')\n",
        "    table = str.maketrans(dict.fromkeys('.!?:,'))\n",
        "    s = s.translate(table)\n",
        "    for i in to_remove:\n",
        "        s=s.replace(i,'')   \n",
        "    s = s.strip()\n",
        "    \n",
        "    return s\n",
        "\n",
        "\n",
        "from typing import List\n",
        "from collections import Counter, namedtuple\n",
        "from itertools import zip_longest\n",
        "\n",
        "def tokenize_13a(line):\n",
        "    \"\"\"\n",
        "    Tokenizes an input line using a relatively minimal tokenization that is however equivalent to mteval-v13a, used by WMT.\n",
        "    :param line: a segment to tokenize\n",
        "    :return: the tokenized line\n",
        "    \"\"\"\n",
        "\n",
        "    norm = line\n",
        "\n",
        "    # language-independent part:\n",
        "    norm = norm.replace('<skipped>', '')\n",
        "    norm = norm.replace('-\\n', '')\n",
        "    norm = norm.replace('\\n', ' ')\n",
        "    norm = norm.replace('&quot;', '\"')\n",
        "    norm = norm.replace('&amp;', '&')\n",
        "    norm = norm.replace('&lt;', '<')\n",
        "    norm = norm.replace('&gt;', '>')\n",
        "\n",
        "    # language-dependent part (assuming Western languages):\n",
        "    norm = \" {} \".format(norm)\n",
        "    norm = re.sub(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', ' \\\\1 ', norm)\n",
        "    norm = re.sub(r'([^0-9])([\\.,])', '\\\\1 \\\\2 ', norm)  # tokenize period and comma unless preceded by a digit\n",
        "    norm = re.sub(r'([\\.,])([^0-9])', ' \\\\1 \\\\2', norm)  # tokenize period and comma unless followed by a digit\n",
        "    norm = re.sub(r'([0-9])(-)', '\\\\1 \\\\2 ', norm)  # tokenize dash when preceded by a digit\n",
        "    norm = re.sub(r'\\s+', ' ', norm)  # one space only between words\n",
        "    norm = re.sub(r'^\\s+', '', norm)  # no leading space\n",
        "    norm = re.sub(r'\\s+$', '', norm)  # no trailing space\n",
        "\n",
        "    return norm\n",
        "\n",
        "def corpus_bleu(sys_stream, ref_streams, smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n",
        "                 use_effective_order=False):\n",
        "    \"\"\"Produces BLEU scores along with its sufficient statistics from a source against one or more references.\n",
        "    :param sys_stream: The system stream (a sequence of segments)\n",
        "    :param ref_streams: A list of one or more reference streams (each a sequence of segments)\n",
        "    :param smooth: The smoothing method to use\n",
        "    :param smooth_floor: For 'floor' smoothing, the floor to use\n",
        "    :param force: Ignore data that looks already tokenized\n",
        "    :param lowercase: Lowercase the data\n",
        "    :param tokenize: The tokenizer to use\n",
        "    :return: a BLEU object containing everything you'd want\n",
        "    \"\"\"\n",
        "\n",
        "    # Add some robustness to the input arguments\n",
        "    if isinstance(sys_stream, str):\n",
        "        sys_stream = [sys_stream]\n",
        "    if isinstance(ref_streams, str):\n",
        "        ref_streams = [[ref_streams]]\n",
        "\n",
        "    sys_len = 0\n",
        "    ref_len = 0\n",
        "\n",
        "    correct = [0 for n in range(NGRAM_ORDER)]\n",
        "    total = [0 for n in range(NGRAM_ORDER)]\n",
        "    \n",
        "\n",
        "    # look for already-tokenized sentences\n",
        "    tokenized_count = 0\n",
        "\n",
        "    fhs = [sys_stream] + ref_streams\n",
        "    for lines in zip_longest(*fhs):\n",
        "        if None in lines:\n",
        "            raise EOFError(\"Source and reference streams have different lengths!\")\n",
        "\n",
        "        if lowercase:\n",
        "            lines = [x.lower() for x in lines]\n",
        "            \n",
        "        tokenize= 'tokenize_13a'    \n",
        "\n",
        "        if not (force or tokenize == 'none') and lines[0].rstrip().endswith(' .'):\n",
        "            tokenized_count += 1\n",
        "\n",
        "            if tokenized_count == 100:\n",
        "                logging.warning('That\\'s 100 lines that end in a tokenized period (\\'.\\')')\n",
        "                logging.warning('It looks like you forgot to detokenize your test data, which may hurt your score.')\n",
        "                logging.warning('If you insist your data is detokenized, or don\\'t care, you can suppress this message with \\'--force\\'.')\n",
        "\n",
        "        output, *refs = [tokenize_13a(x.rstrip()) for x in lines]\n",
        "        \n",
        "\n",
        "        ref_ngrams, closest_diff, closest_len = ref_stats(output, refs)\n",
        "        \n",
        "\n",
        "        sys_len += len(output.split())\n",
        "        ref_len += closest_len\n",
        "\n",
        "        sys_ngrams = extract_ngrams(output)\n",
        "        for ngram in sys_ngrams.keys():\n",
        "            n = len(ngram.split())\n",
        "            correct[n-1] += min(sys_ngrams[ngram], ref_ngrams.get(ngram, 0))\n",
        "            total[n-1] += sys_ngrams[ngram]\n",
        "            \n",
        "\n",
        "    return compute_bleu(correct, total, sys_len, ref_len, smooth, smooth_floor, use_effective_order)\n",
        "  \n",
        "  \n",
        "# n-gram order. Don't change this.\n",
        "NGRAM_ORDER = 4\n",
        "  \n",
        "def compute_bleu(correct: List[int], total: List[int], sys_len: int, ref_len: int, smooth = 'none', smooth_floor = 0.01,\n",
        "                 use_effective_order = False):\n",
        "    \"\"\"Computes BLEU score from its sufficient statistics. Adds smoothing.\n",
        "    :param correct: List of counts of correct ngrams, 1 <= n <= NGRAM_ORDER\n",
        "    :param total: List of counts of total ngrams, 1 <= n <= NGRAM_ORDER\n",
        "    :param sys_len: The cumulative system length\n",
        "    :param ref_len: The cumulative reference length\n",
        "    :param smooth: The smoothing method to use\n",
        "    :param smooth_floor: The smoothing value added, if smooth method 'floor' is used\n",
        "    :param use_effective_order: Use effective order.\n",
        "    :return: A BLEU object with the score (100-based) and other statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    precisions = [0 for x in range(NGRAM_ORDER)]\n",
        "\n",
        "    smooth_mteval = 1.\n",
        "    effective_order = NGRAM_ORDER\n",
        "    for n in range(NGRAM_ORDER):\n",
        "        if total[n] == 0:\n",
        "            break\n",
        "\n",
        "        if use_effective_order:\n",
        "            effective_order = n + 1\n",
        "\n",
        "        if correct[n] == 0:\n",
        "            if smooth == 'exp':\n",
        "                smooth_mteval *= 2\n",
        "                precisions[n] = 100. / (smooth_mteval * total[n])\n",
        "            elif smooth == 'floor':\n",
        "                precisions[n] = 100. * smooth_floor / total[n]\n",
        "        else:\n",
        "            precisions[n] = 100. * correct[n] / total[n]\n",
        "\n",
        "    # If the system guesses no i-grams, 1 <= i <= NGRAM_ORDER, the BLEU score is 0 (technically undefined).\n",
        "    # This is a problem for sentence-level BLEU or a corpus of short sentences, where systems will get no credit\n",
        "    # if sentence lengths fall under the NGRAM_ORDER threshold. This fix scales NGRAM_ORDER to the observed\n",
        "    # maximum order. It is only available through the API and off by default\n",
        "\n",
        "    brevity_penalty = 1.0\n",
        "    if sys_len < ref_len:\n",
        "        brevity_penalty = math.exp(1 - ref_len / sys_len) if sys_len > 0 else 0.0\n",
        "        \n",
        "\n",
        "    bleu = brevity_penalty * math.exp(sum(map(my_log, precisions[:effective_order])) / effective_order)\n",
        "\n",
        "    return bleu \n",
        "  \n",
        "  \n",
        "def ref_stats(output, refs):\n",
        "    ngrams = Counter()\n",
        "    closest_diff = None\n",
        "    closest_len = None\n",
        "    for ref in refs:\n",
        "        tokens = ref.split()\n",
        "        reflen = len(tokens)\n",
        "        diff = abs(len(output.split()) - reflen)\n",
        "        if closest_diff is None or diff < closest_diff:\n",
        "            closest_diff = diff\n",
        "            closest_len = reflen\n",
        "        elif diff == closest_diff:\n",
        "            if reflen < closest_len:\n",
        "                closest_len = reflen\n",
        "\n",
        "        ngrams_ref = extract_ngrams(ref)\n",
        "        for ngram in ngrams_ref.keys():\n",
        "            ngrams[ngram] = max(ngrams[ngram], ngrams_ref[ngram])\n",
        "\n",
        "    return ngrams, closest_diff, closest_len\n",
        "  \n",
        "  \n",
        "def extract_ngrams(line, min_order=1, max_order=NGRAM_ORDER) -> Counter:\n",
        "    \"\"\"Extracts all the ngrams (1 <= n <= NGRAM_ORDER) from a sequence of tokens.\n",
        "    :param line: a segment containing a sequence of words\n",
        "    :param max_order: collect n-grams from 1<=n<=max\n",
        "    :return: a dictionary containing ngrams and counts\n",
        "    \"\"\"\n",
        "\n",
        "    ngrams = Counter()\n",
        "    tokens = line.split()\n",
        "    for n in range(min_order, max_order + 1):\n",
        "        for i in range(0, len(tokens) - n + 1):\n",
        "            ngram = ' '.join(tokens[i: i + n])\n",
        "            ngrams[ngram] += 1\n",
        "\n",
        "    return ngrams  \n",
        "\n",
        "def my_log(num):\n",
        "    \"\"\"\n",
        "    Floors the log function\n",
        "    :param num: the number\n",
        "    :return: log(num) floored to a very low number\n",
        "    \"\"\"\n",
        "\n",
        "    if num == 0.0:\n",
        "        return -9999999999\n",
        "    return math.log(num)\n",
        "  \n",
        "\n",
        "class Lang:\n",
        "#     def __init__(self, name):\n",
        "#         self.name = name\n",
        "#         self.word2index = {}\n",
        "#         self.word2index = {\"PAD\" : 0, \"<SOS>\" : 1, \"<EOS>\" : 2, \"UNK\" : 3}\n",
        "#         self.word2count = {}\n",
        "#         self.index2word = {0: \"PAD\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"UNK\"}\n",
        "#         self.n_words = 4  # Count SOS and EOS and Pad\n",
        "#         self.all_words = []\n",
        "    def __init__(self, name,word2index,index2word):\n",
        "        self.name = name\n",
        "        self.word2index = word2index\n",
        "        #self.word2count = {}\n",
        "        self.index2word = index2word\n",
        "        self.n_words = len(word2index)\n",
        "\n",
        "#     def addSentence(self, sentence):\n",
        "#         'Add all words from all sentences'\n",
        "#         for word in sentence.split(' '):\n",
        "#             if word.strip(): #if not empty space\n",
        "#                 self.all_words.append(word)\n",
        "                \n",
        "                \n",
        "#     def build_vocab(self, vocab_size=MAX_VOCAB_SIZE):\n",
        "#         'Build vocabulary of vocab_size most common words'\n",
        "        \n",
        "#         token_counter = Counter(self.all_words)\n",
        "#         vocab, count = zip(*token_counter.most_common(vocab_size)) #* unzips the tuples\n",
        "#         for word in vocab:\n",
        "#             self.addWord(word)\n",
        "\n",
        "#     def addWord(self, word):\n",
        "#         if word not in self.word2index:\n",
        "#             self.word2index[word] = self.n_words\n",
        "#             self.word2count[word] = 1\n",
        "#             self.index2word[self.n_words] = word\n",
        "#             self.n_words += 1\n",
        "#         else:\n",
        "#             self.word2count[word] += 1\n",
        "            \n",
        "def remove_blanks(pair):\n",
        "    '''Remove empty lines'''\n",
        "    if len(pair[0]) == 0 and len(pair[1]) == 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def set_max_length(pair, max_length=MAX_LENGTH):\n",
        "    if len(pair[0].split(' ')) > max_length or len(pair[1].split(' '))>max_length:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def readLangs(filename1, filename2):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    with open(filename1, encoding='utf-8') as f:\n",
        "        lines1 = f.read().strip().split('\\n')\n",
        "        \n",
        "    with open(filename2, encoding='utf-8') as f:\n",
        "        lines2 = f.read().strip().split('\\n')   \n",
        "        \n",
        "    # Remove punctuation\n",
        "    lines1 = [removePunctuation(l) for l in lines1]\n",
        "    lines2 = [removePunctuation(l) for l in lines2]\n",
        "              \n",
        "#     # Reverse pairs, make Lang instances\n",
        "#     if reverse: #change from english->french to french->english for example\n",
        "#         pairs =list(zip(lines2, lines1))\n",
        "#         input_lang = Lang(filename2[-2:]) #take last two letters\n",
        "#         output_lang = Lang(filename1[-2:])\n",
        "#     else:\n",
        "    pairs =list(zip(lines1, lines2))\n",
        "    input_lang = Lang(\"vi\",words2id_vi,idx2words_vi)\n",
        "    output_lang = Lang(\"en\",words2id_eng,idx2words_eng)\n",
        "\n",
        "    pairs = list(filter(remove_blanks, pairs))  \n",
        "    pairs = list(filter(set_max_length, pairs))\n",
        "\n",
        "    return input_lang, output_lang, pairs \n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, num_sent=None):\n",
        "    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    \n",
        "    pairs = pairs[:num_sent]\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    \n",
        "    print(\"Counting words...\")\n",
        "#     for pair in pairs:\n",
        "#         input_lang.addSentence(pair[0])\n",
        "#         output_lang.addSentence(pair[1])\n",
        "        \n",
        "#     input_lang.build_vocab()\n",
        "#     output_lang.build_vocab()\n",
        "        \n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    return input_lang, output_lang, pairs\n",
        "  \n",
        "class VocabDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_tuple, word2id_lang1, word2id_lang2):\n",
        "        \"\"\"\n",
        "        @param data_list: list of character\n",
        "        @param target_list: list of targets\n",
        "\n",
        "        \"\"\"\n",
        "        self.data_list1, self.data_list2 = zip(*data_tuple)\n",
        "        assert (len(self.data_list1) == len(self.data_list2))\n",
        "        self.word2id1 = word2id_lang1\n",
        "        self.word2id2 = word2id_lang2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list1)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        input_sentence = [self.word2id1[c] if c in self.word2id1.keys() \n",
        "                         else UNK_IDX for c in self.data_list1[key].split()][:MAX_LENGTH-1]\n",
        "        input_sentence.append(EOS_token)\n",
        "                                                                   \n",
        "        output_sentence = [self.word2id2[c] if c in self.word2id2.keys() \n",
        "                          else UNK_IDX for c in self.data_list2[key].split()][:MAX_LENGTH-1]\n",
        "        output_sentence.append(EOS_token)\n",
        "\n",
        "        return [input_sentence, output_sentence, len(input_sentence), len(output_sentence)]\n",
        "\n",
        "def vocab_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all\n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    data_list1 = []\n",
        "    data_list2 = []\n",
        "    length_list1 = []\n",
        "    length_list2 = []\n",
        "     \n",
        "    # padding\n",
        "    for datum in batch:\n",
        "        x1 = datum[0]\n",
        "        x2 = datum[1]\n",
        "        len1 = datum[2]\n",
        "        len2 = datum[3]\n",
        "        \n",
        "        length_list1.append(len1)\n",
        "        length_list2.append(len2)\n",
        "        #Pad first sentences\n",
        "        padded_vec1 = np.pad(np.array(x1),\n",
        "                                pad_width=((0,MAX_LENGTH-len1)),\n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        data_list1.append(padded_vec1)\n",
        "        \n",
        "        #Pad second sentences\n",
        "        padded_vec2 = np.pad(np.array(x2),\n",
        "                        pad_width=((0,MAX_LENGTH-len2)),\n",
        "                        mode=\"constant\", constant_values=0)\n",
        "        data_list2.append(padded_vec2)\n",
        "        \n",
        "    data_list1 = np.array(data_list1)\n",
        "    data_list2 = np.array(data_list2)\n",
        "    length_list1 = np.array(length_list1)\n",
        "    lenth_list2 = np.array(length_list2)\n",
        "    \n",
        "    return [torch.from_numpy(np.array(data_list1)), \n",
        "            torch.from_numpy(np.array(data_list2)),\n",
        "            torch.LongTensor(length_list1), \n",
        "            torch.LongTensor(length_list2)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self,weights_matrix, hidden_size, vocab_size, dropout=0):\n",
        "        '''Bidirectional RNN'''\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Embedding input: max_length x batch_size\n",
        "        # Embedding output: max_length x batch_size x hidden size\n",
        "        #self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0) #vocab size x hidden size\n",
        "        \n",
        "        \n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim,padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(weights_matrix)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        # Input: (max_length x batch_size x hidden_size)\n",
        "        # Output: hidden - 2 x batch_size x hidden_size\n",
        "        # Output: outputs max_length x batch_size x hidden_size*2\n",
        "        self.gru = nn.GRU(self.embedding_dim, hidden_size, dropout=self.dropout, bidirectional=False)\n",
        "        #Stellas\n",
        "        #self.gru = nn.GRU(self.embedding_dim, hidden_size, n_layers, bidirectional=True)\n",
        "        \n",
        "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
        "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
        "        embedded = self.embedding(input_seqs)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "#         output, output_len = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
        "#         output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
        "        return outputs, hidden\n",
        "\n",
        "# class EncoderRNN(nn.Module):\n",
        "#     def __init__(self, weights_matrix, input_size, hidden_size,n_layers=1):\n",
        "#         super(EncoderRNN, self).__init__()\n",
        "     \n",
        "        \n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.input_size = input_size\n",
        "#         self.n_layers = n_layers\n",
        "#         self.batch_size = BATCH_SIZE\n",
        "#         self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "        \n",
        "#         self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "#         self.embedding.weight.data.copy_(weights_matrix)\n",
        "#         self.embedding.weight.requires_grad = False\n",
        "\n",
        "        \n",
        "#         self.gru = nn.GRU(self.embedding_dim, hidden_size, n_layers, bidirectional=True)\n",
        "        \n",
        "\n",
        "#     def forward(self, input_seqs, input_len, hidden=None):\n",
        "\n",
        "       \n",
        "#         embedded = self.embedding(input_seqs)\n",
        "#         packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_len)\n",
        "#         output, hidden = self.gru(packed, hidden)\n",
        "\n",
        "#         output, output_len = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
        "#         output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
        "        \n",
        "#         return output,hidden\n",
        "\n",
        "\n",
        "# class DecoderRNN(nn.Module):\n",
        "#     def __init__(self, weights_matrix, hidden_size, output_size,n_layers=1):\n",
        "#         super(DecoderRNN, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.output_size = output_size\n",
        "#         self.n_layers = n_layers\n",
        "#         self.batch_size = BATCH_SIZE\n",
        "#         self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "        \n",
        "#         #self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "#         self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "#         self.embedding.weight.data.copy_(weights_matrix)\n",
        "#         self.embedding.weight.requires_grad = False\n",
        "        \n",
        "#         self.gru1 = nn.GRU(self.embedding_dim, hidden_size,n_layers)\n",
        "#         self.gru2 = nn.GRU(hidden_size, hidden_size,n_layers)\n",
        "        \n",
        "#         self.out = nn.Linear(hidden_size, output_size)\n",
        "#         self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "#     def forward(self, input_seq, hidden):\n",
        "        \n",
        "#         embedded = self.embedding(input_seq) # dim = Batch_Size x embedding_dim\n",
        "#         embedded = embedded.view(1, self.batch_size, self.embedding_dim) # S=1 x Batch_Size x embedding_dim\n",
        "        \n",
        "#         rnn_output, hidden = self.gru1(embedded, hidden)\n",
        "#         output = F.relu(rnn_output)\n",
        "        \n",
        "#         output, hidden = self.gru2(output, hidden)\n",
        "#         output = self.softmax(self.out(output[0]))\n",
        "        \n",
        "#         return output,hidden\n",
        "\n",
        "\n",
        "\n",
        "#     def initHidden(self):\n",
        "#         return torch.zeros(1, 1, self.hidden_size).to(device)\n",
        "    \n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, weights_matrix, hidden_size, vocab_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "        #self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "        self.embedding.weight.data.copy_(weights_matrix)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        \n",
        "        self.gru = nn.GRU(self.embedding_dim, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inp, hidden):\n",
        "        embedded = self.embedding(inp).unsqueeze(0) #so that we have 1 x batch x hidden\n",
        "        #print('embedded', embedded.size())\n",
        "        output = F.relu(embedded)\n",
        "        #print('after relu', output.size())\n",
        "        #print('hidden size', hidden.size())\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "    \n",
        "    \n",
        "def train(inputs, input_lengths, targets, target_lengths, \n",
        "          encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH,\n",
        "         teacher_forcing_ratio=0.5):\n",
        "    \n",
        "    # Zero gradients of both optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss = 0 #\n",
        "    batch_size = inputs.size()[1]\n",
        "    #print('input size', inputs.size())\n",
        "    #print('batch size', batch_size)\n",
        "    max_targ_len = max_length\n",
        "\n",
        "    # Run words through encoder\n",
        "    _, encoder_hidden = encoder(inputs, input_lengths, None)\n",
        "\n",
        "    \n",
        "    # Prepare input and output variables\n",
        "    decoder_input = torch.LongTensor([SOS_token] * batch_size).to(device)\n",
        "    decoder_hidden = encoder_hidden#[:1] # Use last (forward) hidden state from encoder\n",
        "    \n",
        "    #print('time 1 size', decoder_input.size())\n",
        "    #print('time 1 hidden size', decoder_hidden.size())\n",
        "    \n",
        "    #randomly use teacher forcing or not\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
        "    all_decoder_outputs = Variable(torch.zeros(max_targ_len, batch_size, output_lang.n_words))\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_targ_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            \n",
        "            all_decoder_outputs[t] = decoder_output\n",
        "            decoder_input = targets[t]\n",
        "            \n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(max_targ_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            \n",
        "            all_decoder_outputs[di] = decoder_output\n",
        "\n",
        "                \n",
        "    loss = masked_cross_entropy(\n",
        "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
        "    targets.transpose(0, 1).contiguous(),\n",
        "    target_lengths)\n",
        "        \n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters with optimizers\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item()\n",
        "\n",
        "def trainIters(loader, encoder, decoder, n_iters, print_every=1000, plot_every=100, validate_every=1,\n",
        "               learning_rate=0.01,\n",
        "              teacher_forcing_ratio=0.5):\n",
        "    \n",
        "    start = time.time()\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "    plot_losses = []\n",
        "    val_bleu = []\n",
        "\n",
        "    counter = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < n_iters:\n",
        "        epoch += 1\n",
        "\n",
        "        # Get training data for this cycle\n",
        "        for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
        "\n",
        "            counter += 1\n",
        "\n",
        "            # Run the train function\n",
        "            loss = train(\n",
        "                source.long().transpose(0,1).to(device), lengths1, target.long().transpose(0,1).to(device), lengths2,\n",
        "                encoder, decoder,\n",
        "                encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio=teacher_forcing_ratio\n",
        "            )\n",
        "\n",
        "            # Keep track of loss\n",
        "            print_loss_total += loss\n",
        "            plot_loss_total += loss\n",
        "\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_iters), epoch, \n",
        "                                                       epoch / n_iters * 100, print_loss_avg)\n",
        "                print(print_summary)\n",
        "\n",
        "\n",
        "            if counter % plot_every == 0:\n",
        "                plot_loss_avg = plot_loss_total / plot_every\n",
        "                plot_losses.append(plot_loss_avg)\n",
        "                plot_loss_total = 0\n",
        "                \n",
        "                torch.save(encoder.state_dict(), en_loc + '/encoder.pt')\n",
        "                torch.save(decoder.state_dict(), en_loc + '/decoder.pt')\n",
        "\n",
        "                with open(en_loc + '/loss.p', 'wb') as fp:\n",
        "                    pickle.dump(plot_losses, fp)\n",
        "                    \n",
        "                with open(en_loc + '/bleu.p', 'wb') as fp:\n",
        "                    pickle.dump(val_bleu, fp)    \n",
        "                \n",
        "                \n",
        "            if counter % validate_every == 0:\n",
        "                bleu = validate(encoder, decoder, val_loader)\n",
        "                print('Bleu ', bleu)\n",
        "                \n",
        "          \n",
        "\n",
        "    showPlot(plot_losses)\n",
        "    return plot_losses, val_bleu\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lengths, translated, search='greedy', max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Function that generate translation.\n",
        "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
        "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
        "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
        "    And collect the attention for each output words.\n",
        "    @param encoder: the encoder network\n",
        "    @param decoder: the decoder network\n",
        "    @param sentence: string, a sentence in source language to be translated\n",
        "    @param max_length: the max # of words that the decoder can return\n",
        "    @output decoded_words: a list of words in target language\n",
        "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
        "    \"\"\"    \n",
        "    # process input sentence\n",
        "    with torch.no_grad():\n",
        "        input_tensor = sentence.transpose(0,1).to(device)\n",
        "        input_length = sentence.size()[0]\n",
        "        \n",
        "        # encode the source lanugage\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor, input_lengths, None)\n",
        "\n",
        "        decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden[:1] # Use last (forward) hidden state from encoder \n",
        "        # output of this function\n",
        "        decoded_words = ''\n",
        "\n",
        "        for di in range(max_length):\n",
        "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            \n",
        "            # hint: print out decoder_output and decoder_attention\n",
        "            # TODO: add your code here to populate decoded_words and decoder_attentions\n",
        "            # TODO: do this in 2 ways discussed in class: greedy & beam_search\n",
        "            \n",
        "            # GREEDY\n",
        "            topv, topi = decoder_output.data.topk(1) \n",
        "\n",
        "            if topi.item() == EOS_token:\n",
        "                #decoded_words.append('<EOS>')\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                if topi.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
        "                    decoded_words = decoded_words + ' ' + output_lang.index2word[topi.item()]\n",
        "            \n",
        "            decoder_input = topi[0].detach()\n",
        "        \n",
        "        translation = ''\n",
        "        for i in translated: #expected translation\n",
        "            if i.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
        "                translation = translation + ' ' + output_lang_v.index2word[i.item()]\n",
        "\n",
        "        return decoded_words, translation\n",
        "    \n",
        "    \n",
        "def evaluate_batch(loader, encoder, decoder):\n",
        "    \n",
        "    decoded_sentences = []\n",
        "    actual_sentences = []\n",
        "    \n",
        "    for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
        "        #iterate over batch\n",
        "        \n",
        "        for n in range(len(source)):\n",
        "            # Go sentence by sentence\n",
        "            \n",
        "            decoded, actual = evaluate(encoder, decoder, source[n].unsqueeze(0), lengths1[n], target[n])\n",
        "            decoded_sentences.append(decoded)\n",
        "            actual_sentences.append(actual)\n",
        "            \n",
        "    return decoded_sentences, actual_sentences\n",
        "\n",
        "\n",
        "def validate(encoder, decoder, val_loader):\n",
        "    decoded_sentences, actual_sentences = evaluate_batch(val_loader, encoder, decoder)\n",
        "    bleu = evaluate_bleu(decoded_sentences, actual_sentences)\n",
        "    \n",
        "    \n",
        "    return bleu\n",
        "\n",
        "\n",
        "def evaluate_bleu(translation_list, reference_list):\n",
        "    \n",
        "    return corpus_bleu(translation_list, [reference_list])\n",
        "\n",
        "#Plot results\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    \n",
        "\n",
        "def as_minutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def time_since(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40LmMo7FNrde",
        "colab_type": "code",
        "outputId": "e9ac1d66-9d8a-4fbd-c51b-2b6ebceeeaa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData(en_loc+'/train.tok.vi', en_loc+'/train.tok.en', None)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 121083 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "vi 49955\n",
            "en 50004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fFUUsUS_vCcT",
        "colab_type": "code",
        "outputId": "279cabbb-5359-4ae7-d128-da648c382e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "input_lang_v, output_lang_v, pairs_v = prepareData(en_loc+'/dev.tok.vi', en_loc+'/dev.tok.en', num_sent=None)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 1143 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "vi 49955\n",
            "en 50004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z-16KwF45n4V",
        "colab_type": "code",
        "outputId": "514cfca8-8a29-4ce0-e904-28d9411d9c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "cell_type": "code",
      "source": [
        "pairs[0:5]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Khoa_hc ng_sau mt tiu_ v kh_hu',\n",
              "  'Rachel Pike  The science behind a climate headline'),\n",
              " ('Ti mun cho cc bn bit v s to_ln ca nhng n_lc khoa_hc  gp_phn lm_nn cc dng tt bn thng thy trn bo',\n",
              "  'I d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper'),\n",
              " ('C nhng dng trng nh th_ny khi bn v bin_i kh_hu  v nh th_ny khi ni v cht_lng khng_kh hay khi bi',\n",
              "  'Headlines that look like this when they have to do with climate change  and headlines that look like this when they have to do with air quality or smog'),\n",
              " ('C hai u l mt nhnh ca cng mt lnh_vc trong ngnh khoa_hc kh_quyn',\n",
              "  'They are both two branches of the same field of atmospheric science'),\n",
              " ('Cc tiu_ gn_y trng nh th_ny khi Ban iu_hnh Bin_i kh_hu Lin_chnh_ph  gi tt l IPCC a ra_bi nghin_cu ca h v h_thng kh_quyn',\n",
              "  'Recently the headlines looked like this when the Intergovernmental Panel on Climate Change  or IPCC  put out their report on the state of understanding of the atmospheric system')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "e7chm6BdvKpj",
        "colab_type": "code",
        "outputId": "02f2f67e-d267-4f81-da37-e8c26a76e8ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "cell_type": "code",
      "source": [
        "pairs_v[0:5]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Khi ti cn nh  Ti ngh rng BcTriu Tin l t_nc tt nht trn th_gii v ti thng ht bi \" Chng_ta chng c g phi ghen_t  \"',\n",
              "  'When I was little  I thought my country was the best on the planet  and I grew up singing a song called  Nothing To Envy'),\n",
              " ('Ti  rt t_ho v t_nc ti', 'And I was very proud'),\n",
              " (' trng  chng_ti dnh rt nhiu thi_gian  hc v cuc_i ca ch_tch Kim II - Sung  nhng li khng hc nhiu v th_gii bn_ngoi  ngoi_tr vic Hoa_K  Hn_Quc v Nht_Bn l k_th ca chng_ti',\n",
              "  'In school  we spent a lot of time studying the history of Kim Il-Sung  but we never learned much about the outside world  except that America  South Korea  Japan are the enemies'),\n",
              " ('Mc_d ti  tng t_hi khng bit th_gii bn_ngoi kia nh th_no  nhng ti vn ngh rng mnh s sng c cuc_i  BcTriu Tin  cho ti khi tt_c mi th t_nhin thay_i',\n",
              "  'Although I often wondered about the outside world  I thought I would spend my entire life in North Korea  until everything suddenly changed'),\n",
              " ('Khi ti ln 7  ti chng_kin cnh ngi_ta x_bn cng_khai ln u_tin trong i  nhng ti vn ngh cuc_sng ca mnh  y l hon_ton bnh_thng',\n",
              "  'When I was seven years old  I saw my first public execution  but I thought my life in North Korea was normal')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "NHwVSNAU31LU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "BATCH_SIZE=32\n",
        "hidden_size=265\n",
        "\n",
        "encoder = EncoderRNN(weights_matrix_vi,hidden_size = hidden_size, vocab_size = input_lang.n_words).to(device)\n",
        "\n",
        "#encoder = EncoderRNN(weights_matrix_vi, input_lang.n_words, hidden_size,n_layers = 2).to(device)\n",
        "#encoder.load_state_dict(torch.load(en_loc + '/encoder.pt'))\n",
        "decoder = DecoderRNN(weights_matrix_eng,hidden_size = hidden_size, vocab_size = output_lang.n_words).to(device)\n",
        "\n",
        "#decoder = DecoderRNN(weights_matrix_eng, hidden_size, output_lang.n_words,n_layers = 2).to(device)\n",
        "\n",
        "#decoder.load_state_dict(torch.load(en_loc + '/decoder.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vGjIUX7KOXaY",
        "colab_type": "code",
        "outputId": "ac7e63eb-a12c-4b73-d1a3-7c3b583fd5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1051
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset = VocabDataset(pairs,input_lang.word2index, output_lang.word2index)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_dataset = VocabDataset(pairs_v, input_lang_v.word2index, output_lang_v.word2index)\n",
        "# 1 batch input dimension: num_sentences x max sentence length\n",
        "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plot_losses, bleu = trainIters(train_loader, encoder, decoder, n_iters=5, \n",
        "                         print_every=100\n",
        "                         , plot_every=100, validate_every = 500, learning_rate=0.001, teacher_forcing_ratio=0.5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7804574cd85c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m plot_losses, bleu = trainIters(train_loader, encoder, decoder, n_iters=5, \n\u001b[1;32m     21\u001b[0m                          \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                          , plot_every=100, validate_every = 500, learning_rate=0.001, teacher_forcing_ratio=0.5)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-5debb02b31cb>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(loader, encoder, decoder, n_iters, print_every, plot_every, validate_every, learning_rate, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m                 \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m             )\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-5debb02b31cb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inputs, input_lengths, targets, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;31m# Run words through encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-5debb02b31cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seqs, input_lengths, hidden)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;31m# Note: we run this all at once (over multiple batches of multiple sequences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;31m#         output, output_len = torch.nn.utils.rnn.pad_packed_sequence(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# fast pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmight_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_tensors_permissive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackPadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/packing.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, lengths, batch_first)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mprev_l\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'lengths' array has to be sorted in decreasing order\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'lengths' array has to be sorted in decreasing order"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ywpBaT2B3CGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "senc = EncoderRNN(hidden_size = hidden_size, vocab_size = input_lang.n_words)\n",
        "enc.load_state_dict(torch.load(en_loc + 'encoder.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSrjsTovObM8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoded, actual = evaluate_batch(train_loader, encoder, decoder)\n",
        "\n",
        "for i in zip(decoded, actual):\n",
        "    if i == 10:\n",
        "        break\n",
        "    print('\\n')\n",
        "    print('Expected:', i[1])\n",
        "    print('Actual:' ,i[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yOCJ9jhIbQUi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluate_bleu(decoded, actual)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNi9-FA77GXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}