{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import functional\n",
    "\n",
    "import io\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "MAX_LENGTH = 30 #temp\n",
    "\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "\n",
    "PAD_IDX = 0 \n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "UNK_IDX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(torch.LongTensor(length))\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1)).to(device)\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    #log_probs_flat = functional.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(logits_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1)).to(device)\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum().to(device)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "def removePunctuation(s):\n",
    "    to_remove = ('&lt;', '&gt;', '&amp;', '&apos;', '&quot;')\n",
    "    #table = str.maketrans(dict.fromkeys('.!?:,'))\n",
    "    #s = s.translate(table)\n",
    "    for i in to_remove:\n",
    "        s=s.replace(i,'')   \n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from collections import Counter, namedtuple\n",
    "from itertools import zip_longest\n",
    "\n",
    "def tokenize_13a(line):\n",
    "    \"\"\"\n",
    "    Tokenizes an input line using a relatively minimal tokenization that is however equivalent to mteval-v13a, used by WMT.\n",
    "    :param line: a segment to tokenize\n",
    "    :return: the tokenized line\n",
    "    \"\"\"\n",
    "    norm = line\n",
    "\n",
    "    # language-independent part:\n",
    "    norm = norm.replace('<skipped>', '')\n",
    "    norm = norm.replace('-\\n', '')\n",
    "    norm = norm.replace('\\n', ' ')\n",
    "    norm = norm.replace('&quot;', '\"')\n",
    "    norm = norm.replace('&amp;', '&')\n",
    "    norm = norm.replace('&lt;', '<')\n",
    "    norm = norm.replace('&gt;', '>')\n",
    "\n",
    "    # language-dependent part (assuming Western languages):\n",
    "    norm = \" {} \".format(norm)\n",
    "    norm = re.sub(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', ' \\\\1 ', norm)\n",
    "    norm = re.sub(r'([^0-9])([\\.,])', '\\\\1 \\\\2 ', norm)  # tokenize period and comma unless preceded by a digit\n",
    "    norm = re.sub(r'([\\.,])([^0-9])', ' \\\\1 \\\\2', norm)  # tokenize period and comma unless followed by a digit\n",
    "    norm = re.sub(r'([0-9])(-)', '\\\\1 \\\\2 ', norm)  # tokenize dash when preceded by a digit\n",
    "    norm = re.sub(r'\\s+', ' ', norm)  # one space only between words\n",
    "    norm = re.sub(r'^\\s+', '', norm)  # no leading space\n",
    "    norm = re.sub(r'\\s+$', '', norm)  # no trailing space\n",
    "\n",
    "    return norm\n",
    "\n",
    "def corpus_bleu(sys_stream, ref_streams, smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n",
    "                 use_effective_order=False):\n",
    "    \"\"\"Produces BLEU scores along with its sufficient statistics from a source against one or more references.\n",
    "    :param sys_stream: The system stream (a sequence of segments)\n",
    "    :param ref_streams: A list of one or more reference streams (each a sequence of segments)\n",
    "    :param smooth: The smoothing method to use\n",
    "    :param smooth_floor: For 'floor' smoothing, the floor to use\n",
    "    :param force: Ignore data that looks already tokenized\n",
    "    :param lowercase: Lowercase the data\n",
    "    :param tokenize: The tokenizer to use\n",
    "    :return: a BLEU object containing everything you'd want\n",
    "    \"\"\"\n",
    "\n",
    "    # Add some robustness to the input arguments\n",
    "    if isinstance(sys_stream, str):\n",
    "        sys_stream = [sys_stream]\n",
    "    if isinstance(ref_streams, str):\n",
    "        ref_streams = [[ref_streams]]\n",
    "\n",
    "    sys_len = 0\n",
    "    ref_len = 0\n",
    "\n",
    "    correct = [0 for n in range(NGRAM_ORDER)]\n",
    "    total = [0 for n in range(NGRAM_ORDER)]\n",
    "    \n",
    "    # look for already-tokenized sentences\n",
    "    tokenized_count = 0\n",
    "\n",
    "    fhs = [sys_stream] + ref_streams\n",
    "    for lines in zip_longest(*fhs):\n",
    "        if None in lines:\n",
    "            raise EOFError(\"Source and reference streams have different lengths!\")\n",
    "\n",
    "        if lowercase:\n",
    "            lines = [x.lower() for x in lines]\n",
    "            \n",
    "        tokenize= 'tokenize_13a'    \n",
    "\n",
    "        if not (force or tokenize == 'none') and lines[0].rstrip().endswith(' .'):\n",
    "            tokenized_count += 1\n",
    "\n",
    "            #if tokenized_count == 100:\n",
    "                #logging.warning('That\\'s 100 lines that end in a tokenized period (\\'.\\')')\n",
    "                #logging.warning('It looks like you forgot to detokenize your test data, which may hurt your score.')\n",
    "                #logging.warning('If you insist your data is detokenized, or don\\'t care, you can suppress this message with \\'--force\\'.')\n",
    "\n",
    "        output, *refs = [tokenize_13a(x.rstrip()) for x in lines]\n",
    "        \n",
    "\n",
    "        ref_ngrams, closest_diff, closest_len = ref_stats(output, refs)\n",
    "        \n",
    "\n",
    "        sys_len += len(output.split())\n",
    "        ref_len += closest_len\n",
    "\n",
    "        sys_ngrams = extract_ngrams(output)\n",
    "        for ngram in sys_ngrams.keys():\n",
    "            n = len(ngram.split())\n",
    "            correct[n-1] += min(sys_ngrams[ngram], ref_ngrams.get(ngram, 0))\n",
    "            total[n-1] += sys_ngrams[ngram]\n",
    "            \n",
    "    return compute_bleu(correct, total, sys_len, ref_len, smooth, smooth_floor, use_effective_order)\n",
    "  \n",
    "  \n",
    "# n-gram order. Don't change this.\n",
    "NGRAM_ORDER = 4\n",
    "  \n",
    "def compute_bleu(correct: List[int], total: List[int], sys_len: int, ref_len: int, smooth = 'none', smooth_floor = 0.01,\n",
    "                 use_effective_order = False):\n",
    "    \"\"\"Computes BLEU score from its sufficient statistics. Adds smoothing.\n",
    "    :param correct: List of counts of correct ngrams, 1 <= n <= NGRAM_ORDER\n",
    "    :param total: List of counts of total ngrams, 1 <= n <= NGRAM_ORDER\n",
    "    :param sys_len: The cumulative system length\n",
    "    :param ref_len: The cumulative reference length\n",
    "    :param smooth: The smoothing method to use\n",
    "    :param smooth_floor: The smoothing value added, if smooth method 'floor' is used\n",
    "    :param use_effective_order: Use effective order.\n",
    "    :return: A BLEU object with the score (100-based) and other statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    precisions = [0 for x in range(NGRAM_ORDER)]\n",
    "\n",
    "    smooth_mteval = 1.\n",
    "    effective_order = NGRAM_ORDER\n",
    "    for n in range(NGRAM_ORDER):\n",
    "        if total[n] == 0:\n",
    "            break\n",
    "\n",
    "        if use_effective_order:\n",
    "            effective_order = n + 1\n",
    "\n",
    "        if correct[n] == 0:\n",
    "            if smooth == 'exp':\n",
    "                smooth_mteval *= 2\n",
    "                precisions[n] = 100. / (smooth_mteval * total[n])\n",
    "            elif smooth == 'floor':\n",
    "                precisions[n] = 100. * smooth_floor / total[n]\n",
    "        else:\n",
    "            precisions[n] = 100. * correct[n] / total[n]\n",
    "\n",
    "    # If the system guesses no i-grams, 1 <= i <= NGRAM_ORDER, the BLEU score is 0 (technically undefined).\n",
    "    # This is a problem for sentence-level BLEU or a corpus of short sentences, where systems will get no credit\n",
    "    # if sentence lengths fall under the NGRAM_ORDER threshold. This fix scales NGRAM_ORDER to the observed\n",
    "    # maximum order. It is only available through the API and off by default\n",
    "\n",
    "    brevity_penalty = 1.0\n",
    "    if sys_len < ref_len:\n",
    "        brevity_penalty = math.exp(1 - ref_len / sys_len) if sys_len > 0 else 0.0\n",
    "        \n",
    "\n",
    "    bleu = brevity_penalty * math.exp(sum(map(my_log, precisions[:effective_order])) / effective_order)\n",
    "\n",
    "    return bleu \n",
    "  \n",
    "def ref_stats(output, refs):\n",
    "    ngrams = Counter()\n",
    "    closest_diff = None\n",
    "    closest_len = None\n",
    "    for ref in refs:\n",
    "        tokens = ref.split()\n",
    "        reflen = len(tokens)\n",
    "        diff = abs(len(output.split()) - reflen)\n",
    "        if closest_diff is None or diff < closest_diff:\n",
    "            closest_diff = diff\n",
    "            closest_len = reflen\n",
    "        elif diff == closest_diff:\n",
    "            if reflen < closest_len:\n",
    "                closest_len = reflen\n",
    "\n",
    "        ngrams_ref = extract_ngrams(ref)\n",
    "        for ngram in ngrams_ref.keys():\n",
    "            ngrams[ngram] = max(ngrams[ngram], ngrams_ref[ngram])\n",
    "\n",
    "    return ngrams, closest_diff, closest_len\n",
    "  \n",
    "\n",
    "def extract_ngrams(line, min_order=1, max_order=NGRAM_ORDER) -> Counter:\n",
    "    \"\"\"Extracts all the ngrams (1 <= n <= NGRAM_ORDER) from a sequence of tokens.\n",
    "    :param line: a segment containing a sequence of words\n",
    "    :param max_order: collect n-grams from 1<=n<=max\n",
    "    :return: a dictionary containing ngrams and counts\n",
    "    \"\"\"\n",
    "\n",
    "    ngrams = Counter()\n",
    "    tokens = line.split()\n",
    "    for n in range(min_order, max_order + 1):\n",
    "        for i in range(0, len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i: i + n])\n",
    "            ngrams[ngram] += 1\n",
    "\n",
    "    return ngrams  \n",
    "\n",
    "def my_log(num):\n",
    "    \"\"\"\n",
    "    Floors the log function\n",
    "    :param num: the number\n",
    "    :return: log(num) floored to a very low number\n",
    "    \"\"\"\n",
    "\n",
    "    if num == 0.0:\n",
    "        return -9999999999\n",
    "    return math.log(num)\n",
    "  \n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        #self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 4  # Count SOS and EOS and Pad\n",
    "        self.all_words = []\n",
    "        \n",
    "    def build_vocab(self, embedding, max_vocab_size = 100000):\n",
    "\n",
    "    # save index 1 for unk and 0 for pad\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    #all_tokens = [item for sublist in all_tokens for item in sublist]\n",
    "    #max_len = max([len(word) for word in all_tokens])\n",
    "    \n",
    "    \n",
    "        unique_words = list(embedding.keys())[0:max_vocab_size]\n",
    "\n",
    "        index2word =  unique_words #list of words available in embedding\n",
    "        index2word = ['<pad>', '<sos>', '<eos>','<unk>'] + index2word #add pad and unknown to the beginning\n",
    "\n",
    "        word2index = dict(zip(unique_words, range(4,4+len(unique_words)))) # dictionary of words and indices \n",
    "        word2index['<pad>'] = PAD_IDX  #add pad symbol to the dictionary\n",
    "        word2index['<unk>'] = UNK_IDX  #add unkown symbol to the dictionary\n",
    "        word2index['<eos>'] = EOS_IDX\n",
    "        word2index['<sos>'] = SOS_IDX\n",
    "        \n",
    "        self.word2index = word2index\n",
    "        self.index2word = index2word\n",
    "        self.n_words = len(self.word2index)\n",
    "\n",
    "        return word2index, index2word \n",
    "            \n",
    "def remove_blanks(pair):\n",
    "    '''Remove empty lines'''\n",
    "    if len(pair[0]) == 0 or len(pair[1]) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def set_max_length(pair, max_length=MAX_LENGTH):\n",
    "    if len(pair[0].split(' ')) > max_length or len(pair[1].split(' '))>max_length:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def readLangs(filename1, filename2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    with open(filename1, encoding='utf-8') as f:\n",
    "        lines1 = f.read().strip().split('\\n')\n",
    "        \n",
    "    with open(filename2, encoding='utf-8') as f:\n",
    "        lines2 = f.read().strip().split('\\n')   \n",
    "        \n",
    "    # Remove punctuation\n",
    "    lines1 = [removePunctuation(l) for l in lines1]\n",
    "    lines2 = [removePunctuation(l) for l in lines2]\n",
    "              \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse: #change from english->french to french->english for example\n",
    "        pairs =list(zip(lines2, lines1))\n",
    "        input_lang = Lang(filename2[-2:]) #take last two letters\n",
    "        output_lang = Lang(filename1[-2:])\n",
    "    else:\n",
    "        pairs =list(zip(lines1, lines2))\n",
    "        input_lang = Lang(filename1[-2:])\n",
    "        output_lang = Lang(filename2[-2:])\n",
    "            \n",
    "        \n",
    "\n",
    "    pairs = list(filter(remove_blanks, pairs))  \n",
    "    pairs = list(filter(set_max_length, pairs))\n",
    "\n",
    "    return input_lang, output_lang, pairs \n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, embedding_in, embedding_out, num_sent=None, reverse=False):\n",
    "    \n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    \n",
    "    pairs = pairs[:num_sent]\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "        \n",
    "    #print(type(embedding_in))    \n",
    "    input_lang.build_vocab(embedding_in)\n",
    "    output_lang.build_vocab(embedding_out)\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, word2id_lang1, word2id_lang2):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1, self.data_list2 = zip(*data_tuple)\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        self.word2id1 = word2id_lang1\n",
    "        self.word2id2 = word2id_lang2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        input_sentence = [self.word2id1[c] if c in self.word2id1.keys() \n",
    "                         else UNK_IDX for c in self.data_list1[key].split()][:MAX_LENGTH-1]\n",
    "        input_sentence.append(EOS_IDX)\n",
    "                                                                   \n",
    "        output_sentence = [self.word2id2[c] if c in self.word2id2.keys() \n",
    "                          else UNK_IDX for c in self.data_list2[key].split()][:MAX_LENGTH-1]\n",
    "        output_sentence.append(EOS_IDX)\n",
    "\n",
    "        return [input_sentence, output_sentence, len(input_sentence), len(output_sentence)]\n",
    "    \n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "     \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        x1 = datum[0]\n",
    "        x2 = datum[1]\n",
    "        len1 = datum[2]\n",
    "        len2 = datum[3]\n",
    "        \n",
    "        length_list1.append(len1)\n",
    "        length_list2.append(len2)\n",
    "        #Pad first sentences\n",
    "        padded_vec1 = np.pad(np.array(x1),\n",
    "                                pad_width=((0,MAX_LENGTH-len1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "        #Pad second sentences\n",
    "        padded_vec2 = np.pad(np.array(x2),\n",
    "                        pad_width=((0,MAX_LENGTH-len2)),\n",
    "                        mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    data_list1 = np.array(data_list1)\n",
    "    data_list2 = np.array(data_list2)\n",
    "    length_list1 = np.array(length_list1)\n",
    "    lenth_list2 = np.array(length_list2)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list1)), \n",
    "            torch.from_numpy(np.array(data_list2)),\n",
    "            torch.LongTensor(length_list1), \n",
    "            torch.LongTensor(length_list2)]\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, emb_weights, dropout=0):\n",
    "        '''Bidirectional RNN'''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        weight_size = emb_weights.size()[1]\n",
    "        \n",
    "        # Embedding input: max_length x batch_size\n",
    "        # Embedding output: max_length x batch_size x hidden size\n",
    "        self.embedding = nn.Embedding(vocab_size, weight_size, padding_idx=0).from_pretrained(emb_weights, \n",
    "                                                                freeze=True).to(device) #vocab size x hidden size\n",
    "        \n",
    "        # Input: (max_length x batch_size x hidden_size)\n",
    "        # Output: hidden - 2 x batch_size x hidden_size\n",
    "        # Output: outputs max_length x batch_size x hidden_size*2\n",
    "        self.lstm = nn.LSTM(weight_size, hidden_size, dropout=self.dropout, bidirectional=False)\n",
    "\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        outputs, hidden = self.lstm(embedded, hidden)\n",
    "        #outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, emb_weights):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        weight_size = emb_weights.size()[1]\n",
    "        self.embedding = nn.Embedding(vocab_size, weight_size, padding_idx=0).from_pretrained(emb_weights, \n",
    "                                                                freeze=True).to(device)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(weight_size, hidden_size)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        embedded = self.embedding(inp).unsqueeze(0) #so that we have 1 x batch x hidden\n",
    "        #print('embedded', embedded.size())\n",
    "\n",
    "        #print('after relu', output.size())\n",
    "        #print('hidden size', hidden.size())\n",
    "        output, hidden = self.lstm1(embedded, hidden)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm2(output, hidden)\n",
    "        #output = F.relu(embedded)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "        \n",
    "    \n",
    "def train(inputs, input_lengths, targets, target_lengths, \n",
    "          encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH,\n",
    "         teacher_forcing_ratio=0.5, clip = 50):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 #\n",
    "    batch_size = inputs.size()[1]\n",
    "    #print('input size', inputs.size())\n",
    "    #print('batch size', batch_size)\n",
    "    max_targ_len = max_length\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(inputs, input_lengths, None)\n",
    "\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = torch.LongTensor([SOS_IDX] * batch_size).to(device)\n",
    "    decoder_hidden = encoder_hidden#[:1] \n",
    "    \n",
    "    #print('time 1 size', decoder_input.size())\n",
    "    #print('time 1 hidden size', decoder_hidden.size())\n",
    "    \n",
    "    #randomly use teacher forcing or not\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_targ_len, batch_size, output_lang.n_words)).to(device)\n",
    "    \n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_targ_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            decoder_input = targets[t]\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_targ_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "\n",
    "    loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    targets.transpose(0, 1).contiguous(),\n",
    "    target_lengths)\n",
    "        \n",
    "    loss.backward()\n",
    "        \n",
    "    # Clip gradient norms\n",
    "    clip = clip\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    \n",
    "    \n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def trainIters(loader, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iters, print_every=1000, \n",
    "               validate_every = 100,\n",
    "               plot_every=100, learning_rate=0.01,\n",
    "              teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    start = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    plot_losses = []\n",
    "\n",
    "    counter = 0\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch < n_iters:\n",
    "        epoch += 1\n",
    "\n",
    "        # Get training data for this cycle\n",
    "        for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
    "\n",
    "            counter += 1\n",
    "            \n",
    "\n",
    "            # Run the train function\n",
    "            loss = train(\n",
    "                source.long().transpose(0,1).to(device), lengths1, target.long().transpose(0,1).to(device), lengths2,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio=teacher_forcing_ratio\n",
    "            )\n",
    "\n",
    "            # Keep track of loss\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_iters), epoch, \n",
    "                                                       epoch / n_iters * 100, print_loss_avg)\n",
    "                \n",
    "\n",
    "                print(print_summary)\n",
    "\n",
    "\n",
    "            if counter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "                torch.save(encoder.state_dict(), en_loc  + '/' +'encoder_embed3.pt')\n",
    "                torch.save(decoder.state_dict(), en_loc  + '/' +'decoder_embed3.pt')\n",
    "                torch.save(encoder_optimizer.state_dict(), en_loc + '/' + 'encoder_opt.pt')\n",
    "                torch.save(decoder_optimizer.state_dict(), en_loc + '/' + 'decoder_opt.pt')\n",
    "                \n",
    "\n",
    "                with open(en_loc + '/loss_embed.p', 'wb') as fp:\n",
    "                    pickle.dump(plot_losses, fp)\n",
    "                    \n",
    "                #with open(en_loc + '/bleu.p', 'wb') as fp:\n",
    "                #    pickle.dump(val_bleu, fp) \n",
    "                \n",
    "            if counter % validate_every == 0:\n",
    "                bleu = validate(encoder, decoder, val_loader)\n",
    "                print('Bleu ', bleu)\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lengths, translated, search='greedy', max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence.transpose(0,1)\n",
    "        input_length = sentence.size()[0]\n",
    "        \n",
    "        # encode the source lanugage\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor, input_lengths, None)\n",
    "\n",
    "        decoder_input = torch.tensor([SOS_IDX], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden#[:1]  Use last (forward) hidden state from encoder \n",
    "        # output of this function\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "            # hint: print out decoder_output and decoder_attention\n",
    "            # TODO: add your code here to populate decoded_words and decoder_attentions\n",
    "            # TODO: do this in 2 ways discussed in class: greedy & beam_search\n",
    "            \n",
    "            # GREEDY\n",
    "            topv, topi = decoder_output.data.topk(1) \n",
    "\n",
    "            if topi.item() == EOS_IDX:\n",
    "                #decoded_words.append('<EOS>')\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                if topi.item() not in [SOS_IDX, EOS_IDX, UNK_IDX, PAD_IDX]:\n",
    "                    decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            \n",
    "            decoder_input = topi[0].detach()\n",
    "        \n",
    "        translation = []\n",
    "        for i in translated: #expected translation\n",
    "            if i.item() not in [SOS_IDX, EOS_IDX, UNK_IDX, PAD_IDX]:\n",
    "                translation.append(output_lang.index2word[i.item()])\n",
    "\n",
    "        return decoded_words, translation\n",
    "    \n",
    "    \n",
    "def evaluate_batch(loader, encoder, decoder):\n",
    "    \n",
    "    decoded_sentences = []\n",
    "    actual_sentences = []\n",
    "    \n",
    "    for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
    "        #iterate over batch\n",
    "        \n",
    "        for n in range(len(source)):\n",
    "            # Go sentence by sentence\n",
    "            \n",
    "            decoded, actual = evaluate(encoder, decoder, source[n].unsqueeze(0).to(device), lengths1[n], target[n])\n",
    "            decoded_sentences.append(decoded)\n",
    "            actual_sentences.append(actual)\n",
    "            \n",
    "    return decoded_sentences, actual_sentences\n",
    "\n",
    "\n",
    "def evaluate_bleu(translation_list, reference_list):\n",
    "     \n",
    "    translations = [' '.join(v) for v in translation_list]\n",
    "    references = [' '.join(v) for v in reference_list]\n",
    "    \n",
    "    return corpus_bleu(translations, [references])\n",
    "\n",
    "\n",
    "def validate(encoder, decoder, val_loader):\n",
    "    decoded_sentences, actual_sentences = evaluate_batch(val_loader, encoder, decoder)\n",
    "    bleu = evaluate_bleu(decoded_sentences, actual_sentences)\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "#Plot results\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))    \n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    \"\"\"\n",
    "    Function that takes in attention and visualize the attention.\n",
    "    @param - input_sentence: string the represent a list of words from source language\n",
    "    @param - output_words: the gold translation in target language\n",
    "    @param - attentions: a numpy array\n",
    "    \"\"\"\n",
    "    # Set up figure with colorbar    \n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # TODO: Add your code here to visualize the attention\n",
    "    # look at documentation for imshow https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.matshow.html\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder, decoder, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "def embed_to_tensor(embeddings):\n",
    "    y=np.array([np.array(list(xi)) for xi in embeddings.values()])\n",
    "    padding = np.zeros((1, y.shape[1]))\n",
    "    unknown = np.random.rand(1, y.shape[1]) # to account for Padding and Unknown\n",
    "    sos = np.random.rand(1, y.shape[1])\n",
    "    eos = np.random.rand(1, y.shape[1])\n",
    "    full_size = np.concatenate([padding, sos, eos, unknown, y], axis=0)\n",
    "    emb_weights = torch.from_numpy(full_size)\n",
    "    \n",
    "    return emb_weights   \n",
    "\n",
    "\n",
    "def load_embedding(fname, max_count=None):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    counter=0\n",
    "    for line in fin:\n",
    "        counter+=1\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "        if counter==max_count:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_loc = 'iwslt-vi-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VIETNAMESE\n",
    "#Read in pretrained embedding vectors - subset for now\n",
    "embeddings_map_vi = load_embedding(en_loc + '/cc.vi.300.vec', max_count=50000)\n",
    "\n",
    "#Convert embedding values to lists\n",
    "embeddings_vi = {}\n",
    "\n",
    "for key, value in embeddings_map_vi.items():\n",
    "    embeddings_vi[key] = list(value)\n",
    "    \n",
    "### ENGLISH    \n",
    "#Read in pretrained embedding vectors - subset for now\n",
    "embeddings_map_en = load_embedding(en_loc + '/wiki-news-300d-1M.vec', max_count=50000)\n",
    "\n",
    "#Convert embedding values to lists\n",
    "embeddings_en = {}\n",
    "\n",
    "for key, value in embeddings_map_en.items():\n",
    "    embeddings_en[key] = list(value)\n",
    "    \n",
    "embed_vi_tensor = embed_to_tensor(embeddings_vi).float()\n",
    "embed_en_tensor = embed_to_tensor(embeddings_en).float()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 106391 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 50004\n",
      "en 50004\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(en_loc+'/train.tok.vi', en_loc+'/train.tok.en', \n",
    "                                             embedding_in = embeddings_vi,\n",
    "                                             embedding_out = embeddings_en, num_sent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 969 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 50004\n",
      "en 50004\n"
     ]
    }
   ],
   "source": [
    "input_lang_v, output_lang_v, pairs_v = prepareData(en_loc+'/dev.tok.vi', en_loc+'/dev.tok.en', \n",
    "                                                   embedding_in = embeddings_vi,\n",
    "                                                   embedding_out = embeddings_en,\n",
    "                                                   num_sent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Khoa_học đằng_sau một tiêu_đề về khí_hậu',\n",
       "  'Rachel Pike : The science behind a climate headline'),\n",
       " ('Tôi muốn cho các bạn biết về sự to_lớn của những nỗ_lực khoa_học đã góp_phần làm_nên các dòng tít bạn thường thấy trên báo .',\n",
       "  'I d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .'),\n",
       " ('Cả hai đều là một nhánh của cùng một lĩnh_vực trong ngành khoa_học khí_quyển .',\n",
       "  'They are both two branches of the same field of atmospheric science .'),\n",
       " ('Nghiên_cứu được viết bởi 620 nhà khoa_học từ 40 quốc_gia khác nhau .',\n",
       "  'That report was written by 620 scientists from 40 countries .'),\n",
       " ('Họ viết gần 1000 trang về chủ_đề này .',\n",
       "  'They wrote almost a thousand pages on the topic .')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=1000\n",
    "learning_rate=0.001\n",
    "\n",
    "encoder = EncoderRNN(hidden_size = hidden_size, vocab_size = input_lang.n_words,\n",
    "                    emb_weights = embed_vi_tensor).to(device)\n",
    "decoder = DecoderRNN(hidden_size = hidden_size, vocab_size = output_lang.n_words,\n",
    "                    emb_weights = embed_en_tensor).to(device)\n",
    "#encoder.load_state_dict(torch.load(en_loc + '/encoder_embed2.pt'))\n",
    "#decoder.load_state_dict(torch.load(en_loc + '/decoder_embed2.pt'))\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "#encoder_optimizer.load_state_dict(torch.load(en_loc + '/encoder_opt.pt'))\n",
    "#decoder_optimizer.load_state_dict(torch.load(en_loc + '/decoder_opt.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "\n",
    "train_dataset = VocabDataset(pairs, input_lang.word2index, output_lang.word2index)\n",
    "# 1 batch input dimension: num_sentences x max sentence length\n",
    "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(pairs_v, input_lang.word2index, output_lang.word2index)\n",
    "# 1 batch input dimension: num_sentences x max sentence length\n",
    "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ks4841/pytorch-gpu/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 49s (- 7m 23s) (1 10%) 5.1227\n",
      "1m 38s (- 14m 44s) (1 10%) 5.0505\n",
      "2m 30s (- 22m 33s) (1 10%) 5.0317\n",
      "3m 19s (- 29m 55s) (1 10%) 5.0442\n",
      "4m 11s (- 37m 45s) (1 10%) 5.0089\n",
      "5m 0s (- 45m 8s) (1 10%) 4.9807\n",
      "5m 53s (- 52m 57s) (1 10%) 4.9706\n",
      "6m 42s (- 60m 22s) (1 10%) 4.9676\n",
      "7m 34s (- 68m 12s) (1 10%) 4.9415\n",
      "8m 23s (- 75m 34s) (1 10%) 4.8490\n",
      "Bleu  0.5435684140716306\n",
      "9m 57s (- 89m 38s) (1 10%) 4.8800\n",
      "10m 46s (- 97m 0s) (1 10%) 4.8694\n",
      "11m 38s (- 104m 50s) (1 10%) 4.8460\n",
      "12m 28s (- 112m 12s) (1 10%) 4.7721\n",
      "13m 19s (- 119m 59s) (1 10%) 4.8007\n",
      "14m 9s (- 127m 21s) (1 10%) 4.7625\n",
      "15m 1s (- 135m 11s) (1 10%) 4.7656\n",
      "15m 50s (- 142m 34s) (1 10%) 4.7256\n",
      "16m 42s (- 150m 23s) (1 10%) 4.7385\n",
      "17m 31s (- 157m 46s) (1 10%) 4.6973\n",
      "Bleu  0.6622094795315054\n",
      "19m 16s (- 173m 27s) (1 10%) 4.6761\n",
      "20m 5s (- 180m 50s) (1 10%) 4.6399\n",
      "20m 57s (- 188m 41s) (1 10%) 4.7082\n",
      "21m 47s (- 196m 3s) (1 10%) 4.6232\n",
      "22m 39s (- 203m 51s) (1 10%) 4.6434\n",
      "23m 28s (- 211m 13s) (1 10%) 4.6160\n",
      "24m 20s (- 219m 2s) (1 10%) 4.5603\n",
      "25m 9s (- 226m 24s) (1 10%) 4.5527\n",
      "26m 1s (- 234m 12s) (1 10%) 4.5294\n",
      "26m 50s (- 241m 36s) (1 10%) 4.5821\n",
      "Bleu  0.44623343998233583\n",
      "28m 42s (- 258m 22s) (1 10%) 4.5238\n",
      "29m 31s (- 265m 46s) (1 10%) 4.5484\n",
      "30m 24s (- 273m 37s) (1 10%) 4.5023\n",
      "31m 13s (- 281m 0s) (1 10%) 4.5366\n",
      "32m 5s (- 288m 48s) (1 10%) 4.5160\n",
      "32m 54s (- 296m 11s) (1 10%) 4.4762\n",
      "33m 46s (- 303m 59s) (1 10%) 4.4700\n",
      "34m 35s (- 311m 22s) (1 10%) 4.4372\n",
      "35m 27s (- 319m 11s) (1 10%) 4.4535\n",
      "36m 17s (- 326m 35s) (1 10%) 4.4342\n",
      "Bleu  0.6889313990163403\n",
      "38m 4s (- 342m 43s) (1 10%) 4.4696\n",
      "38m 54s (- 350m 7s) (1 10%) 4.4390\n",
      "39m 46s (- 358m 0s) (1 10%) 4.3956\n",
      "40m 35s (- 365m 23s) (1 10%) 4.4124\n",
      "41m 28s (- 373m 13s) (1 10%) 4.4230\n",
      "42m 17s (- 380m 35s) (1 10%) 4.3715\n",
      "43m 9s (- 388m 25s) (1 10%) 4.4182\n",
      "43m 58s (- 395m 48s) (1 10%) 4.4211\n",
      "44m 50s (- 403m 38s) (1 10%) 4.3809\n",
      "45m 40s (- 411m 1s) (1 10%) 4.3686\n",
      "Bleu  0.6903059099898969\n",
      "47m 30s (- 427m 34s) (1 10%) 4.3697\n",
      "48m 19s (- 434m 59s) (1 10%) 4.3543\n",
      "49m 12s (- 442m 49s) (1 10%) 4.3797\n",
      "50m 1s (- 450m 12s) (1 10%) 4.3153\n",
      "50m 54s (- 458m 7s) (1 10%) 4.3286\n",
      "51m 43s (- 465m 29s) (1 10%) 4.3220\n",
      "52m 35s (- 473m 18s) (1 10%) 4.2937\n",
      "53m 24s (- 480m 40s) (1 10%) 4.3148\n",
      "54m 16s (- 488m 30s) (1 10%) 4.3020\n",
      "55m 5s (- 495m 53s) (1 10%) 4.3029\n",
      "Bleu  1.1714913689198416\n",
      "56m 51s (- 511m 42s) (1 10%) 4.3181\n",
      "57m 40s (- 519m 7s) (1 10%) 4.3177\n",
      "58m 32s (- 526m 56s) (1 10%) 4.2496\n",
      "59m 22s (- 534m 19s) (1 10%) 4.2474\n",
      "60m 14s (- 542m 8s) (1 10%) 4.2400\n",
      "61m 3s (- 549m 32s) (1 10%) 4.2357\n",
      "61m 55s (- 247m 42s) (2 20%) 4.1176\n",
      "62m 44s (- 250m 59s) (2 20%) 4.0746\n",
      "63m 36s (- 254m 27s) (2 20%) 4.0857\n",
      "64m 26s (- 257m 44s) (2 20%) 4.0988\n",
      "Bleu  1.1378984385746094\n",
      "66m 12s (- 264m 48s) (2 20%) 4.0795\n",
      "67m 1s (- 268m 6s) (2 20%) 4.0681\n",
      "67m 53s (- 271m 35s) (2 20%) 4.0499\n",
      "68m 43s (- 274m 52s) (2 20%) 4.0720\n",
      "69m 35s (- 278m 21s) (2 20%) 4.0502\n",
      "70m 24s (- 281m 38s) (2 20%) 4.0246\n",
      "71m 16s (- 285m 6s) (2 20%) 4.1151\n",
      "72m 5s (- 288m 23s) (2 20%) 4.0235\n",
      "72m 58s (- 291m 52s) (2 20%) 4.0757\n",
      "73m 47s (- 295m 9s) (2 20%) 4.0455\n",
      "Bleu  1.32690171769951\n",
      "75m 31s (- 302m 7s) (2 20%) 4.0608\n",
      "76m 21s (- 305m 24s) (2 20%) 4.0468\n",
      "77m 13s (- 308m 53s) (2 20%) 4.0231\n",
      "78m 2s (- 312m 11s) (2 20%) 4.0516\n",
      "78m 55s (- 315m 40s) (2 20%) 4.0456\n",
      "79m 44s (- 318m 56s) (2 20%) 4.0217\n",
      "80m 36s (- 322m 25s) (2 20%) 4.0445\n",
      "81m 25s (- 325m 42s) (2 20%) 4.0412\n",
      "82m 17s (- 329m 11s) (2 20%) 4.0154\n",
      "83m 7s (- 332m 28s) (2 20%) 4.0157\n",
      "Bleu  1.1361443029223077\n",
      "84m 51s (- 339m 27s) (2 20%) 4.0104\n",
      "85m 40s (- 342m 43s) (2 20%) 4.0327\n",
      "86m 33s (- 346m 12s) (2 20%) 4.0301\n",
      "87m 22s (- 349m 30s) (2 20%) 4.0124\n",
      "88m 15s (- 353m 1s) (2 20%) 4.0243\n",
      "89m 4s (- 356m 18s) (2 20%) 4.0085\n",
      "89m 56s (- 359m 47s) (2 20%) 4.0289\n",
      "90m 46s (- 363m 4s) (2 20%) 3.9842\n",
      "91m 38s (- 366m 33s) (2 20%) 4.0308\n",
      "92m 27s (- 369m 49s) (2 20%) 3.9678\n",
      "Bleu  1.5822300285989845\n",
      "94m 10s (- 376m 41s) (2 20%) 3.9748\n",
      "94m 59s (- 379m 57s) (2 20%) 3.9528\n",
      "95m 51s (- 383m 26s) (2 20%) 3.9964\n",
      "96m 40s (- 386m 43s) (2 20%) 3.9840\n",
      "97m 33s (- 390m 13s) (2 20%) 3.9867\n",
      "98m 22s (- 393m 30s) (2 20%) 3.9443\n",
      "99m 14s (- 396m 59s) (2 20%) 3.9728\n",
      "100m 4s (- 400m 16s) (2 20%) 3.9248\n",
      "100m 56s (- 403m 46s) (2 20%) 3.9135\n",
      "101m 45s (- 407m 3s) (2 20%) 3.8934\n",
      "Bleu  1.9070433096114237\n",
      "103m 32s (- 414m 11s) (2 20%) 3.9530\n",
      "104m 22s (- 417m 28s) (2 20%) 3.9429\n",
      "105m 14s (- 420m 56s) (2 20%) 3.8980\n",
      "106m 3s (- 424m 13s) (2 20%) 3.9064\n",
      "106m 55s (- 427m 42s) (2 20%) 3.9454\n",
      "107m 45s (- 431m 0s) (2 20%) 3.8945\n",
      "108m 37s (- 434m 29s) (2 20%) 3.9028\n",
      "109m 26s (- 437m 46s) (2 20%) 3.9363\n",
      "110m 19s (- 441m 16s) (2 20%) 3.8949\n",
      "111m 8s (- 444m 33s) (2 20%) 3.9266\n",
      "Bleu  1.896203810498683\n",
      "112m 52s (- 451m 31s) (2 20%) 3.9125\n",
      "113m 41s (- 454m 47s) (2 20%) 3.8959\n",
      "114m 34s (- 458m 17s) (2 20%) 3.9035\n",
      "115m 23s (- 461m 34s) (2 20%) 3.8823\n",
      "116m 15s (- 465m 3s) (2 20%) 3.8895\n",
      "117m 5s (- 468m 21s) (2 20%) 3.8790\n",
      "117m 57s (- 471m 49s) (2 20%) 3.8404\n",
      "118m 46s (- 475m 6s) (2 20%) 3.8819\n",
      "119m 38s (- 478m 34s) (2 20%) 3.8663\n",
      "120m 28s (- 481m 52s) (2 20%) 3.9099\n",
      "Bleu  2.049821604686609\n",
      "122m 14s (- 488m 58s) (2 20%) 3.8445\n",
      "123m 3s (- 492m 15s) (2 20%) 3.8733\n",
      "123m 55s (- 495m 43s) (2 20%) 3.8238\n",
      "124m 45s (- 291m 5s) (3 30%) 3.5986\n",
      "125m 37s (- 293m 7s) (3 30%) 3.5832\n",
      "126m 26s (- 295m 2s) (3 30%) 3.6116\n",
      "127m 19s (- 297m 4s) (3 30%) 3.5812\n",
      "128m 8s (- 299m 0s) (3 30%) 3.6430\n",
      "129m 1s (- 301m 3s) (3 30%) 3.6230\n",
      "129m 50s (- 302m 58s) (3 30%) 3.6213\n",
      "Bleu  2.4681847886562496\n",
      "131m 36s (- 307m 5s) (3 30%) 3.6294\n",
      "132m 25s (- 309m 0s) (3 30%) 3.6006\n",
      "133m 18s (- 311m 2s) (3 30%) 3.6422\n",
      "134m 7s (- 312m 56s) (3 30%) 3.6079\n",
      "134m 59s (- 314m 58s) (3 30%) 3.6188\n",
      "135m 48s (- 316m 53s) (3 30%) 3.6044\n",
      "136m 40s (- 318m 55s) (3 30%) 3.6418\n",
      "137m 30s (- 320m 50s) (3 30%) 3.6063\n",
      "138m 22s (- 322m 53s) (3 30%) 3.6356\n",
      "139m 12s (- 324m 48s) (3 30%) 3.6194\n",
      "Bleu  1.9982075644869173\n",
      "140m 57s (- 328m 54s) (3 30%) 3.6260\n",
      "141m 47s (- 330m 49s) (3 30%) 3.6282\n",
      "142m 39s (- 332m 51s) (3 30%) 3.6364\n",
      "143m 28s (- 334m 46s) (3 30%) 3.6188\n",
      "144m 20s (- 336m 48s) (3 30%) 3.6235\n",
      "145m 10s (- 338m 43s) (3 30%) 3.6094\n",
      "146m 2s (- 340m 45s) (3 30%) 3.6215\n",
      "146m 51s (- 342m 40s) (3 30%) 3.6721\n",
      "147m 44s (- 344m 43s) (3 30%) 3.5951\n",
      "148m 33s (- 346m 38s) (3 30%) 3.5925\n",
      "Bleu  2.6575201351091366\n",
      "150m 18s (- 350m 42s) (3 30%) 3.5680\n",
      "151m 7s (- 352m 37s) (3 30%) 3.6028\n",
      "151m 59s (- 354m 39s) (3 30%) 3.5951\n",
      "152m 48s (- 356m 34s) (3 30%) 3.5872\n",
      "153m 41s (- 358m 35s) (3 30%) 3.6298\n",
      "154m 30s (- 360m 30s) (3 30%) 3.5983\n",
      "155m 22s (- 362m 32s) (3 30%) 3.5999\n",
      "156m 11s (- 364m 27s) (3 30%) 3.5526\n",
      "157m 3s (- 366m 29s) (3 30%) 3.6303\n",
      "157m 53s (- 368m 24s) (3 30%) 3.5958\n",
      "Bleu  2.5049377066522847\n",
      "159m 37s (- 372m 27s) (3 30%) 3.5837\n",
      "160m 26s (- 374m 22s) (3 30%) 3.6006\n",
      "161m 19s (- 376m 24s) (3 30%) 3.5988\n",
      "162m 8s (- 378m 19s) (3 30%) 3.5638\n",
      "163m 0s (- 380m 21s) (3 30%) 3.5959\n",
      "163m 49s (- 382m 16s) (3 30%) 3.5755\n",
      "164m 42s (- 384m 18s) (3 30%) 3.5981\n",
      "165m 31s (- 386m 12s) (3 30%) 3.5443\n",
      "166m 23s (- 388m 15s) (3 30%) 3.5845\n",
      "167m 13s (- 390m 10s) (3 30%) 3.5668\n",
      "Bleu  2.7840981470902175\n",
      "168m 57s (- 394m 14s) (3 30%) 3.5850\n",
      "169m 47s (- 396m 9s) (3 30%) 3.5555\n",
      "170m 39s (- 398m 11s) (3 30%) 3.6077\n",
      "171m 28s (- 400m 7s) (3 30%) 3.5573\n",
      "172m 21s (- 402m 9s) (3 30%) 3.6024\n",
      "173m 10s (- 404m 4s) (3 30%) 3.5904\n",
      "174m 2s (- 406m 6s) (3 30%) 3.5729\n",
      "174m 51s (- 408m 0s) (3 30%) 3.5450\n",
      "175m 44s (- 410m 3s) (3 30%) 3.5516\n",
      "176m 33s (- 411m 58s) (3 30%) 3.5483\n",
      "Bleu  2.762785024184968\n",
      "178m 15s (- 415m 56s) (3 30%) 3.5715\n",
      "179m 5s (- 417m 51s) (3 30%) 3.5353\n",
      "179m 57s (- 419m 54s) (3 30%) 3.5525\n",
      "180m 47s (- 421m 50s) (3 30%) 3.5722\n",
      "181m 40s (- 423m 53s) (3 30%) 3.5506\n",
      "182m 29s (- 425m 49s) (3 30%) 3.5380\n",
      "183m 22s (- 427m 52s) (3 30%) 3.5730\n",
      "184m 12s (- 429m 48s) (3 30%) 3.5628\n",
      "185m 5s (- 431m 51s) (3 30%) 3.5770\n",
      "185m 54s (- 278m 51s) (4 40%) 3.3787\n",
      "Bleu  2.8237495819892566\n",
      "187m 40s (- 281m 30s) (4 40%) 3.2465\n",
      "188m 30s (- 282m 45s) (4 40%) 3.2085\n",
      "189m 22s (- 284m 4s) (4 40%) 3.2209\n",
      "190m 12s (- 285m 18s) (4 40%) 3.2498\n",
      "191m 5s (- 286m 37s) (4 40%) 3.2567\n",
      "191m 55s (- 287m 53s) (4 40%) 3.2325\n",
      "192m 49s (- 289m 13s) (4 40%) 3.2613\n",
      "193m 39s (- 290m 29s) (4 40%) 3.2421\n",
      "194m 33s (- 291m 49s) (4 40%) 3.2410\n",
      "195m 23s (- 293m 5s) (4 40%) 3.2587\n",
      "Bleu  2.7496354677172268\n",
      "197m 10s (- 295m 45s) (4 40%) 3.2498\n",
      "198m 0s (- 297m 0s) (4 40%) 3.2442\n",
      "198m 54s (- 298m 21s) (4 40%) 3.2637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199m 45s (- 299m 38s) (4 40%) 3.2454\n",
      "200m 39s (- 300m 59s) (4 40%) 3.2560\n",
      "201m 30s (- 302m 15s) (4 40%) 3.2585\n",
      "202m 24s (- 303m 36s) (4 40%) 3.2657\n",
      "203m 14s (- 304m 52s) (4 40%) 3.2776\n",
      "204m 9s (- 306m 13s) (4 40%) 3.2703\n",
      "204m 59s (- 307m 29s) (4 40%) 3.3078\n",
      "Bleu  2.8361320407642396\n",
      "206m 46s (- 310m 9s) (4 40%) 3.2665\n",
      "207m 37s (- 311m 25s) (4 40%) 3.3041\n",
      "208m 30s (- 312m 46s) (4 40%) 3.2958\n",
      "209m 21s (- 314m 1s) (4 40%) 3.2637\n",
      "210m 15s (- 315m 22s) (4 40%) 3.2665\n",
      "211m 5s (- 316m 38s) (4 40%) 3.2533\n",
      "211m 59s (- 317m 59s) (4 40%) 3.3164\n",
      "212m 49s (- 319m 13s) (4 40%) 3.2684\n",
      "213m 43s (- 320m 34s) (4 40%) 3.2692\n",
      "214m 33s (- 321m 50s) (4 40%) 3.2719\n",
      "Bleu  2.842771581455258\n",
      "216m 20s (- 324m 30s) (4 40%) 3.2981\n",
      "217m 9s (- 325m 44s) (4 40%) 3.2467\n",
      "218m 2s (- 327m 3s) (4 40%) 3.2367\n",
      "218m 52s (- 328m 18s) (4 40%) 3.2796\n",
      "219m 44s (- 329m 37s) (4 40%) 3.2659\n",
      "220m 34s (- 330m 52s) (4 40%) 3.2625\n",
      "221m 27s (- 332m 10s) (4 40%) 3.2762\n",
      "222m 17s (- 333m 25s) (4 40%) 3.2739\n",
      "223m 9s (- 334m 44s) (4 40%) 3.2627\n",
      "223m 59s (- 335m 59s) (4 40%) 3.3016\n",
      "Bleu  2.9918981527054362\n",
      "225m 45s (- 338m 38s) (4 40%) 3.3101\n",
      "226m 35s (- 339m 53s) (4 40%) 3.2631\n",
      "227m 28s (- 341m 13s) (4 40%) 3.3073\n",
      "228m 18s (- 342m 27s) (4 40%) 3.2438\n",
      "229m 11s (- 343m 46s) (4 40%) 3.3115\n",
      "230m 0s (- 345m 1s) (4 40%) 3.2471\n",
      "230m 53s (- 346m 20s) (4 40%) 3.2981\n",
      "231m 43s (- 347m 34s) (4 40%) 3.2770\n",
      "232m 36s (- 348m 54s) (4 40%) 3.2812\n",
      "233m 25s (- 350m 8s) (4 40%) 3.2873\n",
      "Bleu  3.416322106388568\n",
      "235m 11s (- 352m 47s) (4 40%) 3.2860\n",
      "236m 1s (- 354m 2s) (4 40%) 3.2707\n",
      "236m 54s (- 355m 21s) (4 40%) 3.2851\n",
      "237m 43s (- 356m 35s) (4 40%) 3.3125\n",
      "238m 36s (- 357m 54s) (4 40%) 3.3043\n",
      "239m 26s (- 359m 9s) (4 40%) 3.3131\n",
      "240m 19s (- 360m 28s) (4 40%) 3.2551\n",
      "241m 8s (- 361m 43s) (4 40%) 3.2964\n",
      "242m 1s (- 363m 2s) (4 40%) 3.2778\n",
      "242m 51s (- 364m 16s) (4 40%) 3.2906\n",
      "Bleu  3.136932561044016\n",
      "244m 34s (- 366m 51s) (4 40%) 3.2724\n",
      "245m 24s (- 368m 6s) (4 40%) 3.2742\n",
      "246m 16s (- 369m 25s) (4 40%) 3.2829\n",
      "247m 6s (- 370m 39s) (4 40%) 3.2968\n",
      "247m 59s (- 371m 59s) (4 40%) 3.2700\n",
      "248m 48s (- 373m 13s) (4 40%) 3.3008\n",
      "249m 41s (- 249m 41s) (5 50%) 2.8932\n",
      "250m 31s (- 250m 31s) (5 50%) 2.8995\n",
      "251m 24s (- 251m 24s) (5 50%) 2.9210\n",
      "252m 13s (- 252m 13s) (5 50%) 2.8944\n",
      "Bleu  2.613434490740589\n"
     ]
    }
   ],
   "source": [
    "plot_losses = trainIters(train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iters=10, \n",
    "                         print_every=50, plot_every=100, validate_every = 500, learning_rate=0.001, \n",
    "                                          teacher_forcing_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
