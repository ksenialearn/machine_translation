{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import functional\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "MAX_LENGTH = 5 #temp\n",
    "\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "PAD_IDX = 0 \n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_IDX = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(torch.LongTensor(length))\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = functional.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "def removePunctuation(s):\n",
    "\n",
    "    to_remove = ('&lt;', '&gt;', '&amp;', '&apos;', '&quot;')\n",
    "    table = str.maketrans(dict.fromkeys('.!?:,'))\n",
    "    s = s.translate(table)\n",
    "    for i in to_remove:\n",
    "        s=s.replace(i,'')   \n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from collections import Counter, namedtuple\n",
    "from itertools import zip_longest\n",
    "\n",
    "def tokenize_13a(line):\n",
    "    \"\"\"\n",
    "    Tokenizes an input line using a relatively minimal tokenization that is however equivalent to mteval-v13a, used by WMT.\n",
    "    :param line: a segment to tokenize\n",
    "    :return: the tokenized line\n",
    "    \"\"\"\n",
    "\n",
    "    norm = line\n",
    "\n",
    "    # language-independent part:\n",
    "    norm = norm.replace('<skipped>', '')\n",
    "    norm = norm.replace('-\\n', '')\n",
    "    norm = norm.replace('\\n', ' ')\n",
    "    norm = norm.replace('&quot;', '\"')\n",
    "    norm = norm.replace('&amp;', '&')\n",
    "    norm = norm.replace('&lt;', '<')\n",
    "    norm = norm.replace('&gt;', '>')\n",
    "\n",
    "    # language-dependent part (assuming Western languages):\n",
    "    norm = \" {} \".format(norm)\n",
    "    norm = re.sub(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', ' \\\\1 ', norm)\n",
    "    norm = re.sub(r'([^0-9])([\\.,])', '\\\\1 \\\\2 ', norm)  # tokenize period and comma unless preceded by a digit\n",
    "    norm = re.sub(r'([\\.,])([^0-9])', ' \\\\1 \\\\2', norm)  # tokenize period and comma unless followed by a digit\n",
    "    norm = re.sub(r'([0-9])(-)', '\\\\1 \\\\2 ', norm)  # tokenize dash when preceded by a digit\n",
    "    norm = re.sub(r'\\s+', ' ', norm)  # one space only between words\n",
    "    norm = re.sub(r'^\\s+', '', norm)  # no leading space\n",
    "    norm = re.sub(r'\\s+$', '', norm)  # no trailing space\n",
    "\n",
    "    return norm\n",
    "\n",
    "def corpus_bleu(sys_stream, ref_streams, smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n",
    "                 use_effective_order=False):\n",
    "    \"\"\"Produces BLEU scores along with its sufficient statistics from a source against one or more references.\n",
    "    :param sys_stream: The system stream (a sequence of segments)\n",
    "    :param ref_streams: A list of one or more reference streams (each a sequence of segments)\n",
    "    :param smooth: The smoothing method to use\n",
    "    :param smooth_floor: For 'floor' smoothing, the floor to use\n",
    "    :param force: Ignore data that looks already tokenized\n",
    "    :param lowercase: Lowercase the data\n",
    "    :param tokenize: The tokenizer to use\n",
    "    :return: a BLEU object containing everything you'd want\n",
    "    \"\"\"\n",
    "\n",
    "    # Add some robustness to the input arguments\n",
    "    if isinstance(sys_stream, str):\n",
    "        sys_stream = [sys_stream]\n",
    "    if isinstance(ref_streams, str):\n",
    "        ref_streams = [[ref_streams]]\n",
    "\n",
    "    sys_len = 0\n",
    "    ref_len = 0\n",
    "\n",
    "    correct = [0 for n in range(NGRAM_ORDER)]\n",
    "    total = [0 for n in range(NGRAM_ORDER)]\n",
    "    \n",
    "\n",
    "    # look for already-tokenized sentences\n",
    "    tokenized_count = 0\n",
    "\n",
    "    fhs = [sys_stream] + ref_streams\n",
    "    for lines in zip_longest(*fhs):\n",
    "        if None in lines:\n",
    "            raise EOFError(\"Source and reference streams have different lengths!\")\n",
    "\n",
    "        if lowercase:\n",
    "            lines = [x.lower() for x in lines]\n",
    "            \n",
    "        tokenize= 'tokenize_13a'    \n",
    "\n",
    "        if not (force or tokenize == 'none') and lines[0].rstrip().endswith(' .'):\n",
    "            tokenized_count += 1\n",
    "\n",
    "            if tokenized_count == 100:\n",
    "                logging.warning('That\\'s 100 lines that end in a tokenized period (\\'.\\')')\n",
    "                logging.warning('It looks like you forgot to detokenize your test data, which may hurt your score.')\n",
    "                logging.warning('If you insist your data is detokenized, or don\\'t care, you can suppress this message with \\'--force\\'.')\n",
    "\n",
    "        output, *refs = [tokenize_13a(x.rstrip()) for x in lines]\n",
    "        \n",
    "\n",
    "        ref_ngrams, closest_diff, closest_len = ref_stats(output, refs)\n",
    "        \n",
    "\n",
    "        sys_len += len(output.split())\n",
    "        ref_len += closest_len\n",
    "\n",
    "        sys_ngrams = extract_ngrams(output)\n",
    "        for ngram in sys_ngrams.keys():\n",
    "            n = len(ngram.split())\n",
    "            correct[n-1] += min(sys_ngrams[ngram], ref_ngrams.get(ngram, 0))\n",
    "            total[n-1] += sys_ngrams[ngram]\n",
    "            \n",
    "\n",
    "    return compute_bleu(correct, total, sys_len, ref_len, smooth, smooth_floor, use_effective_order)\n",
    "  \n",
    "  \n",
    "# n-gram order. Don't change this.\n",
    "NGRAM_ORDER = 4\n",
    "  \n",
    "def compute_bleu(correct: List[int], total: List[int], sys_len: int, ref_len: int, smooth = 'none', smooth_floor = 0.01,\n",
    "                 use_effective_order = False):\n",
    "    \"\"\"Computes BLEU score from its sufficient statistics. Adds smoothing.\n",
    "    :param correct: List of counts of correct ngrams, 1 <= n <= NGRAM_ORDER\n",
    "    :param total: List of counts of total ngrams, 1 <= n <= NGRAM_ORDER\n",
    "    :param sys_len: The cumulative system length\n",
    "    :param ref_len: The cumulative reference length\n",
    "    :param smooth: The smoothing method to use\n",
    "    :param smooth_floor: The smoothing value added, if smooth method 'floor' is used\n",
    "    :param use_effective_order: Use effective order.\n",
    "    :return: A BLEU object with the score (100-based) and other statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    precisions = [0 for x in range(NGRAM_ORDER)]\n",
    "\n",
    "    smooth_mteval = 1.\n",
    "    effective_order = NGRAM_ORDER\n",
    "    for n in range(NGRAM_ORDER):\n",
    "        if total[n] == 0:\n",
    "            break\n",
    "\n",
    "        if use_effective_order:\n",
    "            effective_order = n + 1\n",
    "\n",
    "        if correct[n] == 0:\n",
    "            if smooth == 'exp':\n",
    "                smooth_mteval *= 2\n",
    "                precisions[n] = 100. / (smooth_mteval * total[n])\n",
    "            elif smooth == 'floor':\n",
    "                precisions[n] = 100. * smooth_floor / total[n]\n",
    "        else:\n",
    "            precisions[n] = 100. * correct[n] / total[n]\n",
    "\n",
    "    # If the system guesses no i-grams, 1 <= i <= NGRAM_ORDER, the BLEU score is 0 (technically undefined).\n",
    "    # This is a problem for sentence-level BLEU or a corpus of short sentences, where systems will get no credit\n",
    "    # if sentence lengths fall under the NGRAM_ORDER threshold. This fix scales NGRAM_ORDER to the observed\n",
    "    # maximum order. It is only available through the API and off by default\n",
    "\n",
    "    brevity_penalty = 1.0\n",
    "    if sys_len < ref_len:\n",
    "        brevity_penalty = math.exp(1 - ref_len / sys_len) if sys_len > 0 else 0.0\n",
    "        \n",
    "\n",
    "    bleu = brevity_penalty * math.exp(sum(map(my_log, precisions[:effective_order])) / effective_order)\n",
    "\n",
    "    return bleu \n",
    "  \n",
    "  \n",
    "def ref_stats(output, refs):\n",
    "    ngrams = Counter()\n",
    "    closest_diff = None\n",
    "    closest_len = None\n",
    "    for ref in refs:\n",
    "        tokens = ref.split()\n",
    "        reflen = len(tokens)\n",
    "        diff = abs(len(output.split()) - reflen)\n",
    "        if closest_diff is None or diff < closest_diff:\n",
    "            closest_diff = diff\n",
    "            closest_len = reflen\n",
    "        elif diff == closest_diff:\n",
    "            if reflen < closest_len:\n",
    "                closest_len = reflen\n",
    "\n",
    "        ngrams_ref = extract_ngrams(ref)\n",
    "        for ngram in ngrams_ref.keys():\n",
    "            ngrams[ngram] = max(ngrams[ngram], ngrams_ref[ngram])\n",
    "\n",
    "    return ngrams, closest_diff, closest_len\n",
    "  \n",
    "  \n",
    "def extract_ngrams(line, min_order=1, max_order=NGRAM_ORDER) -> Counter:\n",
    "    \"\"\"Extracts all the ngrams (1 <= n <= NGRAM_ORDER) from a sequence of tokens.\n",
    "    :param line: a segment containing a sequence of words\n",
    "    :param max_order: collect n-grams from 1<=n<=max\n",
    "    :return: a dictionary containing ngrams and counts\n",
    "    \"\"\"\n",
    "\n",
    "    ngrams = Counter()\n",
    "    tokens = line.split()\n",
    "    for n in range(min_order, max_order + 1):\n",
    "        for i in range(0, len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i: i + n])\n",
    "            ngrams[ngram] += 1\n",
    "\n",
    "    return ngrams  \n",
    "\n",
    "def my_log(num):\n",
    "    \"\"\"\n",
    "    Floors the log function\n",
    "    :param num: the number\n",
    "    :return: log(num) floored to a very low number\n",
    "    \"\"\"\n",
    "\n",
    "    if num == 0.0:\n",
    "        return -9999999999\n",
    "    return math.log(num)\n",
    "  \n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2index = {\"PAD\" : 0, \"<SOS>\" : 1, \"EOS\" : 2, \"UNK\" : 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"<SOS>\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.n_words = 4  # Count SOS and EOS and Pad\n",
    "        self.all_words = []\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        'Add all words from all sentences'\n",
    "        for word in sentence.split(' '):\n",
    "            if word.strip(): #if not empty space\n",
    "                self.all_words.append(word)\n",
    "                \n",
    "                \n",
    "    def build_vocab(self, vocab_size=MAX_VOCAB_SIZE):\n",
    "        'Build vocabulary of vocab_size most common words'\n",
    "        \n",
    "        token_counter = Counter(self.all_words)\n",
    "        vocab, count = zip(*token_counter.most_common(vocab_size)) #* unzips the tuples\n",
    "        for word in vocab:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def remove_blanks(pair):\n",
    "    '''Remove empty lines'''\n",
    "    if len(pair[0]) == 0 and len(pair[1]) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def set_max_length(pair, max_length=MAX_LENGTH):\n",
    "    if len(pair[0].split(' ')) > max_length or len(pair[1].split(' '))>max_length:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def readLangs(filename1, filename2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    with open(filename1, encoding='utf-8') as f:\n",
    "        lines1 = f.read().strip().split('\\n')\n",
    "        \n",
    "    with open(filename2, encoding='utf-8') as f:\n",
    "        lines2 = f.read().strip().split('\\n')   \n",
    "        \n",
    "    # Remove punctuation\n",
    "    lines1 = [removePunctuation(l) for l in lines1]\n",
    "    lines2 = [removePunctuation(l) for l in lines2]\n",
    "              \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse: #change from english->french to french->english for example\n",
    "        pairs =list(zip(lines2, lines1))\n",
    "        input_lang = Lang(filename2[-2:]) #take last two letters\n",
    "        output_lang = Lang(filename1[-2:])\n",
    "    else:\n",
    "        pairs =list(zip(lines1, lines2))\n",
    "        input_lang = Lang(filename1[-2:])\n",
    "        output_lang = Lang(filename2[-2:])\n",
    "            \n",
    "        \n",
    "\n",
    "    pairs = list(filter(remove_blanks, pairs))  \n",
    "    pairs = list(filter(set_max_length, pairs))\n",
    "\n",
    "    return input_lang, output_lang, pairs \n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, num_sent=None, reverse=False):\n",
    "    \n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    \n",
    "    pairs = pairs[:num_sent]\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    input_lang.build_vocab()\n",
    "    output_lang.build_vocab()\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, word2id_lang1, word2id_lang2):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1, self.data_list2 = zip(*data_tuple)\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        self.word2id1 = word2id_lang1\n",
    "        self.word2id2 = word2id_lang2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        input_sentence = [self.word2id1[c] if c in self.word2id1.keys() \n",
    "                         else UNK_IDX for c in self.data_list1[key].split()][:MAX_LENGTH-1]\n",
    "        input_sentence.append(EOS_token)\n",
    "                                                                   \n",
    "        output_sentence = [self.word2id2[c] if c in self.word2id2.keys() \n",
    "                          else UNK_IDX for c in self.data_list2[key].split()][:MAX_LENGTH-1]\n",
    "        output_sentence.append(EOS_token)\n",
    "\n",
    "        return [input_sentence, output_sentence, len(input_sentence), len(output_sentence)]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "     \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        x1 = datum[0]\n",
    "        x2 = datum[1]\n",
    "        len1 = datum[2]\n",
    "        len2 = datum[3]\n",
    "        \n",
    "        length_list1.append(len1)\n",
    "        length_list2.append(len2)\n",
    "        #Pad first sentences\n",
    "        padded_vec1 = np.pad(np.array(x1),\n",
    "                                pad_width=((0,MAX_LENGTH-len1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "        #Pad second sentences\n",
    "        padded_vec2 = np.pad(np.array(x2),\n",
    "                        pad_width=((0,MAX_LENGTH-len2)),\n",
    "                        mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    data_list1 = np.array(data_list1)\n",
    "    data_list2 = np.array(data_list2)\n",
    "    length_list1 = np.array(length_list1)\n",
    "    lenth_list2 = np.array(length_list2)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list1)), \n",
    "            torch.from_numpy(np.array(data_list2)),\n",
    "            torch.LongTensor(length_list1), \n",
    "            torch.LongTensor(length_list2)]\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, dropout=0):\n",
    "        '''Bidirectional RNN'''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Embedding input: max_length x batch_size\n",
    "        # Embedding output: max_length x batch_size x hidden size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0) #vocab size x hidden size\n",
    "        \n",
    "        # Input: (max_length x batch_size x hidden_size)\n",
    "        # Output: hidden - 2 x batch_size x hidden_size\n",
    "        # Output: outputs max_length x batch_size x hidden_size*2\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, dropout=self.dropout, bidirectional=False)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        #outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        embedded = self.embedding(inp).unsqueeze(0) #so that we have 1 x batch x hidden\n",
    "        #print('embedded', embedded.size())\n",
    "        output = F.relu(embedded)\n",
    "        #print('after relu', output.size())\n",
    "        #print('hidden size', hidden.size())\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "def train(inputs, input_lengths, targets, target_lengths, \n",
    "          encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH,\n",
    "         teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 #\n",
    "    batch_size = inputs.size()[1]\n",
    "    #print('input size', inputs.size())\n",
    "    #print('batch size', batch_size)\n",
    "    max_targ_len = max_length\n",
    "\n",
    "    # Run words through encoder\n",
    "    _, encoder_hidden = encoder(inputs, input_lengths, None)\n",
    "\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = torch.LongTensor([SOS_token] * batch_size).to(device)\n",
    "    decoder_hidden = encoder_hidden#[:1] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    #print('time 1 size', decoder_input.size())\n",
    "    #print('time 1 hidden size', decoder_hidden.size())\n",
    "    \n",
    "    #randomly use teacher forcing or not\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_targ_len, batch_size, output_lang.n_words))\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_targ_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            decoder_input = targets[t]\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_targ_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "\n",
    "                \n",
    "    loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    targets.transpose(0, 1).contiguous(),\n",
    "    target_lengths)\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def trainIters(loader, encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01,\n",
    "              teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    start = time.time()\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    plot_losses = []\n",
    "\n",
    "    counter = 0\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch < n_iters:\n",
    "        epoch += 1\n",
    "\n",
    "        # Get training data for this cycle\n",
    "        for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # Run the train function\n",
    "            loss = train(\n",
    "                source.long().transpose(0,1), lengths1, target.long().transpose(0,1), lengths2,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio=teacher_forcing_ratio\n",
    "            )\n",
    "\n",
    "            # Keep track of loss\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_iters), epoch, \n",
    "                                                       epoch / n_iters * 100, print_loss_avg)\n",
    "                print(print_summary)\n",
    "\n",
    "\n",
    "            if counter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lengths, translated, search='greedy', max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence.transpose(0,1)\n",
    "        input_length = sentence.size()[0]\n",
    "        \n",
    "        # encode the source lanugage\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor, input_lengths, None)\n",
    "\n",
    "        decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden[:1] # Use last (forward) hidden state from encoder \n",
    "        # output of this function\n",
    "        decoded_words = ''\n",
    "\n",
    "        for di in range(max_length):\n",
    "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "            # hint: print out decoder_output and decoder_attention\n",
    "            # TODO: add your code here to populate decoded_words and decoder_attentions\n",
    "            # TODO: do this in 2 ways discussed in class: greedy & beam_search\n",
    "            \n",
    "            # GREEDY\n",
    "            topv, topi = decoder_output.data.topk(1) \n",
    "\n",
    "            if topi.item() == EOS_token:\n",
    "                #decoded_words.append('<EOS>')\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                if topi.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
    "                    decoded_words = decoded_words + ' ' + output_lang.index2word[topi.item()]\n",
    "            \n",
    "            decoder_input = topi[0].detach()\n",
    "        \n",
    "        translation = ''\n",
    "        for i in translated: #expected translation\n",
    "            if i.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
    "                translation = translation + ' ' + output_lang_v.index2word[i.item()]\n",
    "\n",
    "        return decoded_words, translation\n",
    "    \n",
    "    \n",
    "def evaluate_batch(loader, encoder, decoder):\n",
    "    \n",
    "    decoded_sentences = []\n",
    "    actual_sentences = []\n",
    "    \n",
    "    for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
    "        #iterate over batch\n",
    "        \n",
    "        for n in range(len(source)):\n",
    "            # Go sentence by sentence\n",
    "            \n",
    "            decoded, actual = evaluate(encoder, decoder, source[n].unsqueeze(0), lengths1[n], target[n])\n",
    "            decoded_sentences.append(decoded)\n",
    "            actual_sentences.append(actual)\n",
    "            \n",
    "    return decoded_sentences, actual_sentences\n",
    "\n",
    "\n",
    "def evaluate_bleu(translation_list, reference_list):\n",
    "    \n",
    "    return corpus_bleu(translation_list, [reference_list])\n",
    "\n",
    "#Plot results\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_loc = 'iwslt-vi-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 1000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1004\n",
      "en 1004\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(en_loc+'/train.tok.vi', en_loc+'/train.tok.en', num_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 61 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 130\n",
      "en 114\n"
     ]
    }
   ],
   "source": [
    "input_lang_v, output_lang_v, pairs_v = prepareData(en_loc+'/dev.tok.vi', en_loc+'/dev.tok.en', num_sent=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tôi đã bị sốc', 'I was so shocked'),\n",
       " ('Tôi hoàn_toàn tuyệt_vọng', 'I lost all hope'),\n",
       " ('Cám_ơn các bạn', 'Thank you'),\n",
       " ('Remi biết tình_yêu là gì', 'Remi knows what love is'),\n",
       " ('Em la_hét nhiều', 'He screamed a lot')]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_v[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksenia/anaconda/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/ksenia/anaconda/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 48m 4s) (2 0%) 6.4885\n",
      "0m 5s (- 48m 23s) (4 0%) 5.7174\n",
      "0m 8s (- 57m 44s) (5 0%) 5.4729\n",
      "0m 11s (- 54m 46s) (7 0%) 5.2995\n",
      "0m 14s (- 59m 48s) (8 0%) 5.1545\n",
      "0m 17s (- 57m 13s) (10 0%) 5.0436\n",
      "0m 20s (- 60m 37s) (11 0%) 4.9322\n",
      "0m 22s (- 58m 18s) (13 0%) 4.8414\n",
      "0m 25s (- 56m 45s) (15 0%) 4.7991\n",
      "0m 28s (- 59m 10s) (16 0%) 4.6950\n",
      "0m 31s (- 57m 44s) (18 0%) 4.6777\n",
      "0m 34s (- 59m 36s) (19 0%) 4.6226\n",
      "0m 37s (- 58m 20s) (21 1%) 4.5847\n",
      "0m 39s (- 59m 51s) (22 1%) 4.5349\n",
      "0m 42s (- 58m 52s) (24 1%) 4.5146\n",
      "0m 46s (- 60m 56s) (25 1%) 4.4664\n",
      "0m 49s (- 59m 59s) (27 1%) 4.4604\n",
      "0m 52s (- 58m 58s) (29 1%) 4.4197\n",
      "0m 54s (- 60m 7s) (30 1%) 4.4266\n",
      "0m 57s (- 59m 12s) (32 1%) 4.3571\n",
      "1m 0s (- 60m 20s) (33 1%) 4.3368\n",
      "1m 3s (- 59m 31s) (35 1%) 4.3608\n",
      "1m 6s (- 60m 28s) (36 1%) 4.3269\n",
      "1m 9s (- 59m 39s) (38 1%) 4.3019\n",
      "1m 12s (- 58m 55s) (40 2%) 4.2689\n",
      "1m 15s (- 59m 45s) (41 2%) 4.2371\n",
      "1m 17s (- 59m 5s) (43 2%) 4.2791\n",
      "1m 20s (- 59m 49s) (44 2%) 4.2546\n",
      "1m 23s (- 59m 9s) (46 2%) 4.2132\n",
      "1m 26s (- 59m 50s) (47 2%) 4.2122\n",
      "1m 29s (- 59m 16s) (49 2%) 4.2097\n",
      "1m 32s (- 59m 56s) (50 2%) 4.1866\n",
      "1m 35s (- 59m 21s) (52 2%) 4.1760\n",
      "1m 38s (- 58m 52s) (54 2%) 4.1412\n",
      "1m 40s (- 59m 30s) (55 2%) 4.1475\n",
      "1m 43s (- 59m 2s) (57 2%) 4.1188\n",
      "1m 46s (- 59m 36s) (58 2%) 4.1254\n",
      "1m 49s (- 59m 5s) (60 3%) 4.1140\n",
      "1m 52s (- 59m 35s) (61 3%) 4.0987\n",
      "1m 55s (- 59m 6s) (63 3%) 4.0923\n",
      "1m 58s (- 58m 36s) (65 3%) 4.0692\n",
      "2m 0s (- 59m 5s) (66 3%) 4.0892\n",
      "2m 3s (- 58m 37s) (68 3%) 4.0523\n",
      "2m 6s (- 59m 4s) (69 3%) 4.0736\n",
      "2m 9s (- 58m 39s) (71 3%) 4.0449\n",
      "2m 12s (- 59m 5s) (72 3%) 4.0422\n",
      "2m 15s (- 58m 40s) (74 3%) 4.0540\n",
      "2m 18s (- 59m 2s) (75 3%) 3.9864\n",
      "2m 20s (- 58m 37s) (77 3%) 3.9987\n",
      "2m 23s (- 58m 13s) (79 3%) 4.0242\n",
      "2m 26s (- 58m 35s) (80 4%) 4.0032\n",
      "2m 29s (- 58m 10s) (82 4%) 4.0108\n",
      "2m 32s (- 58m 32s) (83 4%) 3.9821\n",
      "2m 34s (- 58m 9s) (85 4%) 3.9733\n",
      "2m 37s (- 58m 30s) (86 4%) 4.0014\n",
      "2m 40s (- 58m 7s) (88 4%) 3.9627\n",
      "2m 43s (- 57m 45s) (90 4%) 3.9593\n",
      "2m 46s (- 58m 4s) (91 4%) 3.9501\n",
      "2m 48s (- 57m 44s) (93 4%) 3.9345\n",
      "2m 51s (- 58m 3s) (94 4%) 3.9587\n",
      "2m 54s (- 57m 42s) (96 4%) 3.9476\n",
      "2m 57s (- 58m 0s) (97 4%) 3.9024\n",
      "3m 0s (- 57m 40s) (99 4%) 3.9683\n",
      "3m 3s (- 57m 57s) (100 5%) 3.9236\n",
      "3m 5s (- 57m 38s) (102 5%) 3.9022\n",
      "3m 8s (- 57m 19s) (104 5%) 3.9207\n",
      "3m 11s (- 57m 35s) (105 5%) 3.8809\n",
      "3m 14s (- 57m 16s) (107 5%) 3.8737\n",
      "3m 17s (- 57m 34s) (108 5%) 3.9392\n",
      "3m 21s (- 57m 35s) (110 5%) 3.8642\n",
      "3m 24s (- 58m 1s) (111 5%) 3.8945\n",
      "3m 27s (- 57m 45s) (113 5%) 3.9105\n",
      "3m 30s (- 57m 30s) (115 5%) 3.8666\n",
      "3m 33s (- 57m 49s) (116 5%) 3.8308\n",
      "3m 36s (- 57m 34s) (118 5%) 3.8774\n",
      "3m 39s (- 57m 48s) (119 5%) 3.8775\n",
      "3m 42s (- 57m 30s) (121 6%) 3.8295\n",
      "3m 45s (- 57m 43s) (122 6%) 3.8177\n",
      "3m 47s (- 57m 27s) (124 6%) 3.8834\n",
      "3m 50s (- 57m 40s) (125 6%) 3.8480\n",
      "3m 53s (- 57m 24s) (127 6%) 3.8268\n",
      "3m 56s (- 57m 8s) (129 6%) 3.8178\n",
      "3m 59s (- 57m 20s) (130 6%) 3.8283\n",
      "4m 1s (- 57m 4s) (132 6%) 3.7955\n",
      "4m 4s (- 57m 17s) (133 6%) 3.7840\n",
      "4m 7s (- 57m 2s) (135 6%) 3.8466\n",
      "4m 10s (- 57m 14s) (136 6%) 3.7615\n",
      "4m 13s (- 56m 59s) (138 6%) 3.7996\n",
      "4m 16s (- 56m 48s) (140 7%) 3.7639\n",
      "4m 19s (- 56m 59s) (141 7%) 3.7953\n",
      "4m 22s (- 56m 44s) (143 7%) 3.7511\n",
      "4m 25s (- 56m 57s) (144 7%) 3.7841\n",
      "4m 28s (- 56m 44s) (146 7%) 3.7410\n",
      "4m 30s (- 56m 55s) (147 7%) 3.7513\n",
      "4m 33s (- 56m 41s) (149 7%) 3.7708\n",
      "4m 36s (- 56m 51s) (150 7%) 3.7831\n",
      "4m 39s (- 56m 38s) (152 7%) 3.7544\n",
      "4m 42s (- 56m 26s) (154 7%) 3.7292\n",
      "4m 45s (- 56m 36s) (155 7%) 3.7100\n",
      "4m 48s (- 56m 22s) (157 7%) 3.7426\n",
      "4m 51s (- 56m 32s) (158 7%) 3.7389\n",
      "4m 53s (- 56m 18s) (160 8%) 3.6811\n",
      "4m 56s (- 56m 27s) (161 8%) 3.7641\n",
      "4m 59s (- 56m 14s) (163 8%) 3.7383\n",
      "5m 2s (- 56m 0s) (165 8%) 3.6718\n",
      "5m 5s (- 56m 16s) (166 8%) 3.7161\n",
      "5m 8s (- 56m 3s) (168 8%) 3.7420\n",
      "5m 11s (- 56m 14s) (169 8%) 3.6736\n",
      "5m 14s (- 56m 3s) (171 8%) 3.7131\n",
      "5m 17s (- 56m 13s) (172 8%) 3.6768\n",
      "5m 20s (- 56m 2s) (174 8%) 3.6730\n",
      "5m 23s (- 56m 11s) (175 8%) 3.6965\n",
      "5m 26s (- 56m 0s) (177 8%) 3.6903\n",
      "5m 29s (- 55m 48s) (179 8%) 3.6748\n",
      "5m 32s (- 55m 57s) (180 9%) 3.6629\n",
      "5m 34s (- 55m 44s) (182 9%) 3.6470\n",
      "5m 37s (- 55m 53s) (183 9%) 3.6708\n",
      "5m 40s (- 55m 40s) (185 9%) 3.6795\n",
      "5m 43s (- 55m 48s) (186 9%) 3.6572\n",
      "5m 46s (- 55m 36s) (188 9%) 3.7066\n",
      "5m 49s (- 55m 25s) (190 9%) 3.6036\n",
      "5m 51s (- 55m 32s) (191 9%) 3.6669\n",
      "5m 54s (- 55m 20s) (193 9%) 3.6519\n",
      "5m 57s (- 55m 27s) (194 9%) 3.6128\n",
      "6m 0s (- 55m 15s) (196 9%) 3.5966\n",
      "6m 3s (- 55m 22s) (197 9%) 3.6131\n",
      "6m 6s (- 55m 12s) (199 9%) 3.6318\n",
      "6m 8s (- 55m 20s) (200 10%) 3.6118\n",
      "6m 12s (- 55m 14s) (202 10%) 3.6075\n",
      "6m 15s (- 55m 4s) (204 10%) 3.6174\n",
      "6m 18s (- 55m 12s) (205 10%) 3.6090\n",
      "6m 21s (- 55m 1s) (207 10%) 3.5822\n",
      "6m 24s (- 55m 13s) (208 10%) 3.6238\n",
      "6m 28s (- 55m 7s) (210 10%) 3.6254\n",
      "6m 31s (- 55m 15s) (211 10%) 3.5960\n",
      "6m 34s (- 55m 8s) (213 10%) 3.5781\n",
      "6m 37s (- 55m 3s) (215 10%) 3.5733\n",
      "6m 41s (- 55m 15s) (216 10%) 3.5766\n",
      "6m 44s (- 55m 5s) (218 10%) 3.5811\n",
      "6m 47s (- 55m 12s) (219 10%) 3.5633\n",
      "6m 50s (- 55m 2s) (221 11%) 3.6119\n",
      "6m 53s (- 55m 7s) (222 11%) 3.5420\n",
      "6m 55s (- 54m 57s) (224 11%) 3.5570\n",
      "6m 58s (- 55m 3s) (225 11%) 3.5804\n",
      "7m 1s (- 54m 52s) (227 11%) 3.5934\n",
      "7m 4s (- 54m 42s) (229 11%) 3.5186\n",
      "7m 7s (- 54m 49s) (230 11%) 3.5450\n",
      "7m 10s (- 54m 40s) (232 11%) 3.5092\n",
      "7m 13s (- 54m 46s) (233 11%) 3.5839\n",
      "7m 16s (- 54m 36s) (235 11%) 3.5493\n",
      "7m 19s (- 54m 43s) (236 11%) 3.5624\n",
      "7m 23s (- 54m 41s) (238 11%) 3.5479\n",
      "7m 26s (- 54m 31s) (240 12%) 3.5028\n",
      "7m 28s (- 54m 36s) (241 12%) 3.5183\n",
      "7m 31s (- 54m 27s) (243 12%) 3.4979\n",
      "7m 34s (- 54m 33s) (244 12%) 3.5663\n",
      "7m 37s (- 54m 23s) (246 12%) 3.5206\n",
      "7m 40s (- 54m 29s) (247 12%) 3.5293\n",
      "7m 43s (- 54m 19s) (249 12%) 3.5142\n",
      "7m 46s (- 54m 24s) (250 12%) 3.5129\n",
      "7m 49s (- 54m 14s) (252 12%) 3.5245\n",
      "7m 52s (- 54m 5s) (254 12%) 3.4825\n",
      "7m 54s (- 54m 9s) (255 12%) 3.4679\n",
      "7m 57s (- 54m 1s) (257 12%) 3.5184\n",
      "8m 1s (- 54m 9s) (258 12%) 3.5010\n",
      "8m 4s (- 53m 59s) (260 13%) 3.4800\n",
      "8m 6s (- 54m 3s) (261 13%) 3.5175\n",
      "8m 9s (- 53m 54s) (263 13%) 3.4738\n",
      "8m 12s (- 53m 44s) (265 13%) 3.4775\n",
      "8m 15s (- 53m 48s) (266 13%) 3.5014\n",
      "8m 18s (- 53m 39s) (268 13%) 3.4674\n",
      "8m 20s (- 53m 43s) (269 13%) 3.4716\n",
      "8m 24s (- 53m 36s) (271 13%) 3.4955\n",
      "8m 27s (- 53m 41s) (272 13%) 3.4986\n",
      "8m 29s (- 53m 32s) (274 13%) 3.4562\n",
      "8m 33s (- 53m 39s) (275 13%) 3.4590\n",
      "8m 36s (- 53m 35s) (277 13%) 3.4808\n",
      "8m 40s (- 53m 31s) (279 13%) 3.4577\n",
      "8m 43s (- 53m 37s) (280 14%) 3.4346\n",
      "8m 46s (- 53m 30s) (282 14%) 3.4802\n",
      "8m 49s (- 53m 34s) (283 14%) 3.4740\n",
      "8m 52s (- 53m 26s) (285 14%) 3.4652\n",
      "8m 55s (- 53m 31s) (286 14%) 3.4265\n",
      "8m 59s (- 53m 25s) (288 14%) 3.4366\n",
      "9m 2s (- 53m 19s) (290 14%) 3.4741\n",
      "9m 5s (- 53m 24s) (291 14%) 3.4770\n",
      "9m 9s (- 53m 18s) (293 14%) 3.4320\n",
      "9m 11s (- 53m 22s) (294 14%) 3.3979\n",
      "9m 14s (- 53m 13s) (296 14%) 3.4626\n",
      "9m 17s (- 53m 19s) (297 14%) 3.3880\n",
      "9m 20s (- 53m 11s) (299 14%) 3.4176\n",
      "9m 23s (- 53m 15s) (300 15%) 3.4139\n",
      "9m 26s (- 53m 7s) (302 15%) 3.4118\n",
      "9m 29s (- 52m 59s) (304 15%) 3.3960\n",
      "9m 32s (- 53m 3s) (305 15%) 3.4198\n",
      "9m 35s (- 52m 55s) (307 15%) 3.4130\n",
      "9m 38s (- 52m 59s) (308 15%) 3.3810\n",
      "9m 41s (- 52m 50s) (310 15%) 3.4098\n",
      "9m 44s (- 52m 54s) (311 15%) 3.3906\n",
      "9m 47s (- 52m 45s) (313 15%) 3.3753\n",
      "9m 50s (- 52m 37s) (315 15%) 3.3705\n",
      "9m 53s (- 52m 40s) (316 15%) 3.3949\n",
      "9m 55s (- 52m 32s) (318 15%) 3.3417\n",
      "9m 58s (- 52m 35s) (319 15%) 3.4045\n",
      "10m 1s (- 52m 27s) (321 16%) 3.3378\n",
      "10m 4s (- 52m 31s) (322 16%) 3.3825\n",
      "10m 7s (- 52m 22s) (324 16%) 3.3719\n",
      "10m 10s (- 52m 25s) (325 16%) 3.4139\n",
      "10m 13s (- 52m 17s) (327 16%) 3.3770\n",
      "10m 16s (- 52m 10s) (329 16%) 3.3489\n",
      "10m 19s (- 52m 13s) (330 16%) 3.3916\n",
      "10m 22s (- 52m 5s) (332 16%) 3.3383\n",
      "10m 25s (- 52m 8s) (333 16%) 3.3746\n",
      "10m 27s (- 52m 0s) (335 16%) 3.3472\n",
      "10m 30s (- 52m 4s) (336 16%) 3.3405\n",
      "10m 33s (- 51m 55s) (338 16%) 3.3682\n",
      "10m 36s (- 51m 47s) (340 17%) 3.3681\n",
      "10m 39s (- 51m 50s) (341 17%) 3.3550\n",
      "10m 42s (- 51m 42s) (343 17%) 3.3221\n",
      "10m 45s (- 51m 46s) (344 17%) 3.3322\n",
      "10m 48s (- 51m 39s) (346 17%) 3.3394\n",
      "10m 51s (- 51m 42s) (347 17%) 3.3325\n",
      "10m 54s (- 51m 34s) (349 17%) 3.2893\n",
      "10m 57s (- 51m 37s) (350 17%) 3.3226\n",
      "10m 59s (- 51m 29s) (352 17%) 3.2925\n",
      "11m 2s (- 51m 22s) (354 17%) 3.2891\n",
      "11m 5s (- 51m 25s) (355 17%) 3.3064\n",
      "11m 8s (- 51m 17s) (357 17%) 3.3239\n",
      "11m 11s (- 51m 20s) (358 17%) 3.2728\n",
      "11m 14s (- 51m 12s) (360 18%) 3.3047\n",
      "11m 17s (- 51m 15s) (361 18%) 3.2995\n",
      "11m 20s (- 51m 7s) (363 18%) 3.2820\n",
      "11m 23s (- 51m 0s) (365 18%) 3.3037\n",
      "11m 26s (- 51m 3s) (366 18%) 3.3047\n",
      "11m 29s (- 50m 56s) (368 18%) 3.2948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11m 32s (- 51m 0s) (369 18%) 3.2574\n",
      "11m 35s (- 50m 52s) (371 18%) 3.2546\n",
      "11m 37s (- 50m 54s) (372 18%) 3.2707\n",
      "11m 41s (- 50m 48s) (374 18%) 3.2446\n",
      "11m 44s (- 50m 51s) (375 18%) 3.2910\n",
      "11m 47s (- 50m 44s) (377 18%) 3.2403\n",
      "11m 50s (- 50m 37s) (379 18%) 3.2474\n",
      "11m 53s (- 50m 41s) (380 19%) 3.2468\n",
      "11m 56s (- 50m 34s) (382 19%) 3.2705\n",
      "11m 59s (- 50m 37s) (383 19%) 3.2182\n",
      "12m 2s (- 50m 31s) (385 19%) 3.2911\n",
      "12m 5s (- 50m 34s) (386 19%) 3.2314\n",
      "12m 8s (- 50m 27s) (388 19%) 3.2707\n",
      "12m 11s (- 50m 19s) (390 19%) 3.2374\n",
      "12m 14s (- 50m 22s) (391 19%) 3.2671\n",
      "12m 18s (- 50m 20s) (393 19%) 3.2003\n",
      "12m 22s (- 50m 26s) (394 19%) 3.2246\n",
      "12m 25s (- 50m 20s) (396 19%) 3.2154\n",
      "12m 29s (- 50m 26s) (397 19%) 3.2130\n",
      "12m 32s (- 50m 20s) (399 19%) 3.2074\n",
      "12m 35s (- 50m 23s) (400 20%) 3.2213\n",
      "12m 38s (- 50m 15s) (402 20%) 3.2074\n",
      "12m 42s (- 50m 10s) (404 20%) 3.1505\n",
      "12m 45s (- 50m 13s) (405 20%) 3.2536\n",
      "12m 48s (- 50m 6s) (407 20%) 3.1900\n",
      "12m 51s (- 50m 8s) (408 20%) 3.2062\n",
      "12m 53s (- 50m 1s) (410 20%) 3.1446\n",
      "12m 57s (- 50m 4s) (411 20%) 3.2275\n",
      "13m 1s (- 50m 1s) (413 20%) 3.1908\n",
      "13m 4s (- 49m 54s) (415 20%) 3.1844\n",
      "13m 7s (- 49m 56s) (416 20%) 3.1893\n",
      "13m 10s (- 49m 51s) (418 20%) 3.1582\n",
      "13m 13s (- 49m 54s) (419 20%) 3.1764\n",
      "13m 17s (- 49m 49s) (421 21%) 3.1240\n",
      "13m 20s (- 49m 52s) (422 21%) 3.1907\n",
      "13m 23s (- 49m 47s) (424 21%) 3.1793\n",
      "13m 27s (- 49m 51s) (425 21%) 3.1238\n",
      "13m 30s (- 49m 46s) (427 21%) 3.1253\n",
      "13m 33s (- 49m 40s) (429 21%) 3.1658\n",
      "13m 37s (- 49m 45s) (430 21%) 3.1505\n",
      "13m 40s (- 49m 38s) (432 21%) 3.1786\n",
      "13m 43s (- 49m 40s) (433 21%) 3.1307\n",
      "13m 46s (- 49m 33s) (435 21%) 3.1040\n",
      "13m 50s (- 49m 40s) (436 21%) 3.1606\n",
      "13m 54s (- 49m 34s) (438 21%) 3.1105\n",
      "13m 57s (- 49m 28s) (440 22%) 3.1999\n",
      "14m 0s (- 49m 31s) (441 22%) 3.1467\n",
      "14m 3s (- 49m 26s) (443 22%) 3.1283\n",
      "14m 6s (- 49m 27s) (444 22%) 3.0894\n",
      "14m 9s (- 49m 20s) (446 22%) 3.1197\n",
      "14m 12s (- 49m 22s) (447 22%) 3.1480\n",
      "14m 16s (- 49m 17s) (449 22%) 3.1187\n",
      "14m 19s (- 49m 19s) (450 22%) 3.0808\n",
      "14m 22s (- 49m 13s) (452 22%) 3.1226\n",
      "14m 25s (- 49m 6s) (454 22%) 3.1140\n",
      "14m 28s (- 49m 8s) (455 22%) 3.1298\n",
      "14m 31s (- 49m 1s) (457 22%) 3.1148\n",
      "14m 34s (- 49m 2s) (458 22%) 3.1163\n",
      "14m 37s (- 48m 56s) (460 23%) 3.0748\n",
      "14m 40s (- 48m 57s) (461 23%) 3.0843\n",
      "14m 42s (- 48m 50s) (463 23%) 3.0442\n",
      "14m 45s (- 48m 44s) (465 23%) 3.0757\n",
      "14m 48s (- 48m 45s) (466 23%) 3.0403\n",
      "14m 51s (- 48m 38s) (468 23%) 3.1102\n",
      "14m 54s (- 48m 40s) (469 23%) 3.0750\n",
      "14m 57s (- 48m 33s) (471 23%) 3.0644\n",
      "15m 0s (- 48m 35s) (472 23%) 3.0570\n",
      "15m 3s (- 48m 28s) (474 23%) 3.0713\n",
      "15m 6s (- 48m 29s) (475 23%) 3.0404\n",
      "15m 9s (- 48m 23s) (477 23%) 3.1083\n",
      "15m 12s (- 48m 16s) (479 23%) 3.0451\n",
      "15m 15s (- 48m 18s) (480 24%) 3.0406\n",
      "15m 18s (- 48m 11s) (482 24%) 3.0600\n",
      "15m 21s (- 48m 13s) (483 24%) 3.0443\n",
      "15m 24s (- 48m 6s) (485 24%) 3.0361\n",
      "15m 27s (- 48m 8s) (486 24%) 3.0563\n",
      "15m 29s (- 48m 1s) (488 24%) 3.0556\n",
      "15m 32s (- 47m 54s) (490 24%) 3.0240\n",
      "15m 36s (- 47m 57s) (491 24%) 3.0274\n",
      "15m 39s (- 47m 51s) (493 24%) 2.9892\n",
      "15m 42s (- 47m 52s) (494 24%) 3.0102\n",
      "15m 45s (- 47m 46s) (496 24%) 3.0244\n",
      "15m 48s (- 47m 48s) (497 24%) 3.0000\n",
      "15m 51s (- 47m 42s) (499 24%) 3.0129\n",
      "15m 54s (- 47m 43s) (500 25%) 2.9717\n",
      "15m 57s (- 47m 37s) (502 25%) 2.9818\n",
      "16m 0s (- 47m 31s) (504 25%) 2.9800\n",
      "16m 3s (- 47m 32s) (505 25%) 2.9527\n",
      "16m 6s (- 47m 26s) (507 25%) 2.9487\n",
      "16m 9s (- 47m 28s) (508 25%) 3.0170\n",
      "16m 12s (- 47m 22s) (510 25%) 2.9907\n",
      "16m 15s (- 47m 23s) (511 25%) 2.9840\n",
      "16m 18s (- 47m 17s) (513 25%) 2.9794\n",
      "16m 22s (- 47m 11s) (515 25%) 2.9777\n",
      "16m 25s (- 47m 13s) (516 25%) 2.9312\n",
      "16m 28s (- 47m 7s) (518 25%) 2.9735\n",
      "16m 31s (- 47m 8s) (519 25%) 2.9175\n",
      "16m 33s (- 47m 1s) (521 26%) 2.9493\n",
      "16m 36s (- 47m 2s) (522 26%) 2.9278\n",
      "16m 40s (- 46m 56s) (524 26%) 2.9425\n",
      "16m 43s (- 46m 58s) (525 26%) 2.8955\n",
      "16m 46s (- 46m 52s) (527 26%) 2.9780\n",
      "16m 49s (- 46m 47s) (529 26%) 2.9099\n",
      "16m 53s (- 46m 50s) (530 26%) 2.9463\n",
      "16m 57s (- 46m 46s) (532 26%) 2.9487\n",
      "17m 0s (- 46m 47s) (533 26%) 2.9065\n",
      "17m 3s (- 46m 41s) (535 26%) 2.9082\n",
      "17m 6s (- 46m 42s) (536 26%) 2.9051\n",
      "17m 9s (- 46m 36s) (538 26%) 2.9422\n",
      "17m 12s (- 46m 30s) (540 27%) 2.9268\n",
      "17m 14s (- 46m 31s) (541 27%) 2.8694\n",
      "17m 17s (- 46m 24s) (543 27%) 2.9354\n",
      "17m 20s (- 46m 25s) (544 27%) 2.8938\n",
      "17m 23s (- 46m 19s) (546 27%) 2.9386\n",
      "17m 26s (- 46m 20s) (547 27%) 2.9019\n",
      "17m 29s (- 46m 14s) (549 27%) 2.8730\n",
      "17m 32s (- 46m 15s) (550 27%) 2.8987\n",
      "17m 35s (- 46m 9s) (552 27%) 2.8864\n",
      "17m 38s (- 46m 3s) (554 27%) 2.8207\n",
      "17m 41s (- 46m 3s) (555 27%) 2.8830\n",
      "17m 44s (- 45m 57s) (557 27%) 2.8576\n",
      "17m 47s (- 45m 58s) (558 27%) 2.9091\n",
      "17m 50s (- 45m 52s) (560 28%) 2.8759\n",
      "17m 53s (- 45m 52s) (561 28%) 2.8831\n",
      "17m 56s (- 45m 46s) (563 28%) 2.8462\n",
      "17m 59s (- 45m 40s) (565 28%) 2.8140\n",
      "18m 1s (- 45m 41s) (566 28%) 2.8552\n",
      "18m 4s (- 45m 35s) (568 28%) 2.8406\n",
      "18m 7s (- 45m 35s) (569 28%) 2.8123\n",
      "18m 10s (- 45m 29s) (571 28%) 2.8294\n",
      "18m 13s (- 45m 30s) (572 28%) 2.8581\n",
      "18m 16s (- 45m 24s) (574 28%) 2.8365\n",
      "18m 19s (- 45m 24s) (575 28%) 2.8076\n",
      "18m 22s (- 45m 18s) (577 28%) 2.8487\n",
      "18m 25s (- 45m 12s) (579 28%) 2.8344\n",
      "18m 28s (- 45m 13s) (580 28%) 2.8296\n",
      "18m 31s (- 45m 7s) (582 29%) 2.8415\n",
      "18m 34s (- 45m 8s) (583 29%) 2.7984\n",
      "18m 37s (- 45m 2s) (585 29%) 2.7943\n",
      "18m 40s (- 45m 2s) (586 29%) 2.8253\n",
      "18m 42s (- 44m 56s) (588 29%) 2.7690\n",
      "18m 45s (- 44m 50s) (590 29%) 2.7541\n",
      "18m 48s (- 44m 50s) (591 29%) 2.7834\n",
      "18m 51s (- 44m 44s) (593 29%) 2.8061\n",
      "18m 54s (- 44m 45s) (594 29%) 2.7870\n",
      "18m 57s (- 44m 39s) (596 29%) 2.7733\n",
      "19m 0s (- 44m 40s) (597 29%) 2.7938\n",
      "19m 3s (- 44m 34s) (599 29%) 2.7898\n",
      "19m 6s (- 44m 34s) (600 30%) 2.7536\n",
      "19m 9s (- 44m 28s) (602 30%) 2.7739\n",
      "19m 11s (- 44m 22s) (604 30%) 2.7131\n",
      "19m 14s (- 44m 22s) (605 30%) 2.7830\n",
      "19m 18s (- 44m 18s) (607 30%) 2.7120\n",
      "19m 21s (- 44m 18s) (608 30%) 2.7808\n",
      "19m 24s (- 44m 12s) (610 30%) 2.7722\n",
      "19m 27s (- 44m 13s) (611 30%) 2.7517\n",
      "19m 29s (- 44m 6s) (613 30%) 2.7519\n",
      "19m 32s (- 44m 0s) (615 30%) 2.7457\n",
      "19m 35s (- 44m 0s) (616 30%) 2.7156\n",
      "19m 38s (- 43m 54s) (618 30%) 2.7184\n",
      "19m 40s (- 43m 54s) (619 30%) 2.7164\n",
      "19m 43s (- 43m 48s) (621 31%) 2.7106\n",
      "19m 46s (- 43m 48s) (622 31%) 2.6753\n",
      "19m 49s (- 43m 42s) (624 31%) 2.6960\n",
      "19m 52s (- 43m 43s) (625 31%) 2.6675\n",
      "19m 55s (- 43m 37s) (627 31%) 2.6553\n",
      "19m 58s (- 43m 31s) (629 31%) 2.7022\n",
      "20m 1s (- 43m 32s) (630 31%) 2.7171\n",
      "20m 4s (- 43m 26s) (632 31%) 2.6928\n",
      "20m 6s (- 43m 26s) (633 31%) 2.7328\n",
      "20m 9s (- 43m 20s) (635 31%) 2.7088\n",
      "20m 12s (- 43m 20s) (636 31%) 2.6852\n",
      "20m 15s (- 43m 14s) (638 31%) 2.6833\n",
      "20m 18s (- 43m 8s) (640 32%) 2.6558\n",
      "20m 21s (- 43m 8s) (641 32%) 2.6623\n",
      "20m 23s (- 43m 2s) (643 32%) 2.6781\n",
      "20m 27s (- 43m 4s) (644 32%) 2.6715\n",
      "20m 30s (- 42m 58s) (646 32%) 2.6653\n",
      "20m 33s (- 42m 59s) (647 32%) 2.6404\n",
      "20m 36s (- 42m 54s) (649 32%) 2.6762\n",
      "20m 39s (- 42m 54s) (650 32%) 2.6591\n",
      "20m 42s (- 42m 49s) (652 32%) 2.6593\n",
      "20m 45s (- 42m 43s) (654 32%) 2.6210\n",
      "20m 48s (- 42m 43s) (655 32%) 2.6118\n",
      "20m 51s (- 42m 38s) (657 32%) 2.6147\n",
      "20m 54s (- 42m 38s) (658 32%) 2.6224\n",
      "20m 57s (- 42m 33s) (660 33%) 2.6200\n",
      "21m 0s (- 42m 33s) (661 33%) 2.6243\n",
      "21m 3s (- 42m 27s) (663 33%) 2.6064\n",
      "21m 6s (- 42m 22s) (665 33%) 2.6379\n",
      "21m 9s (- 42m 22s) (666 33%) 2.6305\n",
      "21m 12s (- 42m 16s) (668 33%) 2.6363\n",
      "21m 15s (- 42m 17s) (669 33%) 2.6487\n",
      "21m 18s (- 42m 11s) (671 33%) 2.6063\n",
      "21m 21s (- 42m 12s) (672 33%) 2.6068\n",
      "21m 24s (- 42m 7s) (674 33%) 2.6256\n",
      "21m 27s (- 42m 7s) (675 33%) 2.6179\n",
      "21m 30s (- 42m 1s) (677 33%) 2.5936\n",
      "21m 33s (- 41m 56s) (679 33%) 2.5832\n",
      "21m 36s (- 41m 56s) (680 34%) 2.6193\n",
      "21m 39s (- 41m 50s) (682 34%) 2.5844\n",
      "21m 42s (- 41m 51s) (683 34%) 2.5998\n",
      "21m 45s (- 41m 45s) (685 34%) 2.5882\n",
      "21m 48s (- 41m 45s) (686 34%) 2.5351\n",
      "21m 51s (- 41m 40s) (688 34%) 2.4965\n",
      "21m 54s (- 41m 34s) (690 34%) 2.5613\n",
      "21m 57s (- 41m 34s) (691 34%) 2.6018\n",
      "21m 59s (- 41m 29s) (693 34%) 2.5766\n",
      "22m 2s (- 41m 29s) (694 34%) 2.5585\n",
      "22m 5s (- 41m 23s) (696 34%) 2.5313\n",
      "22m 8s (- 41m 23s) (697 34%) 2.5634\n",
      "22m 11s (- 41m 18s) (699 34%) 2.5511\n",
      "22m 14s (- 41m 18s) (700 35%) 2.5487\n",
      "22m 17s (- 41m 12s) (702 35%) 2.4951\n",
      "22m 20s (- 41m 7s) (704 35%) 2.5625\n",
      "22m 23s (- 41m 7s) (705 35%) 2.5280\n",
      "22m 26s (- 41m 2s) (707 35%) 2.5241\n",
      "22m 29s (- 41m 1s) (708 35%) 2.5240\n",
      "22m 32s (- 40m 56s) (710 35%) 2.5300\n",
      "22m 34s (- 40m 56s) (711 35%) 2.5187\n",
      "22m 37s (- 40m 51s) (713 35%) 2.5186\n",
      "22m 40s (- 40m 45s) (715 35%) 2.5237\n",
      "22m 43s (- 40m 45s) (716 35%) 2.5296\n",
      "22m 46s (- 40m 40s) (718 35%) 2.4812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22m 49s (- 40m 40s) (719 35%) 2.4753\n",
      "22m 53s (- 40m 36s) (721 36%) 2.4627\n",
      "22m 56s (- 40m 37s) (722 36%) 2.4588\n",
      "23m 1s (- 40m 34s) (724 36%) 2.5164\n",
      "23m 4s (- 40m 34s) (725 36%) 2.4596\n",
      "23m 8s (- 40m 30s) (727 36%) 2.5038\n",
      "23m 11s (- 40m 26s) (729 36%) 2.4700\n",
      "23m 14s (- 40m 26s) (730 36%) 2.4409\n",
      "23m 17s (- 40m 20s) (732 36%) 2.4376\n",
      "23m 20s (- 40m 20s) (733 36%) 2.4789\n",
      "23m 23s (- 40m 14s) (735 36%) 2.4765\n",
      "23m 26s (- 40m 14s) (736 36%) 2.4318\n",
      "23m 28s (- 40m 9s) (738 36%) 2.4360\n",
      "23m 31s (- 40m 4s) (740 37%) 2.4656\n",
      "23m 34s (- 40m 3s) (741 37%) 2.4366\n",
      "23m 37s (- 39m 58s) (743 37%) 2.4295\n",
      "23m 40s (- 39m 58s) (744 37%) 2.4350\n",
      "23m 43s (- 39m 52s) (746 37%) 2.4106\n",
      "23m 46s (- 39m 52s) (747 37%) 2.4292\n",
      "23m 49s (- 39m 46s) (749 37%) 2.4437\n",
      "23m 51s (- 39m 46s) (750 37%) 2.4157\n",
      "23m 54s (- 39m 41s) (752 37%) 2.3855\n",
      "23m 57s (- 39m 35s) (754 37%) 2.4296\n",
      "24m 0s (- 39m 35s) (755 37%) 2.4128\n",
      "24m 3s (- 39m 30s) (757 37%) 2.4327\n",
      "24m 6s (- 39m 29s) (758 37%) 2.3950\n",
      "24m 9s (- 39m 24s) (760 38%) 2.4090\n",
      "24m 11s (- 39m 23s) (761 38%) 2.3596\n",
      "24m 14s (- 39m 18s) (763 38%) 2.3780\n",
      "24m 17s (- 39m 13s) (765 38%) 2.3917\n",
      "24m 20s (- 39m 13s) (766 38%) 2.4073\n",
      "24m 23s (- 39m 8s) (768 38%) 2.4087\n",
      "24m 27s (- 39m 9s) (769 38%) 2.3396\n",
      "24m 30s (- 39m 3s) (771 38%) 2.3552\n",
      "24m 33s (- 39m 3s) (772 38%) 2.4096\n",
      "24m 36s (- 38m 58s) (774 38%) 2.3234\n",
      "24m 38s (- 38m 57s) (775 38%) 2.3810\n",
      "24m 41s (- 38m 52s) (777 38%) 2.3323\n",
      "24m 44s (- 38m 46s) (779 38%) 2.3747\n",
      "24m 47s (- 38m 46s) (780 39%) 2.3382\n",
      "24m 50s (- 38m 41s) (782 39%) 2.3756\n",
      "24m 53s (- 38m 40s) (783 39%) 2.3578\n",
      "24m 55s (- 38m 35s) (785 39%) 2.3707\n",
      "24m 58s (- 38m 34s) (786 39%) 2.3536\n",
      "25m 1s (- 38m 29s) (788 39%) 2.3397\n",
      "25m 4s (- 38m 24s) (790 39%) 2.3235\n",
      "25m 7s (- 38m 23s) (791 39%) 2.3182\n",
      "25m 10s (- 38m 18s) (793 39%) 2.3333\n",
      "25m 13s (- 38m 18s) (794 39%) 2.3516\n",
      "25m 16s (- 38m 13s) (796 39%) 2.2876\n",
      "25m 18s (- 38m 12s) (797 39%) 2.2901\n",
      "25m 21s (- 38m 7s) (799 39%) 2.2907\n",
      "25m 24s (- 38m 7s) (800 40%) 2.3400\n",
      "25m 27s (- 38m 1s) (802 40%) 2.2829\n",
      "25m 30s (- 37m 56s) (804 40%) 2.2979\n",
      "25m 33s (- 37m 56s) (805 40%) 2.3044\n",
      "25m 36s (- 37m 51s) (807 40%) 2.3154\n",
      "25m 39s (- 37m 50s) (808 40%) 2.3042\n",
      "25m 42s (- 37m 45s) (810 40%) 2.2849\n",
      "25m 44s (- 37m 44s) (811 40%) 2.2884\n",
      "25m 47s (- 37m 39s) (813 40%) 2.2531\n",
      "25m 50s (- 37m 34s) (815 40%) 2.2638\n",
      "25m 53s (- 37m 33s) (816 40%) 2.2525\n",
      "25m 56s (- 37m 28s) (818 40%) 2.3182\n",
      "25m 59s (- 37m 28s) (819 40%) 2.2005\n",
      "26m 2s (- 37m 23s) (821 41%) 2.2946\n",
      "26m 5s (- 37m 22s) (822 41%) 2.2473\n",
      "26m 7s (- 37m 17s) (824 41%) 2.2047\n",
      "26m 10s (- 37m 17s) (825 41%) 2.2570\n",
      "26m 13s (- 37m 11s) (827 41%) 2.2525\n",
      "26m 16s (- 37m 6s) (829 41%) 2.2692\n",
      "26m 19s (- 37m 6s) (830 41%) 2.2028\n",
      "26m 21s (- 37m 0s) (832 41%) 2.2595\n",
      "26m 24s (- 37m 0s) (833 41%) 2.2561\n",
      "26m 27s (- 36m 55s) (835 41%) 2.2513\n",
      "26m 30s (- 36m 54s) (836 41%) 2.2183\n",
      "26m 33s (- 36m 49s) (838 41%) 2.2131\n",
      "26m 36s (- 36m 44s) (840 42%) 2.2216\n",
      "26m 38s (- 36m 43s) (841 42%) 2.1915\n",
      "26m 41s (- 36m 38s) (843 42%) 2.1908\n",
      "26m 44s (- 36m 37s) (844 42%) 2.1852\n",
      "26m 47s (- 36m 32s) (846 42%) 2.1876\n",
      "26m 50s (- 36m 32s) (847 42%) 2.1924\n",
      "26m 53s (- 36m 27s) (849 42%) 2.1937\n",
      "26m 56s (- 36m 26s) (850 42%) 2.1678\n",
      "26m 59s (- 36m 22s) (852 42%) 2.2009\n",
      "27m 2s (- 36m 17s) (854 42%) 2.1767\n",
      "27m 5s (- 36m 16s) (855 42%) 2.1969\n",
      "27m 8s (- 36m 12s) (857 42%) 2.1996\n",
      "27m 11s (- 36m 11s) (858 42%) 2.1674\n",
      "27m 14s (- 36m 6s) (860 43%) 2.1788\n",
      "27m 17s (- 36m 6s) (861 43%) 2.1532\n",
      "27m 20s (- 36m 1s) (863 43%) 2.1315\n",
      "27m 23s (- 35m 56s) (865 43%) 2.2031\n",
      "27m 26s (- 35m 56s) (866 43%) 2.1361\n",
      "27m 29s (- 35m 51s) (868 43%) 2.1731\n",
      "27m 32s (- 35m 51s) (869 43%) 2.1703\n",
      "27m 35s (- 35m 46s) (871 43%) 2.1462\n",
      "27m 39s (- 35m 46s) (872 43%) 2.1474\n",
      "27m 42s (- 35m 41s) (874 43%) 2.1797\n",
      "27m 45s (- 35m 40s) (875 43%) 2.1262\n",
      "27m 48s (- 35m 36s) (877 43%) 2.1027\n",
      "27m 51s (- 35m 31s) (879 43%) 2.1395\n",
      "27m 54s (- 35m 31s) (880 44%) 2.1174\n",
      "27m 57s (- 35m 26s) (882 44%) 2.1217\n",
      "28m 0s (- 35m 26s) (883 44%) 2.1559\n",
      "28m 4s (- 35m 22s) (885 44%) 2.0962\n",
      "28m 10s (- 35m 25s) (886 44%) 2.1127\n",
      "28m 13s (- 35m 20s) (888 44%) 2.1336\n",
      "28m 16s (- 35m 15s) (890 44%) 2.0827\n",
      "28m 19s (- 35m 15s) (891 44%) 2.1009\n",
      "28m 22s (- 35m 10s) (893 44%) 2.0654\n",
      "28m 25s (- 35m 10s) (894 44%) 2.1240\n",
      "28m 28s (- 35m 5s) (896 44%) 2.0730\n",
      "28m 31s (- 35m 4s) (897 44%) 2.0907\n",
      "28m 34s (- 34m 59s) (899 44%) 2.0724\n",
      "28m 37s (- 34m 58s) (900 45%) 2.0874\n",
      "28m 40s (- 34m 53s) (902 45%) 2.0343\n",
      "28m 42s (- 34m 48s) (904 45%) 2.0562\n",
      "28m 46s (- 34m 48s) (905 45%) 2.0572\n",
      "28m 49s (- 34m 44s) (907 45%) 2.0742\n",
      "28m 52s (- 34m 43s) (908 45%) 2.0438\n",
      "28m 55s (- 34m 38s) (910 45%) 2.0506\n",
      "28m 58s (- 34m 37s) (911 45%) 2.0422\n",
      "29m 0s (- 34m 32s) (913 45%) 2.0417\n",
      "29m 3s (- 34m 27s) (915 45%) 2.0417\n",
      "29m 6s (- 34m 26s) (916 45%) 2.0425\n",
      "29m 9s (- 34m 21s) (918 45%) 2.0207\n",
      "29m 12s (- 34m 20s) (919 45%) 2.0109\n",
      "29m 14s (- 34m 16s) (921 46%) 2.0162\n",
      "29m 18s (- 34m 15s) (922 46%) 2.0247\n",
      "29m 20s (- 34m 10s) (924 46%) 2.0197\n",
      "29m 23s (- 34m 9s) (925 46%) 2.0453\n",
      "29m 26s (- 34m 5s) (927 46%) 2.0290\n",
      "29m 29s (- 34m 0s) (929 46%) 2.0243\n",
      "29m 32s (- 33m 59s) (930 46%) 1.9784\n",
      "29m 35s (- 33m 54s) (932 46%) 1.9826\n",
      "29m 38s (- 33m 53s) (933 46%) 1.9829\n",
      "29m 41s (- 33m 48s) (935 46%) 1.9785\n",
      "29m 43s (- 33m 47s) (936 46%) 1.9908\n",
      "29m 46s (- 33m 43s) (938 46%) 1.9942\n",
      "29m 49s (- 33m 38s) (940 47%) 1.9710\n",
      "29m 52s (- 33m 37s) (941 47%) 1.9548\n",
      "29m 55s (- 33m 32s) (943 47%) 1.9497\n",
      "29m 58s (- 33m 31s) (944 47%) 1.9661\n",
      "30m 0s (- 33m 26s) (946 47%) 1.9417\n",
      "30m 3s (- 33m 25s) (947 47%) 2.0024\n",
      "30m 6s (- 33m 20s) (949 47%) 1.8823\n",
      "30m 9s (- 33m 20s) (950 47%) 1.9739\n",
      "30m 12s (- 33m 15s) (952 47%) 1.9642\n",
      "30m 15s (- 33m 10s) (954 47%) 1.9350\n",
      "30m 18s (- 33m 9s) (955 47%) 1.9670\n",
      "30m 20s (- 33m 4s) (957 47%) 1.9500\n",
      "30m 23s (- 33m 3s) (958 47%) 1.9002\n",
      "30m 26s (- 32m 59s) (960 48%) 1.9323\n",
      "30m 29s (- 32m 58s) (961 48%) 1.9385\n",
      "30m 32s (- 32m 53s) (963 48%) 1.9409\n",
      "30m 35s (- 32m 48s) (965 48%) 1.9571\n",
      "30m 38s (- 32m 47s) (966 48%) 1.9383\n",
      "30m 41s (- 32m 42s) (968 48%) 1.9098\n",
      "30m 44s (- 32m 42s) (969 48%) 1.9104\n",
      "30m 47s (- 32m 37s) (971 48%) 1.8880\n",
      "30m 49s (- 32m 36s) (972 48%) 1.8797\n",
      "30m 52s (- 32m 31s) (974 48%) 1.8739\n",
      "30m 55s (- 32m 30s) (975 48%) 1.9164\n",
      "30m 58s (- 32m 26s) (977 48%) 1.8911\n",
      "31m 1s (- 32m 21s) (979 48%) 1.8882\n",
      "31m 4s (- 32m 20s) (980 49%) 1.8769\n",
      "31m 7s (- 32m 15s) (982 49%) 1.8967\n",
      "31m 10s (- 32m 14s) (983 49%) 1.8659\n",
      "31m 13s (- 32m 10s) (985 49%) 1.8745\n",
      "31m 15s (- 32m 9s) (986 49%) 1.8842\n",
      "31m 18s (- 32m 4s) (988 49%) 1.8690\n",
      "31m 21s (- 31m 59s) (990 49%) 1.8780\n",
      "31m 24s (- 31m 58s) (991 49%) 1.8666\n",
      "31m 28s (- 31m 54s) (993 49%) 1.8279\n",
      "31m 32s (- 31m 55s) (994 49%) 1.8619\n",
      "31m 35s (- 31m 50s) (996 49%) 1.8671\n",
      "31m 38s (- 31m 50s) (997 49%) 1.8681\n",
      "31m 41s (- 31m 45s) (999 49%) 1.8446\n",
      "31m 44s (- 31m 44s) (1000 50%) 1.8386\n",
      "31m 47s (- 31m 39s) (1002 50%) 1.8129\n",
      "31m 50s (- 31m 35s) (1004 50%) 1.8623\n",
      "31m 53s (- 31m 34s) (1005 50%) 1.8254\n",
      "31m 56s (- 31m 29s) (1007 50%) 1.8638\n",
      "31m 59s (- 31m 29s) (1008 50%) 1.8081\n",
      "32m 2s (- 31m 24s) (1010 50%) 1.8303\n",
      "32m 6s (- 31m 24s) (1011 50%) 1.8082\n",
      "32m 9s (- 31m 19s) (1013 50%) 1.8390\n",
      "32m 12s (- 31m 14s) (1015 50%) 1.7546\n",
      "32m 14s (- 31m 14s) (1016 50%) 1.8017\n",
      "32m 17s (- 31m 9s) (1018 50%) 1.8266\n",
      "32m 20s (- 31m 8s) (1019 50%) 1.7799\n",
      "32m 23s (- 31m 3s) (1021 51%) 1.8256\n",
      "32m 26s (- 31m 2s) (1022 51%) 1.8020\n",
      "32m 29s (- 30m 58s) (1024 51%) 1.7753\n",
      "32m 32s (- 30m 57s) (1025 51%) 1.7847\n",
      "32m 35s (- 30m 52s) (1027 51%) 1.7689\n",
      "32m 38s (- 30m 48s) (1029 51%) 1.8098\n",
      "32m 41s (- 30m 47s) (1030 51%) 1.7649\n",
      "32m 44s (- 30m 43s) (1032 51%) 1.7995\n",
      "32m 47s (- 30m 42s) (1033 51%) 1.7852\n",
      "32m 50s (- 30m 37s) (1035 51%) 1.7757\n",
      "32m 53s (- 30m 36s) (1036 51%) 1.7594\n",
      "32m 56s (- 30m 32s) (1038 51%) 1.7571\n",
      "33m 0s (- 30m 27s) (1040 52%) 1.7304\n",
      "33m 4s (- 30m 28s) (1041 52%) 1.7700\n",
      "33m 7s (- 30m 23s) (1043 52%) 1.7371\n",
      "33m 10s (- 30m 22s) (1044 52%) 1.7418\n",
      "33m 13s (- 30m 18s) (1046 52%) 1.7651\n",
      "33m 16s (- 30m 17s) (1047 52%) 1.6949\n",
      "33m 19s (- 30m 12s) (1049 52%) 1.7192\n",
      "33m 22s (- 30m 11s) (1050 52%) 1.7417\n",
      "33m 25s (- 30m 6s) (1052 52%) 1.7432\n",
      "33m 28s (- 30m 2s) (1054 52%) 1.7251\n",
      "33m 31s (- 30m 1s) (1055 52%) 1.7143\n",
      "33m 34s (- 29m 57s) (1057 52%) 1.7248\n",
      "33m 37s (- 29m 56s) (1058 52%) 1.7031\n",
      "33m 40s (- 29m 51s) (1060 53%) 1.6829\n",
      "33m 43s (- 29m 50s) (1061 53%) 1.7091\n",
      "33m 46s (- 29m 46s) (1063 53%) 1.6915\n",
      "33m 49s (- 29m 41s) (1065 53%) 1.6636\n",
      "33m 52s (- 29m 40s) (1066 53%) 1.7088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33m 55s (- 29m 36s) (1068 53%) 1.7129\n",
      "33m 58s (- 29m 35s) (1069 53%) 1.6377\n",
      "34m 1s (- 29m 30s) (1071 53%) 1.6676\n",
      "34m 4s (- 29m 29s) (1072 53%) 1.6728\n",
      "34m 7s (- 29m 25s) (1074 53%) 1.6663\n",
      "34m 10s (- 29m 24s) (1075 53%) 1.6513\n",
      "34m 13s (- 29m 19s) (1077 53%) 1.6588\n",
      "34m 16s (- 29m 15s) (1079 53%) 1.6513\n",
      "34m 19s (- 29m 14s) (1080 54%) 1.6691\n",
      "34m 23s (- 29m 10s) (1082 54%) 1.6572\n",
      "34m 26s (- 29m 9s) (1083 54%) 1.6620\n",
      "34m 28s (- 29m 4s) (1085 54%) 1.6515\n",
      "34m 31s (- 29m 3s) (1086 54%) 1.6682\n",
      "34m 34s (- 28m 58s) (1088 54%) 1.6456\n",
      "34m 37s (- 28m 54s) (1090 54%) 1.6279\n",
      "34m 40s (- 28m 53s) (1091 54%) 1.6589\n",
      "34m 43s (- 28m 49s) (1093 54%) 1.6498\n",
      "34m 47s (- 28m 48s) (1094 54%) 1.6040\n",
      "34m 50s (- 28m 44s) (1096 54%) 1.6343\n",
      "34m 52s (- 28m 42s) (1097 54%) 1.6218\n",
      "34m 55s (- 28m 38s) (1099 54%) 1.6129\n",
      "34m 58s (- 28m 37s) (1100 55%) 1.6355\n",
      "35m 1s (- 28m 32s) (1102 55%) 1.6095\n",
      "35m 4s (- 28m 27s) (1104 55%) 1.6186\n",
      "35m 7s (- 28m 27s) (1105 55%) 1.5964\n",
      "35m 10s (- 28m 22s) (1107 55%) 1.6241\n",
      "35m 14s (- 28m 22s) (1108 55%) 1.6209\n",
      "35m 17s (- 28m 18s) (1110 55%) 1.6014\n",
      "35m 21s (- 28m 17s) (1111 55%) 1.5863\n",
      "35m 24s (- 28m 13s) (1113 55%) 1.5843\n",
      "35m 27s (- 28m 9s) (1115 55%) 1.6209\n",
      "35m 31s (- 28m 8s) (1116 55%) 1.5974\n",
      "35m 34s (- 28m 3s) (1118 55%) 1.6049\n",
      "35m 36s (- 28m 2s) (1119 55%) 1.5856\n",
      "35m 39s (- 27m 57s) (1121 56%) 1.5669\n",
      "35m 42s (- 27m 56s) (1122 56%) 1.5969\n",
      "35m 45s (- 27m 52s) (1124 56%) 1.5413\n",
      "35m 48s (- 27m 50s) (1125 56%) 1.5796\n",
      "35m 51s (- 27m 46s) (1127 56%) 1.5757\n",
      "35m 54s (- 27m 42s) (1129 56%) 1.5515\n",
      "35m 57s (- 27m 41s) (1130 56%) 1.5607\n",
      "36m 1s (- 27m 37s) (1132 56%) 1.5419\n",
      "36m 4s (- 27m 36s) (1133 56%) 1.5396\n",
      "36m 7s (- 27m 31s) (1135 56%) 1.5520\n",
      "36m 10s (- 27m 30s) (1136 56%) 1.5582\n",
      "36m 13s (- 27m 26s) (1138 56%) 1.5523\n",
      "36m 16s (- 27m 21s) (1140 56%) 1.5058\n",
      "36m 19s (- 27m 20s) (1141 57%) 1.5396\n",
      "36m 22s (- 27m 16s) (1143 57%) 1.5104\n",
      "36m 25s (- 27m 15s) (1144 57%) 1.5350\n",
      "36m 28s (- 27m 10s) (1146 57%) 1.5129\n",
      "36m 31s (- 27m 9s) (1147 57%) 1.5263\n",
      "36m 33s (- 27m 4s) (1149 57%) 1.4920\n",
      "36m 36s (- 27m 3s) (1150 57%) 1.5122\n",
      "36m 40s (- 26m 59s) (1152 57%) 1.5171\n",
      "36m 42s (- 26m 54s) (1154 57%) 1.5015\n",
      "36m 45s (- 26m 53s) (1155 57%) 1.4916\n",
      "36m 48s (- 26m 49s) (1157 57%) 1.5008\n",
      "36m 51s (- 26m 48s) (1158 57%) 1.5117\n",
      "36m 54s (- 26m 43s) (1160 57%) 1.5239\n",
      "36m 57s (- 26m 42s) (1161 58%) 1.4811\n",
      "37m 1s (- 26m 38s) (1163 58%) 1.4794\n",
      "37m 4s (- 26m 34s) (1165 58%) 1.5064\n",
      "37m 7s (- 26m 33s) (1166 58%) 1.4642\n",
      "37m 10s (- 26m 28s) (1168 58%) 1.4973\n",
      "37m 13s (- 26m 27s) (1169 58%) 1.4737\n",
      "37m 16s (- 26m 23s) (1171 58%) 1.4743\n",
      "37m 19s (- 26m 22s) (1172 58%) 1.4496\n",
      "37m 22s (- 26m 17s) (1174 58%) 1.4654\n",
      "37m 25s (- 26m 16s) (1175 58%) 1.4505\n",
      "37m 27s (- 26m 11s) (1177 58%) 1.4661\n",
      "37m 30s (- 26m 7s) (1179 58%) 1.4785\n",
      "37m 33s (- 26m 6s) (1180 59%) 1.4110\n",
      "37m 36s (- 26m 1s) (1182 59%) 1.4314\n",
      "37m 39s (- 26m 0s) (1183 59%) 1.4221\n",
      "37m 42s (- 25m 55s) (1185 59%) 1.4473\n",
      "37m 45s (- 25m 54s) (1186 59%) 1.4278\n",
      "37m 48s (- 25m 50s) (1188 59%) 1.4463\n",
      "37m 51s (- 25m 46s) (1190 59%) 1.4071\n",
      "37m 54s (- 25m 44s) (1191 59%) 1.4565\n",
      "37m 57s (- 25m 40s) (1193 59%) 1.4208\n",
      "38m 0s (- 25m 39s) (1194 59%) 1.4285\n",
      "38m 2s (- 25m 34s) (1196 59%) 1.3878\n",
      "38m 5s (- 25m 33s) (1197 59%) 1.4101\n",
      "38m 8s (- 25m 28s) (1199 59%) 1.3921\n",
      "38m 11s (- 25m 27s) (1200 60%) 1.4048\n",
      "38m 14s (- 25m 23s) (1202 60%) 1.4123\n",
      "38m 16s (- 25m 18s) (1204 60%) 1.3886\n",
      "38m 19s (- 25m 17s) (1205 60%) 1.3993\n",
      "38m 22s (- 25m 12s) (1207 60%) 1.3851\n",
      "38m 25s (- 25m 11s) (1208 60%) 1.3791\n",
      "38m 28s (- 25m 7s) (1210 60%) 1.3595\n",
      "38m 31s (- 25m 5s) (1211 60%) 1.3998\n",
      "38m 34s (- 25m 1s) (1213 60%) 1.3973\n",
      "38m 36s (- 24m 56s) (1215 60%) 1.3644\n",
      "38m 39s (- 24m 55s) (1216 60%) 1.3657\n",
      "38m 42s (- 24m 51s) (1218 60%) 1.3650\n",
      "38m 45s (- 24m 49s) (1219 60%) 1.3702\n",
      "38m 48s (- 24m 45s) (1221 61%) 1.3666\n",
      "38m 51s (- 24m 44s) (1222 61%) 1.3502\n",
      "38m 53s (- 24m 39s) (1224 61%) 1.3302\n",
      "38m 56s (- 24m 38s) (1225 61%) 1.3684\n",
      "38m 59s (- 24m 33s) (1227 61%) 1.3705\n",
      "39m 2s (- 24m 29s) (1229 61%) 1.3688\n",
      "39m 5s (- 24m 28s) (1230 61%) 1.3420\n",
      "39m 8s (- 24m 23s) (1232 61%) 1.2897\n",
      "39m 10s (- 24m 22s) (1233 61%) 1.3381\n",
      "39m 13s (- 24m 18s) (1235 61%) 1.3697\n",
      "39m 17s (- 24m 16s) (1236 61%) 1.3144\n",
      "39m 19s (- 24m 12s) (1238 61%) 1.3054\n",
      "39m 22s (- 24m 8s) (1240 62%) 1.3372\n",
      "39m 25s (- 24m 6s) (1241 62%) 1.3222\n",
      "39m 28s (- 24m 2s) (1243 62%) 1.2964\n",
      "39m 31s (- 24m 0s) (1244 62%) 1.2973\n",
      "39m 33s (- 23m 56s) (1246 62%) 1.2994\n",
      "39m 36s (- 23m 55s) (1247 62%) 1.3206\n",
      "39m 39s (- 23m 50s) (1249 62%) 1.3068\n",
      "39m 42s (- 23m 49s) (1250 62%) 1.2873\n",
      "39m 45s (- 23m 45s) (1252 62%) 1.3214\n",
      "39m 48s (- 23m 40s) (1254 62%) 1.2947\n",
      "39m 51s (- 23m 39s) (1255 62%) 1.3441\n",
      "39m 53s (- 23m 35s) (1257 62%) 1.2581\n",
      "39m 57s (- 23m 33s) (1258 62%) 1.2719\n",
      "40m 0s (- 23m 29s) (1260 63%) 1.2721\n",
      "40m 3s (- 23m 28s) (1261 63%) 1.2941\n",
      "40m 6s (- 23m 24s) (1263 63%) 1.2666\n",
      "40m 9s (- 23m 19s) (1265 63%) 1.2906\n",
      "40m 11s (- 23m 18s) (1266 63%) 1.2989\n",
      "40m 14s (- 23m 14s) (1268 63%) 1.2845\n",
      "40m 17s (- 23m 12s) (1269 63%) 1.2720\n",
      "40m 21s (- 23m 8s) (1271 63%) 1.2889\n",
      "40m 24s (- 23m 7s) (1272 63%) 1.2672\n",
      "40m 26s (- 23m 3s) (1274 63%) 1.2584\n",
      "40m 29s (- 23m 1s) (1275 63%) 1.2523\n",
      "40m 32s (- 22m 57s) (1277 63%) 1.2610\n",
      "40m 35s (- 22m 52s) (1279 63%) 1.2295\n",
      "40m 38s (- 22m 51s) (1280 64%) 1.2478\n",
      "40m 41s (- 22m 47s) (1282 64%) 1.2296\n",
      "40m 44s (- 22m 45s) (1283 64%) 1.2347\n",
      "40m 46s (- 22m 41s) (1285 64%) 1.2381\n",
      "40m 49s (- 22m 40s) (1286 64%) 1.2501\n",
      "40m 52s (- 22m 35s) (1288 64%) 1.2257\n",
      "40m 55s (- 22m 31s) (1290 64%) 1.2294\n",
      "40m 58s (- 22m 30s) (1291 64%) 1.2435\n",
      "41m 1s (- 22m 25s) (1293 64%) 1.1990\n",
      "41m 4s (- 22m 24s) (1294 64%) 1.2144\n",
      "41m 7s (- 22m 20s) (1296 64%) 1.2327\n",
      "41m 10s (- 22m 18s) (1297 64%) 1.2109\n",
      "41m 12s (- 22m 14s) (1299 64%) 1.1885\n",
      "41m 15s (- 22m 13s) (1300 65%) 1.2098\n",
      "41m 18s (- 22m 8s) (1302 65%) 1.2063\n",
      "41m 21s (- 22m 4s) (1304 65%) 1.2225\n",
      "41m 25s (- 22m 3s) (1305 65%) 1.1792\n",
      "41m 28s (- 21m 59s) (1307 65%) 1.1867\n",
      "41m 31s (- 21m 58s) (1308 65%) 1.1843\n",
      "41m 34s (- 21m 53s) (1310 65%) 1.1495\n",
      "41m 37s (- 21m 52s) (1311 65%) 1.1790\n",
      "41m 41s (- 21m 48s) (1313 65%) 1.1698\n",
      "41m 44s (- 21m 44s) (1315 65%) 1.1758\n",
      "41m 48s (- 21m 43s) (1316 65%) 1.1833\n",
      "41m 51s (- 21m 39s) (1318 65%) 1.1487\n",
      "41m 54s (- 21m 38s) (1319 65%) 1.1677\n",
      "41m 57s (- 21m 33s) (1321 66%) 1.1630\n",
      "42m 0s (- 21m 32s) (1322 66%) 1.1576\n",
      "42m 3s (- 21m 28s) (1324 66%) 1.1497\n",
      "42m 6s (- 21m 26s) (1325 66%) 1.1654\n",
      "42m 8s (- 21m 22s) (1327 66%) 1.1547\n",
      "42m 11s (- 21m 18s) (1329 66%) 1.1442\n",
      "42m 14s (- 21m 16s) (1330 66%) 1.1534\n",
      "42m 17s (- 21m 12s) (1332 66%) 1.1648\n",
      "42m 20s (- 21m 11s) (1333 66%) 1.1303\n",
      "42m 23s (- 21m 7s) (1335 66%) 1.1411\n",
      "42m 27s (- 21m 5s) (1336 66%) 1.1567\n",
      "42m 30s (- 21m 1s) (1338 66%) 1.1468\n",
      "42m 33s (- 20m 57s) (1340 67%) 1.1386\n",
      "42m 36s (- 20m 56s) (1341 67%) 1.1522\n",
      "42m 40s (- 20m 52s) (1343 67%) 1.1355\n",
      "42m 43s (- 20m 51s) (1344 67%) 1.1079\n",
      "42m 45s (- 20m 46s) (1346 67%) 1.1008\n",
      "42m 48s (- 20m 45s) (1347 67%) 1.1136\n",
      "42m 52s (- 20m 41s) (1349 67%) 1.1452\n",
      "42m 55s (- 20m 39s) (1350 67%) 1.1107\n",
      "42m 57s (- 20m 35s) (1352 67%) 1.1120\n",
      "43m 0s (- 20m 31s) (1354 67%) 1.0864\n",
      "43m 3s (- 20m 29s) (1355 67%) 1.0814\n",
      "43m 6s (- 20m 25s) (1357 67%) 1.1011\n",
      "43m 9s (- 20m 24s) (1358 67%) 1.0898\n",
      "43m 12s (- 20m 19s) (1360 68%) 1.0895\n",
      "43m 15s (- 20m 18s) (1361 68%) 1.0716\n",
      "43m 18s (- 20m 14s) (1363 68%) 1.0641\n",
      "43m 20s (- 20m 9s) (1365 68%) 1.0803\n",
      "43m 23s (- 20m 8s) (1366 68%) 1.1041\n",
      "43m 26s (- 20m 4s) (1368 68%) 1.0896\n",
      "43m 29s (- 20m 2s) (1369 68%) 1.0728\n",
      "43m 32s (- 19m 58s) (1371 68%) 1.0747\n",
      "43m 35s (- 19m 57s) (1372 68%) 1.0764\n",
      "43m 38s (- 19m 52s) (1374 68%) 1.0740\n",
      "43m 41s (- 19m 51s) (1375 68%) 1.0458\n",
      "43m 44s (- 19m 47s) (1377 68%) 1.0799\n",
      "43m 47s (- 19m 43s) (1379 68%) 1.0481\n",
      "43m 50s (- 19m 41s) (1380 69%) 1.0549\n",
      "43m 53s (- 19m 37s) (1382 69%) 1.0535\n",
      "43m 56s (- 19m 36s) (1383 69%) 1.0589\n",
      "43m 59s (- 19m 31s) (1385 69%) 1.0458\n",
      "44m 2s (- 19m 30s) (1386 69%) 1.0431\n",
      "44m 5s (- 19m 26s) (1388 69%) 1.0254\n",
      "44m 7s (- 19m 22s) (1390 69%) 1.0369\n",
      "44m 10s (- 19m 20s) (1391 69%) 1.0285\n",
      "44m 13s (- 19m 16s) (1393 69%) 1.0559\n",
      "44m 16s (- 19m 14s) (1394 69%) 1.0154\n",
      "44m 19s (- 19m 10s) (1396 69%) 1.0359\n",
      "44m 22s (- 19m 9s) (1397 69%) 1.0139\n",
      "44m 25s (- 19m 4s) (1399 69%) 1.0150\n",
      "44m 27s (- 19m 3s) (1400 70%) 1.0303\n",
      "44m 30s (- 18m 59s) (1402 70%) 1.0302\n",
      "44m 33s (- 18m 54s) (1404 70%) 0.9900\n",
      "44m 36s (- 18m 53s) (1405 70%) 1.0185\n",
      "44m 39s (- 18m 49s) (1407 70%) 1.0088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44m 42s (- 18m 47s) (1408 70%) 1.0036\n",
      "44m 44s (- 18m 43s) (1410 70%) 0.9948\n",
      "44m 47s (- 18m 41s) (1411 70%) 1.0030\n",
      "44m 50s (- 18m 37s) (1413 70%) 0.9946\n",
      "44m 53s (- 18m 33s) (1415 70%) 0.9863\n",
      "44m 56s (- 18m 31s) (1416 70%) 0.9723\n",
      "44m 58s (- 18m 27s) (1418 70%) 0.9835\n",
      "45m 1s (- 18m 26s) (1419 70%) 0.9978\n",
      "45m 4s (- 18m 21s) (1421 71%) 0.9860\n",
      "45m 7s (- 18m 20s) (1422 71%) 0.9674\n",
      "45m 10s (- 18m 16s) (1424 71%) 0.9667\n",
      "45m 12s (- 18m 14s) (1425 71%) 0.9781\n",
      "45m 15s (- 18m 10s) (1427 71%) 0.9824\n",
      "45m 18s (- 18m 6s) (1429 71%) 0.9742\n",
      "45m 21s (- 18m 4s) (1430 71%) 0.9689\n",
      "45m 24s (- 18m 0s) (1432 71%) 0.9600\n",
      "45m 26s (- 17m 58s) (1433 71%) 0.9619\n",
      "45m 29s (- 17m 54s) (1435 71%) 0.9604\n",
      "45m 32s (- 17m 53s) (1436 71%) 0.9469\n",
      "45m 35s (- 17m 49s) (1438 71%) 0.9691\n",
      "45m 38s (- 17m 44s) (1440 72%) 0.9561\n",
      "45m 41s (- 17m 43s) (1441 72%) 0.9682\n",
      "45m 43s (- 17m 39s) (1443 72%) 0.9305\n",
      "45m 46s (- 17m 37s) (1444 72%) 0.9309\n",
      "45m 49s (- 17m 33s) (1446 72%) 0.9694\n",
      "45m 52s (- 17m 31s) (1447 72%) 0.9457\n",
      "45m 55s (- 17m 27s) (1449 72%) 0.9388\n",
      "45m 57s (- 17m 26s) (1450 72%) 0.9195\n",
      "46m 0s (- 17m 21s) (1452 72%) 0.9290\n",
      "46m 3s (- 17m 17s) (1454 72%) 0.9120\n",
      "46m 6s (- 17m 16s) (1455 72%) 0.9201\n",
      "46m 9s (- 17m 12s) (1457 72%) 0.9240\n",
      "46m 11s (- 17m 10s) (1458 72%) 0.9144\n",
      "46m 14s (- 17m 6s) (1460 73%) 0.9150\n",
      "46m 17s (- 17m 4s) (1461 73%) 0.8976\n",
      "46m 20s (- 17m 0s) (1463 73%) 0.8962\n",
      "46m 23s (- 16m 56s) (1465 73%) 0.9184\n",
      "46m 26s (- 16m 54s) (1466 73%) 0.8838\n",
      "46m 28s (- 16m 50s) (1468 73%) 0.8986\n",
      "46m 31s (- 16m 49s) (1469 73%) 0.8744\n",
      "46m 34s (- 16m 44s) (1471 73%) 0.8922\n",
      "46m 37s (- 16m 43s) (1472 73%) 0.8926\n",
      "46m 40s (- 16m 39s) (1474 73%) 0.8985\n",
      "46m 42s (- 16m 37s) (1475 73%) 0.8963\n",
      "46m 45s (- 16m 33s) (1477 73%) 0.9012\n",
      "46m 48s (- 16m 29s) (1479 73%) 0.8733\n",
      "46m 51s (- 16m 27s) (1480 74%) 0.8891\n",
      "46m 54s (- 16m 23s) (1482 74%) 0.8888\n",
      "46m 56s (- 16m 22s) (1483 74%) 0.8674\n",
      "46m 59s (- 16m 17s) (1485 74%) 0.8881\n",
      "47m 2s (- 16m 16s) (1486 74%) 0.8509\n",
      "47m 5s (- 16m 12s) (1488 74%) 0.8658\n",
      "47m 8s (- 16m 8s) (1490 74%) 0.8841\n",
      "47m 10s (- 16m 6s) (1491 74%) 0.8529\n",
      "47m 13s (- 16m 2s) (1493 74%) 0.8741\n",
      "47m 16s (- 16m 0s) (1494 74%) 0.8502\n",
      "47m 19s (- 15m 56s) (1496 74%) 0.8595\n",
      "47m 22s (- 15m 55s) (1497 74%) 0.8560\n",
      "47m 25s (- 15m 50s) (1499 74%) 0.8564\n",
      "47m 27s (- 15m 49s) (1500 75%) 0.8494\n",
      "47m 30s (- 15m 45s) (1502 75%) 0.8327\n",
      "47m 33s (- 15m 41s) (1504 75%) 0.8371\n",
      "47m 36s (- 15m 39s) (1505 75%) 0.8271\n",
      "47m 38s (- 15m 35s) (1507 75%) 0.8324\n",
      "47m 41s (- 15m 33s) (1508 75%) 0.8431\n",
      "47m 44s (- 15m 29s) (1510 75%) 0.8178\n",
      "47m 47s (- 15m 28s) (1511 75%) 0.8327\n",
      "47m 50s (- 15m 23s) (1513 75%) 0.8191\n",
      "47m 53s (- 15m 19s) (1515 75%) 0.8362\n",
      "47m 56s (- 15m 18s) (1516 75%) 0.8090\n",
      "47m 58s (- 15m 14s) (1518 75%) 0.8491\n",
      "48m 1s (- 15m 12s) (1519 75%) 0.8014\n",
      "48m 4s (- 15m 8s) (1521 76%) 0.8300\n",
      "48m 7s (- 15m 6s) (1522 76%) 0.8117\n",
      "48m 10s (- 15m 2s) (1524 76%) 0.7927\n",
      "48m 12s (- 15m 1s) (1525 76%) 0.8128\n",
      "48m 15s (- 14m 56s) (1527 76%) 0.7961\n",
      "48m 18s (- 14m 52s) (1529 76%) 0.8233\n",
      "48m 21s (- 14m 51s) (1530 76%) 0.8042\n",
      "48m 24s (- 14m 47s) (1532 76%) 0.8000\n",
      "48m 26s (- 14m 45s) (1533 76%) 0.8111\n",
      "48m 29s (- 14m 41s) (1535 76%) 0.7965\n",
      "48m 32s (- 14m 39s) (1536 76%) 0.7921\n",
      "48m 36s (- 14m 35s) (1538 76%) 0.7752\n",
      "48m 39s (- 14m 32s) (1540 77%) 0.8103\n",
      "48m 42s (- 14m 30s) (1541 77%) 0.7664\n",
      "48m 45s (- 14m 26s) (1543 77%) 0.7816\n",
      "48m 48s (- 14m 24s) (1544 77%) 0.7965\n",
      "48m 50s (- 14m 20s) (1546 77%) 0.7754\n",
      "48m 53s (- 14m 19s) (1547 77%) 0.7769\n",
      "48m 56s (- 14m 15s) (1549 77%) 0.7756\n",
      "48m 59s (- 14m 13s) (1550 77%) 0.7669\n",
      "49m 2s (- 14m 9s) (1552 77%) 0.7852\n",
      "49m 5s (- 14m 5s) (1554 77%) 0.7471\n",
      "49m 7s (- 14m 3s) (1555 77%) 0.7516\n",
      "49m 10s (- 13m 59s) (1557 77%) 0.7782\n",
      "49m 13s (- 13m 57s) (1558 77%) 0.7753\n",
      "49m 16s (- 13m 53s) (1560 78%) 0.7371\n",
      "49m 19s (- 13m 52s) (1561 78%) 0.7537\n",
      "49m 22s (- 13m 48s) (1563 78%) 0.7497\n",
      "49m 25s (- 13m 44s) (1565 78%) 0.7540\n",
      "49m 27s (- 13m 42s) (1566 78%) 0.7447\n",
      "49m 30s (- 13m 38s) (1568 78%) 0.7498\n",
      "49m 33s (- 13m 36s) (1569 78%) 0.7418\n",
      "49m 36s (- 13m 32s) (1571 78%) 0.7466\n",
      "49m 39s (- 13m 31s) (1572 78%) 0.7404\n",
      "49m 41s (- 13m 27s) (1574 78%) 0.7330\n",
      "49m 44s (- 13m 25s) (1575 78%) 0.7407\n",
      "49m 47s (- 13m 21s) (1577 78%) 0.7288\n",
      "49m 50s (- 13m 17s) (1579 78%) 0.7394\n",
      "49m 53s (- 13m 15s) (1580 79%) 0.7224\n",
      "49m 56s (- 13m 11s) (1582 79%) 0.7122\n",
      "49m 58s (- 13m 9s) (1583 79%) 0.7303\n",
      "50m 1s (- 13m 5s) (1585 79%) 0.7396\n",
      "50m 4s (- 13m 4s) (1586 79%) 0.7166\n",
      "50m 7s (- 13m 0s) (1588 79%) 0.7167\n",
      "50m 10s (- 12m 56s) (1590 79%) 0.7138\n",
      "50m 12s (- 12m 54s) (1591 79%) 0.7177\n",
      "50m 15s (- 12m 50s) (1593 79%) 0.7029\n",
      "50m 18s (- 12m 48s) (1594 79%) 0.7110\n",
      "50m 21s (- 12m 44s) (1596 79%) 0.7125\n",
      "50m 24s (- 12m 43s) (1597 79%) 0.6897\n",
      "50m 27s (- 12m 39s) (1599 79%) 0.6922\n",
      "50m 29s (- 12m 37s) (1600 80%) 0.7061\n",
      "50m 32s (- 12m 33s) (1602 80%) 0.6864\n",
      "50m 35s (- 12m 29s) (1604 80%) 0.7044\n",
      "50m 38s (- 12m 27s) (1605 80%) 0.6906\n",
      "50m 41s (- 12m 23s) (1607 80%) 0.6781\n",
      "50m 43s (- 12m 22s) (1608 80%) 0.6991\n",
      "50m 46s (- 12m 18s) (1610 80%) 0.6874\n",
      "50m 49s (- 12m 16s) (1611 80%) 0.6832\n",
      "50m 52s (- 12m 12s) (1613 80%) 0.6727\n",
      "50m 55s (- 12m 8s) (1615 80%) 0.6848\n",
      "50m 58s (- 12m 6s) (1616 80%) 0.6699\n",
      "51m 0s (- 12m 2s) (1618 80%) 0.6644\n",
      "51m 3s (- 12m 0s) (1619 80%) 0.6822\n",
      "51m 6s (- 11m 56s) (1621 81%) 0.6873\n",
      "51m 9s (- 11m 55s) (1622 81%) 0.6683\n",
      "51m 12s (- 11m 51s) (1624 81%) 0.6772\n",
      "51m 14s (- 11m 49s) (1625 81%) 0.6706\n",
      "51m 18s (- 11m 45s) (1627 81%) 0.6481\n",
      "51m 21s (- 11m 41s) (1629 81%) 0.6591\n",
      "51m 24s (- 11m 40s) (1630 81%) 0.6677\n",
      "51m 27s (- 11m 36s) (1632 81%) 0.6744\n",
      "51m 30s (- 11m 34s) (1633 81%) 0.6489\n",
      "51m 34s (- 11m 30s) (1635 81%) 0.6628\n",
      "51m 37s (- 11m 29s) (1636 81%) 0.6562\n",
      "51m 40s (- 11m 25s) (1638 81%) 0.6426\n",
      "51m 43s (- 11m 21s) (1640 82%) 0.6390\n",
      "51m 45s (- 11m 19s) (1641 82%) 0.6424\n",
      "51m 48s (- 11m 15s) (1643 82%) 0.6345\n",
      "51m 51s (- 11m 13s) (1644 82%) 0.6320\n",
      "51m 54s (- 11m 9s) (1646 82%) 0.6487\n",
      "51m 57s (- 11m 8s) (1647 82%) 0.6349\n",
      "51m 59s (- 11m 4s) (1649 82%) 0.6383\n",
      "52m 2s (- 11m 2s) (1650 82%) 0.6207\n",
      "52m 5s (- 10m 58s) (1652 82%) 0.6227\n",
      "52m 8s (- 10m 54s) (1654 82%) 0.6226\n",
      "52m 11s (- 10m 52s) (1655 82%) 0.6159\n",
      "52m 15s (- 10m 48s) (1657 82%) 0.6199\n",
      "52m 18s (- 10m 47s) (1658 82%) 0.6347\n",
      "52m 21s (- 10m 43s) (1660 83%) 0.6231\n",
      "52m 24s (- 10m 41s) (1661 83%) 0.6037\n",
      "52m 27s (- 10m 37s) (1663 83%) 0.6046\n",
      "52m 30s (- 10m 33s) (1665 83%) 0.6190\n",
      "52m 33s (- 10m 32s) (1666 83%) 0.6172\n",
      "52m 36s (- 10m 28s) (1668 83%) 0.6093\n",
      "52m 39s (- 10m 26s) (1669 83%) 0.6196\n",
      "52m 41s (- 10m 22s) (1671 83%) 0.6163\n",
      "52m 44s (- 10m 20s) (1672 83%) 0.5887\n",
      "52m 47s (- 10m 16s) (1674 83%) 0.5918\n",
      "52m 50s (- 10m 15s) (1675 83%) 0.6033\n",
      "52m 53s (- 10m 11s) (1677 83%) 0.5937\n",
      "52m 56s (- 10m 7s) (1679 83%) 0.6025\n",
      "52m 59s (- 10m 5s) (1680 84%) 0.6020\n",
      "53m 1s (- 10m 1s) (1682 84%) 0.5925\n",
      "53m 4s (- 9m 59s) (1683 84%) 0.5895\n",
      "53m 7s (- 9m 55s) (1685 84%) 0.5762\n",
      "53m 10s (- 9m 54s) (1686 84%) 0.5951\n",
      "53m 14s (- 9m 50s) (1688 84%) 0.5872\n",
      "53m 16s (- 9m 46s) (1690 84%) 0.5796\n",
      "53m 19s (- 9m 44s) (1691 84%) 0.5818\n",
      "53m 22s (- 9m 40s) (1693 84%) 0.5889\n",
      "53m 25s (- 9m 38s) (1694 84%) 0.5731\n",
      "53m 28s (- 9m 35s) (1696 84%) 0.5751\n",
      "53m 30s (- 9m 33s) (1697 84%) 0.5739\n",
      "53m 33s (- 9m 29s) (1699 84%) 0.5736\n",
      "53m 36s (- 9m 27s) (1700 85%) 0.5710\n",
      "53m 39s (- 9m 23s) (1702 85%) 0.5853\n",
      "53m 43s (- 9m 19s) (1704 85%) 0.5746\n",
      "53m 46s (- 9m 18s) (1705 85%) 0.5606\n",
      "53m 48s (- 9m 14s) (1707 85%) 0.5735\n",
      "53m 51s (- 9m 12s) (1708 85%) 0.5615\n",
      "53m 54s (- 9m 8s) (1710 85%) 0.5564\n",
      "53m 57s (- 9m 6s) (1711 85%) 0.5573\n",
      "54m 0s (- 9m 2s) (1713 85%) 0.5540\n",
      "54m 2s (- 8m 58s) (1715 85%) 0.5610\n",
      "54m 5s (- 8m 57s) (1716 85%) 0.5583\n",
      "54m 8s (- 8m 53s) (1718 85%) 0.5556\n",
      "54m 11s (- 8m 51s) (1719 85%) 0.5494\n",
      "54m 14s (- 8m 47s) (1721 86%) 0.5562\n",
      "54m 17s (- 8m 45s) (1722 86%) 0.5463\n",
      "54m 20s (- 8m 41s) (1724 86%) 0.5369\n",
      "54m 22s (- 8m 40s) (1725 86%) 0.5562\n",
      "54m 25s (- 8m 36s) (1727 86%) 0.5423\n",
      "54m 28s (- 8m 32s) (1729 86%) 0.5378\n",
      "54m 31s (- 8m 30s) (1730 86%) 0.5423\n",
      "54m 34s (- 8m 26s) (1732 86%) 0.5236\n",
      "54m 37s (- 8m 24s) (1733 86%) 0.5397\n",
      "54m 39s (- 8m 20s) (1735 86%) 0.5404\n",
      "54m 42s (- 8m 19s) (1736 86%) 0.5345\n",
      "54m 45s (- 8m 15s) (1738 86%) 0.5238\n",
      "54m 48s (- 8m 11s) (1740 87%) 0.5392\n",
      "54m 51s (- 8m 9s) (1741 87%) 0.5413\n",
      "54m 54s (- 8m 5s) (1743 87%) 0.5283\n",
      "54m 57s (- 8m 3s) (1744 87%) 0.5212\n",
      "54m 59s (- 8m 0s) (1746 87%) 0.5288\n",
      "55m 2s (- 7m 58s) (1747 87%) 0.5248\n",
      "55m 5s (- 7m 54s) (1749 87%) 0.5270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55m 8s (- 7m 52s) (1750 87%) 0.5144\n",
      "55m 11s (- 7m 48s) (1752 87%) 0.5173\n",
      "55m 14s (- 7m 44s) (1754 87%) 0.5062\n",
      "55m 17s (- 7m 43s) (1755 87%) 0.5217\n",
      "55m 19s (- 7m 39s) (1757 87%) 0.5112\n",
      "55m 22s (- 7m 37s) (1758 87%) 0.5186\n",
      "55m 25s (- 7m 33s) (1760 88%) 0.5128\n",
      "55m 28s (- 7m 31s) (1761 88%) 0.5011\n",
      "55m 31s (- 7m 27s) (1763 88%) 0.5222\n",
      "55m 34s (- 7m 23s) (1765 88%) 0.5088\n",
      "55m 37s (- 7m 22s) (1766 88%) 0.5057\n",
      "55m 40s (- 7m 18s) (1768 88%) 0.5076\n",
      "55m 43s (- 7m 16s) (1769 88%) 0.4982\n",
      "55m 46s (- 7m 12s) (1771 88%) 0.5100\n",
      "55m 48s (- 7m 10s) (1772 88%) 0.4997\n",
      "55m 51s (- 7m 7s) (1774 88%) 0.4895\n",
      "55m 54s (- 7m 5s) (1775 88%) 0.4983\n",
      "55m 57s (- 7m 1s) (1777 88%) 0.4873\n",
      "56m 0s (- 6m 57s) (1779 88%) 0.4964\n",
      "56m 3s (- 6m 55s) (1780 89%) 0.4902\n",
      "56m 6s (- 6m 51s) (1782 89%) 0.4953\n",
      "56m 9s (- 6m 50s) (1783 89%) 0.4862\n",
      "56m 12s (- 6m 46s) (1785 89%) 0.4891\n",
      "56m 16s (- 6m 44s) (1786 89%) 0.4851\n",
      "56m 20s (- 6m 40s) (1788 89%) 0.4970\n",
      "56m 23s (- 6m 36s) (1790 89%) 0.4795\n",
      "56m 26s (- 6m 35s) (1791 89%) 0.4891\n",
      "56m 29s (- 6m 31s) (1793 89%) 0.4800\n",
      "56m 32s (- 6m 29s) (1794 89%) 0.4734\n",
      "56m 35s (- 6m 25s) (1796 89%) 0.4791\n",
      "56m 38s (- 6m 23s) (1797 89%) 0.4756\n",
      "56m 41s (- 6m 20s) (1799 89%) 0.4756\n",
      "56m 44s (- 6m 18s) (1800 90%) 0.4842\n",
      "56m 47s (- 6m 14s) (1802 90%) 0.4738\n",
      "56m 50s (- 6m 10s) (1804 90%) 0.4708\n",
      "56m 52s (- 6m 8s) (1805 90%) 0.4811\n",
      "56m 55s (- 6m 4s) (1807 90%) 0.4587\n",
      "56m 58s (- 6m 3s) (1808 90%) 0.4638\n",
      "57m 1s (- 5m 59s) (1810 90%) 0.4636\n",
      "57m 4s (- 5m 57s) (1811 90%) 0.4671\n",
      "57m 7s (- 5m 53s) (1813 90%) 0.4588\n",
      "57m 9s (- 5m 49s) (1815 90%) 0.4652\n",
      "57m 12s (- 5m 47s) (1816 90%) 0.4592\n",
      "57m 15s (- 5m 43s) (1818 90%) 0.4558\n",
      "57m 18s (- 5m 42s) (1819 90%) 0.4605\n",
      "57m 21s (- 5m 38s) (1821 91%) 0.4584\n",
      "57m 24s (- 5m 36s) (1822 91%) 0.4547\n",
      "57m 27s (- 5m 32s) (1824 91%) 0.4505\n",
      "57m 29s (- 5m 30s) (1825 91%) 0.4590\n",
      "57m 32s (- 5m 26s) (1827 91%) 0.4488\n",
      "57m 35s (- 5m 23s) (1829 91%) 0.4485\n",
      "57m 38s (- 5m 21s) (1830 91%) 0.4338\n",
      "57m 41s (- 5m 17s) (1832 91%) 0.4475\n",
      "57m 43s (- 5m 15s) (1833 91%) 0.4503\n",
      "57m 46s (- 5m 11s) (1835 91%) 0.4342\n",
      "57m 49s (- 5m 9s) (1836 91%) 0.4474\n",
      "57m 52s (- 5m 6s) (1838 91%) 0.4349\n",
      "57m 55s (- 5m 2s) (1840 92%) 0.4506\n",
      "57m 58s (- 5m 0s) (1841 92%) 0.4333\n",
      "58m 0s (- 4m 56s) (1843 92%) 0.4302\n",
      "58m 3s (- 4m 54s) (1844 92%) 0.4351\n",
      "58m 6s (- 4m 50s) (1846 92%) 0.4415\n",
      "58m 9s (- 4m 49s) (1847 92%) 0.4315\n",
      "58m 12s (- 4m 45s) (1849 92%) 0.4441\n",
      "58m 15s (- 4m 43s) (1850 92%) 0.4333\n",
      "58m 18s (- 4m 39s) (1852 92%) 0.4292\n",
      "58m 21s (- 4m 35s) (1854 92%) 0.4224\n",
      "58m 24s (- 4m 33s) (1855 92%) 0.4266\n",
      "58m 27s (- 4m 30s) (1857 92%) 0.4326\n",
      "58m 30s (- 4m 28s) (1858 92%) 0.4203\n",
      "58m 33s (- 4m 24s) (1860 93%) 0.4256\n",
      "58m 36s (- 4m 22s) (1861 93%) 0.4253\n",
      "58m 39s (- 4m 18s) (1863 93%) 0.4203\n",
      "58m 42s (- 4m 14s) (1865 93%) 0.4228\n",
      "58m 45s (- 4m 13s) (1866 93%) 0.4158\n",
      "58m 48s (- 4m 9s) (1868 93%) 0.4140\n",
      "58m 51s (- 4m 7s) (1869 93%) 0.4216\n",
      "58m 54s (- 4m 3s) (1871 93%) 0.4178\n",
      "58m 57s (- 4m 1s) (1872 93%) 0.4141\n",
      "58m 59s (- 3m 58s) (1874 93%) 0.4112\n",
      "59m 2s (- 3m 56s) (1875 93%) 0.4127\n",
      "59m 5s (- 3m 52s) (1877 93%) 0.4119\n",
      "59m 8s (- 3m 48s) (1879 93%) 0.4070\n",
      "59m 11s (- 3m 46s) (1880 94%) 0.4094\n",
      "59m 15s (- 3m 42s) (1882 94%) 0.4120\n",
      "59m 18s (- 3m 41s) (1883 94%) 0.3987\n",
      "59m 21s (- 3m 37s) (1885 94%) 0.4093\n",
      "59m 24s (- 3m 35s) (1886 94%) 0.3983\n",
      "59m 27s (- 3m 31s) (1888 94%) 0.4065\n",
      "59m 30s (- 3m 27s) (1890 94%) 0.3921\n",
      "59m 33s (- 3m 25s) (1891 94%) 0.4022\n",
      "59m 36s (- 3m 22s) (1893 94%) 0.4025\n",
      "59m 38s (- 3m 20s) (1894 94%) 0.4051\n",
      "59m 41s (- 3m 16s) (1896 94%) 0.4063\n",
      "59m 44s (- 3m 14s) (1897 94%) 0.3983\n",
      "59m 47s (- 3m 10s) (1899 94%) 0.3980\n",
      "59m 50s (- 3m 8s) (1900 95%) 0.3952\n",
      "59m 54s (- 3m 5s) (1902 95%) 0.3878\n",
      "59m 56s (- 3m 1s) (1904 95%) 0.3910\n",
      "59m 59s (- 2m 59s) (1905 95%) 0.3936\n",
      "60m 2s (- 2m 55s) (1907 95%) 0.3914\n",
      "60m 5s (- 2m 53s) (1908 95%) 0.3836\n",
      "60m 8s (- 2m 50s) (1910 95%) 0.4014\n",
      "60m 11s (- 2m 48s) (1911 95%) 0.3911\n",
      "60m 14s (- 2m 44s) (1913 95%) 0.3854\n",
      "60m 17s (- 2m 40s) (1915 95%) 0.3889\n",
      "60m 20s (- 2m 38s) (1916 95%) 0.3856\n",
      "60m 23s (- 2m 34s) (1918 95%) 0.3833\n",
      "60m 26s (- 2m 33s) (1919 95%) 0.3808\n",
      "60m 28s (- 2m 29s) (1921 96%) 0.3808\n",
      "60m 31s (- 2m 27s) (1922 96%) 0.3780\n",
      "60m 34s (- 2m 23s) (1924 96%) 0.3673\n",
      "60m 38s (- 2m 21s) (1925 96%) 0.3940\n",
      "60m 41s (- 2m 17s) (1927 96%) 0.3776\n",
      "60m 44s (- 2m 14s) (1929 96%) 0.3773\n",
      "60m 47s (- 2m 12s) (1930 96%) 0.3785\n",
      "60m 50s (- 2m 8s) (1932 96%) 0.3743\n",
      "60m 53s (- 2m 6s) (1933 96%) 0.3817\n",
      "60m 56s (- 2m 2s) (1935 96%) 0.3701\n",
      "60m 59s (- 2m 0s) (1936 96%) 0.3705\n",
      "61m 2s (- 1m 57s) (1938 96%) 0.3884\n",
      "61m 5s (- 1m 53s) (1940 97%) 0.3665\n",
      "61m 8s (- 1m 51s) (1941 97%) 0.3688\n",
      "61m 11s (- 1m 47s) (1943 97%) 0.3635\n",
      "61m 14s (- 1m 45s) (1944 97%) 0.3692\n",
      "61m 17s (- 1m 42s) (1946 97%) 0.3567\n",
      "61m 19s (- 1m 40s) (1947 97%) 0.3736\n",
      "61m 22s (- 1m 36s) (1949 97%) 0.3638\n",
      "61m 25s (- 1m 34s) (1950 97%) 0.3601\n",
      "61m 28s (- 1m 30s) (1952 97%) 0.3676\n",
      "61m 31s (- 1m 26s) (1954 97%) 0.3590\n",
      "61m 34s (- 1m 25s) (1955 97%) 0.3606\n",
      "61m 36s (- 1m 21s) (1957 97%) 0.3585\n",
      "61m 39s (- 1m 19s) (1958 97%) 0.3562\n",
      "61m 42s (- 1m 15s) (1960 98%) 0.3508\n",
      "61m 45s (- 1m 13s) (1961 98%) 0.3526\n",
      "61m 48s (- 1m 9s) (1963 98%) 0.3619\n",
      "61m 51s (- 1m 6s) (1965 98%) 0.3550\n",
      "61m 54s (- 1m 4s) (1966 98%) 0.3471\n",
      "61m 56s (- 1m 0s) (1968 98%) 0.3524\n",
      "61m 59s (- 0m 58s) (1969 98%) 0.3446\n",
      "62m 2s (- 0m 54s) (1971 98%) 0.3689\n",
      "62m 6s (- 0m 52s) (1972 98%) 0.3467\n",
      "62m 9s (- 0m 49s) (1974 98%) 0.3401\n",
      "62m 13s (- 0m 47s) (1975 98%) 0.3550\n",
      "62m 16s (- 0m 43s) (1977 98%) 0.3448\n",
      "62m 20s (- 0m 39s) (1979 98%) 0.3489\n",
      "62m 23s (- 0m 37s) (1980 99%) 0.3450\n",
      "62m 26s (- 0m 34s) (1982 99%) 0.3476\n",
      "62m 29s (- 0m 32s) (1983 99%) 0.3472\n",
      "62m 33s (- 0m 28s) (1985 99%) 0.3384\n",
      "62m 36s (- 0m 26s) (1986 99%) 0.3442\n",
      "62m 41s (- 0m 22s) (1988 99%) 0.3460\n",
      "62m 44s (- 0m 18s) (1990 99%) 0.3417\n",
      "62m 48s (- 0m 17s) (1991 99%) 0.3404\n",
      "62m 51s (- 0m 13s) (1993 99%) 0.3458\n",
      "62m 53s (- 0m 11s) (1994 99%) 0.3317\n",
      "62m 57s (- 0m 7s) (1996 99%) 0.3352\n",
      "63m 0s (- 0m 5s) (1997 99%) 0.3397\n",
      "63m 3s (- 0m 1s) (1999 99%) 0.3326\n",
      "63m 6s (- 0m 0s) (2000 100%) 0.3292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11126f080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD9CAYAAABHnDf0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12224b0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE=32\n",
    "hidden_size=256\n",
    "\n",
    "train_dataset = VocabDataset(pairs, input_lang.word2index, output_lang.word2index)\n",
    "# 1 batch input dimension: num_sentences x max sentence length\n",
    "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "encoder = EncoderRNN(hidden_size = hidden_size, vocab_size = input_lang.n_words )\n",
    "decoder = DecoderRNN(hidden_size = hidden_size, vocab_size = output_lang.n_words)\n",
    "\n",
    "plot_losses = trainIters(train_loader, encoder, decoder, n_iters=2000, \n",
    "                         print_every=50, plot_every=100, learning_rate=0.01, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dataset = VocabDataset(pairs_v, input_lang_v.word2index, output_lang_v.word2index)\n",
    "# 1 batch input dimension: num_sentences x max sentence length\n",
    "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Expected:  I was so shocked\n",
      "Actual:  And I did\n",
      "\n",
      "\n",
      "Expected:  I lost all hope\n",
      "Actual:  What do\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  Not very\n",
      "\n",
      "\n",
      "Expected:  Remi knows what love\n",
      "Actual:  You will know\n",
      "\n",
      "\n",
      "Expected:  He screamed a lot\n",
      "Actual:  I ve just that\n",
      "\n",
      "\n",
      "Expected:  Extraordinary\n",
      "Actual: \n",
      "\n",
      "\n",
      "Expected:  You can be extraordinary\n",
      "Actual:  We re very afraid\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  Right\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  This is\n",
      "\n",
      "\n",
      "Expected:  It s everywhere\n",
      "Actual:  So it s right\n",
      "\n",
      "\n",
      "Expected:  Not exactly\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  The answer is easy\n",
      "Actual:  We all you\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  Thank you very much\n",
      "Actual:  I was gasping\n",
      "\n",
      "\n",
      "Expected:  A real school\n",
      "Actual:  No one is\n",
      "\n",
      "\n",
      "Expected:  There she is\n",
      "Actual:  We will to it\n",
      "\n",
      "\n",
      "Expected:  Today I am 22\n",
      "Actual:  Does you see you\n",
      "\n",
      "\n",
      "Expected:  My family believes in\n",
      "Actual:  Hi\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  Thanks\n",
      "\n",
      "\n",
      "Expected:  Hold it up\n",
      "Actual:  He s not here\n",
      "\n",
      "\n",
      "Expected:  Raw data\n",
      "Actual:  What are they doing\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  No it didn t\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  Okay\n",
      "\n",
      "\n",
      "Expected:  And I was scared\n",
      "Actual:  And so much\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  That s the idea\n",
      "\n",
      "\n",
      "Expected:  All right\n",
      "Actual:  Thank\n",
      "\n",
      "\n",
      "Expected:  That was awkward\n",
      "Actual:  It s photophilic\n",
      "\n",
      "\n",
      "Expected:  And here s me\n",
      "Actual:  Thank you for your\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  This is not\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  This is not\n",
      "\n",
      "\n",
      "Expected:  Absolutely\n",
      "Actual:  Hey\n",
      "\n",
      "\n",
      "Expected:  Thanks Tom\n",
      "Actual:  I m serious this\n",
      "\n",
      "\n",
      "Expected:  So what happened\n",
      "Actual:  And And that s\n",
      "\n",
      "\n",
      "Expected:  We are the soil\n",
      "Actual:  Thank you so much\n",
      "\n",
      "\n",
      "Expected:  That s the whole\n",
      "Actual:  A very simple definition\n",
      "\n",
      "\n",
      "Expected:  Peace Thank you\n",
      "Actual:  This is Mahler\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  They kill our livestock\n",
      "Actual:  The water is completely\n",
      "\n",
      "\n",
      "Expected:  So they kill the\n",
      "Actual:  But he s okay\n",
      "\n",
      "\n",
      "Expected:  But lions are very\n",
      "Actual:  But just just\n",
      "\n",
      "\n",
      "Expected:  So I set up\n",
      "Actual:  I know I think\n",
      "\n",
      "\n",
      "Expected:  Thanks\n",
      "Actual:  That\n",
      "\n",
      "\n",
      "Expected:  And I was distraught\n",
      "Actual:  Thank you so much\n",
      "\n",
      "\n",
      "Expected:  Government Forget about it\n",
      "Actual:  They re\n",
      "\n",
      "\n",
      "Expected:  Guess what\n",
      "Actual:  So less waste\n",
      "\n",
      "\n",
      "Expected:  It doesn t exist\n",
      "Actual:  Okay so fine\n",
      "\n",
      "\n",
      "Expected:  We activate communities\n",
      "Actual:  It s never been\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  Thanks\n",
      "\n",
      "\n",
      "Expected:  This was the very\n",
      "Actual:  Thank you you eating\n",
      "\n",
      "\n",
      "Expected:  It could take months\n",
      "Actual:  Steaming piles s right\n",
      "\n",
      "\n",
      "Expected:  It was very time-consuming\n",
      "Actual:  We all all different\n",
      "\n",
      "\n",
      "Expected:  We take photos constantly\n",
      "Actual:  Just here it is\n",
      "\n",
      "\n",
      "Expected:  Thank you\n",
      "Actual:  This is not\n",
      "\n",
      "\n",
      "Expected:  There were no back\n",
      "Actual:  That s the\n",
      "\n",
      "\n",
      "Expected:  This is a family\n",
      "Actual:  Thank you you believe\n",
      "\n",
      "\n",
      "Expected:  Many of them drown\n",
      "Actual:  We have that it\n",
      "\n",
      "\n",
      "Expected:  Thank you very much\n",
      "Actual:  I was gasping\n"
     ]
    }
   ],
   "source": [
    "decoded, actual = evaluate_batch(val_loader, encoder, decoder)\n",
    "\n",
    "for i in zip(decoded, actual):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print('\\n')\n",
    "    print('Expected:', i[1])\n",
    "    print('Actual:' ,i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6576123776265187"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(decoded, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
