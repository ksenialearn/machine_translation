{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chinese RNN w attention w pre-trained embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GFHdAEFQtTRG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install torch && Scarbleu"
      ]
    },
    {
      "metadata": {
        "id": "xYlFlWAPnBwG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reference: \n",
        "https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation"
      ]
    },
    {
      "metadata": {
        "id": "YJl1CJvlt5gG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CyJtHlQkGUJL",
        "colab_type": "code",
        "outputId": "775781e4-6d6a-4e0f-9f3f-6e3180db81a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install sacrebleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/37/51/bffea2b666d59d77be0413d35220022040a1f308c39009e5b023bc4eb8ab/sacrebleu-1.2.12.tar.gz\n",
            "Collecting typing (from sacrebleu)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl\n",
            "Building wheels for collected packages: sacrebleu\n",
            "  Running setup.py bdist_wheel for sacrebleu ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/0a/7d/ddcbdcd15a04b72de1b3f78e7e754aab415aff81c423376385\n",
            "Successfully built sacrebleu\n",
            "Installing collected packages: typing, sacrebleu\n",
            "Successfully installed sacrebleu-1.2.12 typing-3.6.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xAQHOkBolXXY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Pacgages"
      ]
    },
    {
      "metadata": {
        "id": "gOAe8qRguGdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sacrebleu import corpus_bleu\n",
        "from collections import Counter\n",
        "import pickle as pkl\n",
        "import random\n",
        "import pdb\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "#specify SOS() and EOS(end of sentence)\n",
        "#specify maximum vocabulary size = 50000\n",
        "PAD_IDX = 2\n",
        "UNK_IDX = 3\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_VOCAB_SIZE = 500000\n",
        "MAX_LENGTH = 30\n",
        "\n",
        "train_en = 'data/train.tok.en'\n",
        "train_zh = 'data/train.tok.zh'\n",
        "val_en = 'data/dev.tok.en'\n",
        "val_zh = 'data/dev.tok.zh'\n",
        "\n",
        "Token_list = [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PsEuCzYYuMoR",
        "colab_type": "code",
        "outputId": "0b38d774-9744-4fb9-f7bf-0c8684307523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#user GPU if possible\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "  print(\"Currently using GPU\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FntyL6ovSDY",
        "colab_type": "code",
        "outputId": "a1558eba-2c30-40ed-909a-bc579132b05a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5vc0D8zWxu_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Embedding"
      ]
    },
    {
      "metadata": {
        "id": "6FZwLxswviX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "folder_path = os.getcwd() + '/gdrive/My Drive/NLP_Project/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrucOX2bsLUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"<SOS>\":0, \"<EOS>\":1, \"<PAD>\":2, \"<UNK>\":3}\n",
        "        #self.word2count = {\"<SOS>\":0, \"<EOS>\":0, \"<PAD>\":0, \"<UNK>\":0}\n",
        "        self.word2count = {}\n",
        "        #self.word2count = {\"<SOS>\":0, \"<EOS>\":0, \"<PAD>\":0, \"<UNK>\":0}\n",
        "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
        "        self.n_words = 4  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            if (word!=\"\") and (word !=\" \"):\n",
        "                self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "      if (word not in Token_list):\n",
        "        if word not in self.word2index:\n",
        "            \n",
        "          self.word2index[word] = self.n_words\n",
        "          self.word2count[word] = 1\n",
        "          self.index2word[self.n_words] = word\n",
        "          self.n_words += 1\n",
        "        else:\n",
        "          self.word2count[word] += 1\n",
        "    def delWord(self, word):\n",
        "        if (word in self.word2index.keys()):\n",
        "            index = self.word2index[word]\n",
        "            del self.word2index[word]\n",
        "            del self.index2word[index] \n",
        "            self.n_words = self.n_words-1\n",
        "        else:\n",
        "          print (\"Word Not Existed\")\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(address_lang1, address_lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines_lang1 = open(folder_path+address_lang1, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    lines_lang2 = open(folder_path+address_lang2, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    \n",
        "    assert (len(lines_lang1)==len(lines_lang2))\n",
        "    # Split every line into pairs and normalize\n",
        "    \n",
        "    pairs = [[lines_lang1[i], normalizeString(lines_lang2[i])] for i in range (len(lines_lang1))]\n",
        "    #print (pairs[-1])\n",
        "    # Reverse pairs, make Lang instances\n",
        "    lang1=address_lang1[-2:]\n",
        "    lang2=address_lang2[-2:]\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def prepareData(address_lang1, address_lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(address_lang1, address_lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGFkxsahuPPT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_emb_matrix(language):\n",
        "    #load fasttext word vectors\n",
        "    words_to_load = MAX_VOCAB_SIZE\n",
        "    if language == 'english':\n",
        "      file = 'wiki-news-300d-1M-subword.vec'\n",
        "    if language == 'chinese':\n",
        "      file = 'cc.zh.300.vec'\n",
        "    \n",
        "\n",
        "    with open(folder_path + 'data/' + file) as f:\n",
        "        #remove the first line\n",
        "        firstLine = f.readline()\n",
        "        loaded_embeddings = np.zeros((words_to_load + 4, 300))\n",
        "        words2id = {}\n",
        "        idx2words = {}\n",
        "        ordered_words = []\n",
        "\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= words_to_load: \n",
        "                break\n",
        "            s = line.split()\n",
        "            loaded_embeddings[i , :] = np.asarray(s[1:])           \n",
        "            words2id[s[0]] = i \n",
        "            idx2words[i] = s[0]\n",
        "    words2id['<SOS>'] = SOS_token\n",
        "    words2id['<EOS>'] = EOS_token\n",
        "    words2id['<PAD>'] = PAD_IDX\n",
        "    words2id['<UNK>'] = UNK_IDX\n",
        "    idx2words[0] = '<SOS>'\n",
        "    idx2words[1] = '<EOS>'\n",
        "    idx2words[2] = '<PAD>'\n",
        "    idx2words[3] = '<UNK>'\n",
        "    return words2id,idx2words,loaded_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxBmZe6Yucnt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_weights_matrix(index2word_lang, word2index_lang, index2word_embed, word2index_embed, loaded_embeddings):\n",
        "    emb_dim=300\n",
        "    missing_count=0\n",
        "    matrix_len = len(index2word_lang)\n",
        "    weights_matrix = np.zeros((matrix_len, 300))\n",
        "    \n",
        "    for key in index2word_lang.keys():\n",
        "        word=index2word_lang[key]\n",
        "        if (word in word2index_embed.keys()):\n",
        "          weights_matrix[key] = loaded_embeddings[word2index_embed[word]]\n",
        "        else:\n",
        "          missing_count=missing_count+1\n",
        "          weights_matrix[key] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
        "    print (\"missing count: \", missing_count)\n",
        "    return weights_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJR9haZ2mfnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data "
      ]
    },
    {
      "metadata": {
        "id": "DV4_a8tZmelN",
        "colab_type": "code",
        "outputId": "2a5e0485-0750-45a6-99e5-3770ae83b410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "train_input_lang, train_output_lang, train_pairs = prepareData(train_zh, train_en, reverse=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 213376 sentence pairs\n",
            "Trimmed to 150216 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "zh 64772\n",
            "en 37140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J5yQakjdn8cA",
        "colab_type": "code",
        "outputId": "16302c47-664d-41d7-8aa7-3bba0518828b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "for word, count in train_input_lang.word2count.items():\n",
        "  if word not in Token_list:\n",
        "    if count<4:\n",
        "      train_input_lang.delWord(word)\n",
        "print (\"train_input_lang lenght: \", len(train_input_lang.word2index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_input_lang lenght:  24622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xJHmUotiq4Sc",
        "colab_type": "code",
        "outputId": "d22f8381-ab1a-4420-d428-61b6dc7596d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "for word, count in train_output_lang.word2count.items():\n",
        "  if word not in Token_list:\n",
        "    if count<4:\n",
        "      train_output_lang.delWord(word)\n",
        "print (\"train_output_lang lenght: \", len(train_output_lang.word2index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_output_lang lenght:  14663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LJMR3Kyf6Umo",
        "colab_type": "code",
        "outputId": "da43c1bf-e53f-4b44-da18-7fb382051d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_input_lang_new=Lang(\"zh\")\n",
        "for word in train_input_lang.word2index.keys():\n",
        "  train_input_lang_new.addWord(word)\n",
        "print (\"train_input_lang lenght: \", len(train_input_lang_new.word2index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_input_lang lenght:  24622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OtxKeyrW8a-h",
        "colab_type": "code",
        "outputId": "dc41f7ed-da58-4f4f-c71d-bef8e7aee142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_output_lang_new=Lang(\"en\")\n",
        "for word in train_output_lang.word2index.keys():\n",
        "  train_output_lang_new.addWord(word)\n",
        "print (\"train_input_lang lenght: \", len(train_output_lang_new.word2index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_input_lang lenght:  14663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BhSFYCqupfjq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# words2id_eng,idx2words_eng,loaded_embeddings_eng = load_emb_matrix('english')\n",
        "# words2id_zh,idx2words_zh,loaded_embeddings_zh = load_emb_matrix('chinese')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pU_sL88Ad3wu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pkl.dump(words2id_eng, open(folder_path + 'data/words2id_eng_0.5M.pkl', 'wb'))\n",
        "# pkl.dump(idx2words_eng, open(folder_path +'data/idx2words_eng_0.5M.pkl', 'wb'))\n",
        "# pkl.dump(loaded_embeddings_eng, open(folder_path +'data/embedding_matrix_eng_0.5M.pkl', 'wb'))\n",
        "\n",
        "# pkl.dump(words2id_zh, open(folder_path + 'data/words2id_zh_0.5M.pkl', 'wb'))\n",
        "# pkl.dump(idx2words_zh, open(folder_path + 'data/idx2words_zh_0.5M.pkl', 'wb'))\n",
        "# pkl.dump(loaded_embeddings_zh, open(folder_path +'data/embedding_matrix_zh_0.5M.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wsLLWQWe3rQe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words2id_eng=pkl.load(open(folder_path + 'data/words2id_eng_0.5M.pkl', 'rb'))\n",
        "idx2words_eng=pkl.load(open(folder_path +'data/idx2words_eng_0.5M.pkl', 'rb'))\n",
        "loaded_embeddings_eng=pkl.load(open(folder_path +'data/embedding_matrix_eng_0.5M.pkl', 'rb'))\n",
        "\n",
        "words2id_zh=pkl.load(open(folder_path + 'data/words2id_zh_0.5M.pkl', 'rb'))\n",
        "idx2words_zh=pkl.load(open(folder_path + 'data/idx2words_zh_0.5M.pkl', 'rb'))\n",
        "loaded_embeddings_zh=pkl.load(open(folder_path +'data/embedding_matrix_zh_0.5M.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OL62QCS436Mj",
        "colab_type": "code",
        "outputId": "7679bd41-4acd-4d59-e785-7a68f86ddfd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "weights_matrix_eng=generate_weights_matrix(train_output_lang_new.index2word, train_output_lang_new.word2index, idx2words_eng, words2id_eng, loaded_embeddings_eng)\n",
        "weights_matrix_eng = torch.from_numpy(weights_matrix_eng).to(device)\n",
        "pkl.dump(weights_matrix_eng, open(folder_path +'data/weights_matrix_eng_1M_final.pkl', 'wb'))\n",
        "\n",
        "\n",
        "weights_matrix_zh=generate_weights_matrix(train_input_lang_new.index2word, train_input_lang_new.word2index, idx2words_zh, words2id_zh, loaded_embeddings_zh) \n",
        "weights_matrix_zh = torch.from_numpy(weights_matrix_zh).to(device)\n",
        "pkl.dump(weights_matrix_zh, open(folder_path +'data/weights_matrix_zh_1M_final.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missing count:  473\n",
            "missing count:  2969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gkvehO5eNxt6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# weights_matrix_eng=generate_weights_matrix(train_output_lang.index2word, train_output_lang.word2index, idx2words_eng, words2id_eng, loaded_embeddings_eng)\n",
        "# weights_matrix_eng = torch.from_numpy(weights_matrix_eng).to(device)\n",
        "# pkl.dump(weights_matrix_eng, open(folder_path +'data/weights_matrix_eng_1M_final.pkl', 'wb'))\n",
        "\n",
        "\n",
        "# weights_matrix_zh=generate_weights_matrix(train_input_lang.index2word, train_input_lang.word2index, idx2words_zh, words2id_zh, loaded_embeddings_zh) \n",
        "# weights_matrix_zh = torch.from_numpy(weights_matrix_zh).to(device)\n",
        "# pkl.dump(weights_matrix_zh, open(folder_path +'data/weights_matrix_zh_1M_final.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ozVXC-QyNPR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ]
    },
    {
      "metadata": {
        "id": "6C4MqQ0dzHzj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    sentence=sentence.strip()\n",
        "    \n",
        "    result = []\n",
        "    for word in sentence.split(\" \"):\n",
        "      if (word!=\"\") and (word!=\" \"):\n",
        "        if word in lang.word2index:\n",
        "          result.append(lang.word2index[word])\n",
        "        else:\n",
        "          result.append(UNK_IDX)\n",
        "    result.append(EOS_token)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLz7Ne86_Op7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VocabDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pairs,input_language, output_language):\n",
        "        \"\"\"\n",
        "        @param pairs: pairs of input and target sentences(raw text sentences)\n",
        "        @param input_language: Class Lang of input languages (zh in this case)\n",
        "        @param output_language: Class Lang of output languages (en in this case)\n",
        "\n",
        "        \"\"\"\n",
        "        self.pairs = pairs\n",
        "        self.inputs = [pair[0] for pair in pairs]\n",
        "        self.input_lang = input_language\n",
        "        self.output_lang = output_language\n",
        "        self.outputs = [pair[1] for pair in pairs]\n",
        "        \n",
        "        \n",
        "        #assert self.input_lang == self.target_lang\n",
        "       \n",
        "    def __len__(self):\n",
        "         return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        #turn raw text sentecens into indices\n",
        "        input_ = indexesFromSentence(self.input_lang, self.inputs[key])\n",
        "        output = indexesFromSentence(self.output_lang, self.outputs[key])\n",
        "        #print (output)\n",
        "        #print both the length of the source sequence and the target sequence\n",
        "        return [input_,len(input_),output,len(output)]\n",
        "    \n",
        "    \n",
        "    def __gettext__(self,key):\n",
        "      return [self.inputs[key],self.outputs[key]]\n",
        "\n",
        "def vocab_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all\n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    input_data_list = []\n",
        "    output_data_list = []\n",
        "   \n",
        "    \n",
        "    for datum in batch:\n",
        "      input_data_list.append(datum[0])\n",
        "      output_data_list.append(datum[2])\n",
        "      \n",
        "      \n",
        "    # Zip into pairs, sort by length (descending), unzip\n",
        "    seq_pairs = sorted(zip(input_data_list, output_data_list), key=lambda p: len(p[0]), reverse=True)\n",
        "    input_seqs, output_seqs = zip(*seq_pairs)\n",
        "    \n",
        "    #store the length of the sequences \n",
        "    input_data_len = [len(p) for p in input_seqs]\n",
        "    output_data_len = [len(p) for p in output_seqs]\n",
        "    #MAX_LENGTH=max\n",
        "    #padding\n",
        "    padded_vec_input = [np.pad(np.array(p),\n",
        "                                 pad_width=((0,max(input_data_len)-len(p))),\n",
        "                                 mode=\"constant\", constant_values=PAD_IDX) for p in input_seqs]\n",
        "        \n",
        "    padded_vec_output = [np.pad(np.array(p),\n",
        "                                 pad_width=((0,max(output_data_len)-len(p))),\n",
        "                                 mode=\"constant\", constant_values=PAD_IDX) for p in output_seqs]      \n",
        "    \n",
        "    \n",
        "    input_var = Variable(torch.LongTensor(padded_vec_input))\n",
        "    output_var = Variable(torch.LongTensor(padded_vec_output))\n",
        "    input_data_len = Variable(torch.LongTensor(input_data_len))\n",
        "    output_data_len = Variable(torch.LongTensor(output_data_len))\n",
        "    \n",
        "    \n",
        "    return [input_var,input_data_len,output_var,output_data_len]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDJsqrFayR4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ]
    },
    {
      "metadata": {
        "id": "Ac8yy73v_QiC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, weights_matrix, input_size, hidden_size,n_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "     \n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.n_layers = n_layers\n",
        "        #self.batch_size = BATCH_SIZE\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "        \n",
        "        self.embedding_dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "        self.embedding.from_pretrained(weights_matrix, freeze=True, sparse=False)\n",
        "        #self.embedding.weight.requires_grad = True\n",
        "\n",
        "        \n",
        "        self.gru = nn.GRU(self.embedding_dim, hidden_size, n_layers, bidirectional=True)\n",
        "        \n",
        "\n",
        "    def forward(self, input_seqs, input_len, hidden=None):\n",
        "        \n",
        "        self.batch_size = input_seqs.size()[0]\n",
        "       \n",
        "        embedded = self.embedding(input_seqs)\n",
        "#         print(\"input\",input_seqs.shape)\n",
        "#         print (\"embeded\", embedded.shape)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_len)\n",
        "        output, hidden = self.gru(packed, hidden)\n",
        "\n",
        "        output, output_len = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
        "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
        "        \n",
        "        return output,hidden\n",
        "    \n",
        "#     def random_init_hidden(self, batch_size):\n",
        "#         hidden = torch.zeros(n_layers*2, batch_size, self.hidden_size, device=device)\n",
        "#         nn.init.xavier_normal_(hidden)\n",
        "#         return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JXSMSJUHyVMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ]
    },
    {
      "metadata": {
        "id": "uxYk3UJQ_W8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        \n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
        "     \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "#       print (\"Attn Forward\")\n",
        "#       print (\"hidden\", hidden.shape)\n",
        "#       print (\"encoder_outputs\", encoder_outputs.shape)\n",
        "      attn_energies = self.score(hidden, encoder_outputs).squeeze(2)\n",
        "#       print (\"attn_energies\", attn_energies.shape)\n",
        "      #score = F.softmax(attn_energies, dim = 1).view(1, self.batch_size, -1)\n",
        "      \n",
        "      \n",
        "      return F.softmax(attn_energies).unsqueeze(1)\n",
        "\n",
        "    def score(self, hidden, encoder_output):\n",
        "#       print (\"hidden\", hidden.shape)\n",
        "#       print (\"encoder_output\", encoder_output.shape)\n",
        "      '''\n",
        "      Args\n",
        "          hidden: size 1 x B x hidden_size\n",
        "          encoder_output: size N x B x hidden_size\n",
        "      Return \n",
        "          energy: size B x N x 1\n",
        "      '''\n",
        "#       print (\"score\")\n",
        "#       print (\"hidden\", hidden.shape)\n",
        "#       print (\"encoder_outputs\", encoder_output.shape)\n",
        "#       print (\"hidden_change\", hidden.squeeze(0).unsqueeze(2).shape)\n",
        "      self.batch_size = hidden.shape[1]\n",
        "      if self.method == 'dot':\n",
        "          energy = torch.bmm(encoder_output.transpose(1,0), hidden.squeeze(0).unsqueeze(2)) \n",
        "#           print (\"energy\", energy.shape)\n",
        "          return energy \n",
        "\n",
        "      elif self.method == 'general':\n",
        "          energy = torch.bmm(encoder_output.transpose(1,0), self.attn(hidden.squeeze(0)).unsqueeze(2)) \n",
        "          #print (\"energy\", energy.shape)\n",
        "          return energy\n",
        "\n",
        "      elif self.method == 'concat':\n",
        "          concat = torch.cat((hidden.transpose(1,0).expand(self.batch_size ,encoder_output.shape[0], self.hidden_size), encoder_output.transpose(1,0)), dim = 2)\n",
        "          #print(\"concat\", concat)\n",
        "          tanh = nn.Tanh()\n",
        "          out = tanh(self.attn(concat)) #size: B x N x hidden_size\n",
        "          #print (\"out\", out)\n",
        "          #print (\"v\", self.v.expand(self.batch_size, 1, self.hidden_size))\n",
        "          energy = torch.bmm(self.v.expand(self.batch_size, 1, self.hidden_size), out.transpose(2,1)).transpose(1,2)\n",
        "          #print (\"energy\", energy)\n",
        "#           print (\"energy\", energy.shape)\n",
        "          return energy\n",
        "\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, weights_matrix, hidden_size, output_size, drop_out, n_layers=1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_out=drop_out\n",
        "        #self.batch_size = BATCH_SIZE\n",
        "        self.num_embeddings, self.embedding_dim = weights_matrix.size()\n",
        "\n",
        "\n",
        "        # Define layers\n",
        "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.embedding = nn.Embedding(self.num_embeddings,self.hidden_size)\n",
        "        \n",
        "#         self.embedding.weight.data.copy_(weights_matrix)\n",
        "#         self.embedding.weight.requires_grad = True\n",
        "        self.embedding.from_pretrained(weights_matrix, freeze=True, sparse=False)\n",
        "        \n",
        "        self.embedding_dropout = nn.Dropout(drop_out)\n",
        "      \n",
        "        #self.gru1 = nn.GRU(self.embedding_dim, hidden_size, n_layers)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=drop_out)\n",
        "       # self.gru2 = nn.GRU(hidden_size, hidden_size,n_layers)\n",
        "        \n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # Choose attention model\n",
        "        if attn_model != 'none':\n",
        "            self.attn = Attn(attn_model, hidden_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step at a time\n",
        "#         print (\"decoder:forward\")\n",
        "        # Get the embedding of the current input word (last output word)\n",
        "#         print(\"input_seq\", input_seq.shape)\n",
        "#         print(\"last_hidden\", last_hidden.shape)\n",
        "#         print(\"encoder_outputs\", encoder_outputs.shape)\n",
        "        self.batch_size=input_seq.size(0)\n",
        "        #print (self.batch_size)\n",
        "        embedded = self.embedding(input_seq) # dim = Batch_Size x embedding_dim\n",
        "#         print (\"input_seq\", input_seq)\n",
        "        #print(\"embedding\", embedded.shape)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        #print(\"embedding\", embedded.shape)\n",
        "#         print(\"embedding\", embedded.shape)\n",
        "        embedded = embedded.view(1, self.batch_size, self.hidden_size)\n",
        "        #print(\"embedding\", embedded.shape)\n",
        "  # S=1 x Batch_Size x embedding_dim\n",
        "#         print(\"embedding\", embedded.shape)\n",
        "        # Get current hidden state from input word and last hidden state\n",
        "        # rnn_output : [1 x batch_size x hidden_size]\n",
        "        # hidden: [layer x batch_size x hidden_size]\n",
        "#         print (embedded.shape)\n",
        "#         print (last_hidden.shape)\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "#         print(\"rnn_output\", rnn_output)\n",
        "        #print(\"hidden\", hidden.shape)\n",
        "        #print(\"rnn_output\", rnn_output.shape)\n",
        "#         print(\"hidden\", hidden.shape)\n",
        "        # Calculate attention from current RNN state and all encoder outputs;\n",
        "        # apply to encoder outputs to get weighted average\n",
        "        #context, attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        \n",
        "        attn_weights = self.attn(hidden[0].view(1, self.batch_size, self.hidden_size), encoder_outputs)\n",
        "#         print(\"attn_weights\", attn_weights)\n",
        "        #print(\"attn_weights\", attn_weights.shape)\n",
        "        \n",
        " \n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        #print (\"context\", context.shape)\n",
        "        #print (\"attn_weights\", attn_weights.shape)\n",
        "        \n",
        "#         attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "#         context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
        "\n",
        "        # Attentional vector using the RNN hidden state and context vector\n",
        "        # concatenated together (Luong eq. 5)\n",
        "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        #print (\"rnn_output\", rnn_output.shape)\n",
        "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
        "        #print (\"context\", context.shape)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        #print (\"concat_input\", concat_input.shape)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        #print (\"concat_output\", concat_output.shape)\n",
        "\n",
        "        # Finally predict next token (Luong eq. 6, without softmax)\n",
        "        output = self.out(concat_output)\n",
        "        #print (\"output\", output.shape)\n",
        "        #output = self.softmax(output)\n",
        "#         print (\"output\", output.shape)\n",
        "        #Return final output, hidden state, and attention weights (for visualization)\n",
        "        return output, hidden, attn_weights\n",
        "        #return attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bct0uwzoybjL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ]
    },
    {
      "metadata": {
        "id": "fR4JgAnI_by8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#record the run time\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wkWNQq3E_dGY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "82n4HDUwygzV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss function"
      ]
    },
    {
      "metadata": {
        "id": "_JfKJPtxVHQ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    batch_size = sequence_length.size(0)\n",
        "    seq_range = torch.range(0, max_len - 1).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
        "    seq_range_expand = Variable(seq_range_expand)\n",
        "    if sequence_length.is_cuda:\n",
        "        seq_range_expand = seq_range_expand.cuda()\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    return seq_range_expand < seq_length_expand\n",
        "\n",
        "\n",
        "def masked_cross_entropy(logits, target, length):\n",
        "    length = Variable(torch.LongTensor(length)).to(device)\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "    \"\"\"\n",
        "#     print (\"length\", length.size())\n",
        "    # logits_flat: (batch * max_len, num_classes)\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # log_probs_flat: (batch * max_len, num_classes)\n",
        "    log_probs_flat = F.log_softmax(logits_flat)\n",
        "    # target_flat: (batch * max_len, 1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    # losses_flat: (batch * max_len, 1)\n",
        "#     print (\"log_probs_flat\", log_probs_flat.shape)\n",
        "#     print (\"target_flat\", target_flat.shape)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    # losses: (batch, max_len)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    # mask: (batch, max_len)\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / length.float().sum()\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzgAAl74_ko5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the train function is now taking a batch at a time\n",
        "def train(input_batch, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, \n",
        "          decoder_optimizer, criteron, max_length=MAX_LENGTH, if_attention = True):\n",
        "    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "#     print (\"input_batch\", input_batch.size())\n",
        "#     print (\"target_batches\", target_batches.size()) \n",
        "    input_length, batch_size = input_batch.size()\n",
        "    #batch_size_2, target_length = output_batch.size()\n",
        "    \n",
        "    loss =0\n",
        "#     print(\"input_batch\", input_batch)\n",
        "#     print(\"input_lengths\", input_lengths)\n",
        "    encoder_outputs, encoder_hidden = encoder(input_batch, input_lengths)\n",
        "#     print(\"encoder_outputs\", encoder_outputs)\n",
        "#     print(\"encoder_hidden\", encoder_hidden)\n",
        "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
        "    \n",
        "    max_target_length = target_lengths.max().item()\n",
        "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
        "    \n",
        "    decoder_input = decoder_input.to(device)\n",
        "    all_decoder_outputs = all_decoder_outputs.to(device)\n",
        "    \n",
        "#     print (\"decoder_input\", decoder_input)\n",
        "#     print (\"decoder_hidden\", decoder_hidden)\n",
        "#     print (\"encoder_outputs\", encoder_outputs)\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    if use_teacher_forcing:\n",
        "      for t in range(max_target_length):\n",
        "#           print(\"decoder_input\", decoder_input.shape)\n",
        "#           print(\"decoder_hidden\", decoder_hidden.shape)\n",
        "#           print(\"encoder_outputs\", encoder_outputs.shape)\n",
        "          decoder_output, decoder_hidden, decoder_attn = decoder(\n",
        "              decoder_input, decoder_hidden, encoder_outputs\n",
        "          )\n",
        "#           print (\"decoder_output\", decoder_output)\n",
        "#           print (\"decoder_hidden\", decoder_hidden)\n",
        "#           print (\"decoder_attn\", decoder_attn)  \n",
        "          all_decoder_outputs[t] = decoder_output\n",
        "          #loss += criteron(decoder_output, target_batches[t])\n",
        "          decoder_input = target_batches[t] # Next input is current target\n",
        "    else:\n",
        "      for t in range(max_target_length):\n",
        "#           print(\"decoder_input\", decoder_input.shape)\n",
        "#           print(\"decoder_hidden\", decoder_hidden.shape)\n",
        "#           print(\"encoder_outputs\", encoder_outputs.shape)\n",
        "          decoder_output, decoder_hidden, decoder_attn = decoder(\n",
        "              decoder_input, decoder_hidden, encoder_outputs\n",
        "          )\n",
        "          \n",
        "          topv, topi = decoder_output.topk(1)\n",
        "          decoder_input = topi.squeeze().detach()\n",
        "          \n",
        "          all_decoder_outputs[t] = decoder_output\n",
        "          decoder_input = decoder_input.unsqueeze(0)\n",
        "\n",
        "    # Loss calculation and backpropagation\n",
        "#     1: torch.Size([32, 39, 2840])\n",
        "#     2: torch.Size([32, 50])\n",
        "#     print (\"1:\", all_decoder_outputs.transpose(0, 1).contiguous().shape)\n",
        "#     print (\"2:\", target_batches.transpose(0, 1).contiguous().shape)\n",
        "    loss = masked_cross_entropy(\n",
        "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
        "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
        "        target_lengths.to(\"cpu\")\n",
        "    )\n",
        "    \n",
        "    #print (\"all_decoder_outputs\", all_decoder_outputs.shape)\n",
        "    #print (\"target_batches\", target_batches.shape)\n",
        "    #loss= criterion(all_decoder_outputs.transpose(1, 2), target_batches)\n",
        "    #print (loss)\n",
        "    \n",
        "#     print (\"all_decoder_outputs\", all_decoder_outputs.transpose(0, 1).contiguous())\n",
        "#     print (\"target_batches\", target_batches.transpose(0, 1).contiguous())\n",
        "#     print (\"target_lengths\", target_lengths)\n",
        "#     print(\"loss\", loss)\n",
        "    loss.backward()\n",
        "    \n",
        "    # Clip gradient norms\n",
        "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
        "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
        "\n",
        "    # Update parameters with optimizers\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "#     print (loss.item())\n",
        "    return loss.item(), ec, dc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfmYR1ZjIvk0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "t475QwOnIxsM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_output_lang(output_list, output_lang):\n",
        "  result=[]\n",
        "  for token_index in output_list:\n",
        "    token=output_lang.index2word[token_index]\n",
        "    result.append(token)\n",
        "  return result    \n",
        "def evaluate(encoder, decoder, sentence, input_lengths, translated, search='greedy', max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Function that generate translation.\n",
        "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
        "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
        "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
        "    And collect the attention for each output words.\n",
        "    @param encoder: the encoder network\n",
        "    @param decoder: the decoder network\n",
        "    @param sentence: string, a sentence in source language to be translated\n",
        "    @param max_length: the max # of words that the decoder can return\n",
        "    @output decoded_words: a list of words in target language\n",
        "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
        "    \"\"\"    \n",
        "    # process input sentence\n",
        "    with torch.no_grad():\n",
        "        input_tensor = sentence.transpose(0,1).to(device)\n",
        "        #input_length = sentence.size()[0]\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor, [input_lengths.item()], None)\n",
        "\n",
        "        decoder_input = Variable(torch.LongTensor([SOS_token])).to(device) # SOS\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder \n",
        "        # output of this function\n",
        "        decoded_words = ''\n",
        "\n",
        "        for di in range(max_length):\n",
        "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
        "            decoder_output, decoder_hidden, attn_weight = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_output)\n",
        "            \n",
        "            \n",
        "            # GREEDY\n",
        "            topv, topi = decoder_output.data.topk(1) \n",
        "\n",
        "            if topi.item() == EOS_token:\n",
        "                #decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                if topi.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
        "                    decoded_words = decoded_words + ' ' + train_output_lang_new.index2word[topi.item()]\n",
        "            \n",
        "            decoder_input = topi[0].detach()\n",
        "        \n",
        "        translation = ''\n",
        "        for i in translated: #expected translation\n",
        "            if i.item() not in [SOS_token, EOS_token, UNK_IDX, PAD_IDX]:\n",
        "                translation = translation + ' ' + train_output_lang_new.index2word[i.item()]\n",
        "\n",
        "        return decoded_words, translation\n",
        "\n",
        "def evaluate_batch(loader, encoder, decoder):\n",
        "    \n",
        "    decoded_sentences = []\n",
        "    actual_sentences = []\n",
        "    \n",
        "    for i, (source, lengths1, target, lengths2) in enumerate(loader):\n",
        "        #iterate over batch\n",
        "        \n",
        "        for n in range(len(source)):\n",
        "            # Go sentence by sentence\n",
        "            \n",
        "            decoded, actual = evaluate(encoder, decoder, source[n].unsqueeze(0), lengths1[n], target[n])\n",
        "            decoded_sentences.append(decoded)\n",
        "            actual_sentences.append(actual)\n",
        "            \n",
        "    return decoded_sentences, actual_sentences\n",
        "\n",
        "\n",
        "def evaluate_bleu(translation_list, reference_list):\n",
        "    \n",
        "    return corpus_bleu(translation_list, [reference_list])      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hsS3e40AIzzR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Functions"
      ]
    },
    {
      "metadata": {
        "id": "ViJaygPT_mY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(Epoch, iters,  encoder, decoder, encoder_optimizer, decoder_optimizer, criteron, n_iters, train_loader, loss_list, print_every=1000, plot_every=100):\n",
        "    bleu_score=0\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    loss_list=[]\n",
        "    Iter_loss_list=[]\n",
        "#     encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "#     decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "#    criterion = nn.NLLLoss()\n",
        "    #iters = 0\n",
        "    loss_iter = 0\n",
        "    while iters <= n_iters:\n",
        "      Epoch +=1\n",
        "      print(\"Epoch:\", Epoch)\n",
        "      num_batch=0\n",
        "      loss_total=0\n",
        "      \n",
        "      for i, (input_var,input_data_len,output_var,output_data_len) in enumerate(train_loader):\n",
        "       \n",
        "        iters += 1\n",
        "        \n",
        "        input_batch = input_var.transpose(0,1).to(device)\n",
        "        output_batch = output_var.transpose(0,1).to(device)\n",
        "        input_data_len=input_data_len.to(device)\n",
        "        output_data_len=output_data_len.to(device)\n",
        "        loss, _, _= train(input_batch,input_data_len,output_batch,output_data_len, encoder,\n",
        "                        decoder, encoder_optimizer, decoder_optimizer, criteron)\n",
        "        \n",
        "#         a, b, c = train(input_batch,input_data_len,output_batch,output_data_len, encoder,\n",
        "#                decoder, encoder_optimizer, decoder_optimizer, criteron)\n",
        "        #return a, b,c \n",
        "        loss_total += loss\n",
        "        loss_iter += loss\n",
        "        \n",
        "        if iters % print_every == 0:\n",
        "          \n",
        "          Iter_loss=loss_iter/print_every\n",
        "          Iter_loss_list.append(Iter_loss)\n",
        "          \n",
        "          loss_list.append(Iter_loss)\n",
        "          \n",
        "          #decoded, actual = evaluate_batch(val_loader, encoder, decoder)\n",
        "#           print (decoded)\n",
        "#           print (actual)\n",
        "          #Bleu_Score = evaluate_bleu(decoded, actual)\n",
        "          \n",
        "         \n",
        "          \n",
        "          print('Iteration Loss: %s (%d %d%%) %.4f' % (timeSince(start, iters / n_iters), iters, iters / n_iters * 100, Iter_loss))\n",
        "         # print (\"Bleu Score: \", Bleu_Score)\n",
        "          loss_iter = 0\n",
        "          \n",
        "#           print('Average Loss: %s (%d %d%%) %.4f' % (timeSince(start, iters / n_iters),\n",
        "#                                              iters, iters / n_iters * 100, print_loss_avg))\n",
        "        if iters % plot_every == 0:\n",
        "          state = {'Epoch': Epoch, \"Itertion\": iters, 'encoder_state_dict': encoder.state_dict(), 'decoder_state_dict': decoder.state_dict, \n",
        "               'encoder_optimizer': encoder_optimizer.state_dict(), 'decoder_optimizer': decoder_optimizer.state_dict(), \"loss_list\": loss_list, \"loss_avg\": Iter_loss_list, \"Bleu\":bleu_score}\n",
        "      torch.save(state, folder_path+\"model_saved/Dec13_stella_compare{}.pt\".format(iters))\n",
        "      decoded_sentences, actual_sentences= evaluate_batch(val_loader, encoder, decoder)\n",
        "      bleu=corpus_bleu(decoded_sentences, [actual_sentences], use_effective_order=False)\n",
        "      print (\"Bleu: \", bleu)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNNxsEbhb1QN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, iteration_num):\n",
        "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
        "    folder_path = os.getcwd() + '/gdrive/My Drive/NLP_Project/'\n",
        "    start_epoch = 0\n",
        "    filename=folder_path+\"model_saved/Dec13_final_500_dot{}.pt\".format(iteration_num)\n",
        "    if os.path.isfile(filename):\n",
        "        print(\"=> loading checkpoint '{}'\".format(iteration_num))\n",
        "        checkpoint = torch.load(filename, map_location=device)\n",
        "        start_epoch = checkpoint['Epoch']\n",
        "        itertion = checkpoint['Itertion']\n",
        "        #model.load_state_dict(checkpoint['state_dict'])\n",
        "        encoder.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
        "        decoder.load_state_dict(checkpoint[\"decoder_state_dict\"]())\n",
        "        encoder_optimizer.load_state_dict(checkpoint[\"encoder_optimizer\"])\n",
        "        decoder_optimizer.load_state_dict(checkpoint[\"decoder_optimizer\"])\n",
        "        loss_list=checkpoint[\"loss_list\"]\n",
        "        Iter_loss_list=checkpoint[\"loss_avg\"]\n",
        "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        #losslogger = checkpoint['losslogger']\n",
        "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(filename, start_epoch))\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "\n",
        "    return start_epoch, itertion, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_list, Iter_loss_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HjGZc3yYLKQ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ]
    },
    {
      "metadata": {
        "id": "OYkV0bT4Kp0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1LYRw5HF03M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = VocabDataset(train_pairs, train_input_lang_new, train_output_lang_new)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=True,  drop_last = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KFIUQt5ulCgN",
        "colab_type": "code",
        "outputId": "333d9ae9-628a-47de-9b29-58a4f1e779eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "#batch_size=128\n",
        "val_input_lang, val_output_lang, val_pairs = prepareData(val_zh, val_en, reverse=False)\n",
        "val_dataset = VocabDataset(val_pairs, train_input_lang_new, train_output_lang_new)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=True,  drop_last = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 1261 sentence pairs\n",
            "Trimmed to 756 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "zh 2828\n",
            "en 1817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iDigeVEDcDUF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start_epoch=0\n",
        "itertion=0\n",
        "hidden_size=1000\n",
        "layers=2\n",
        "drop_out=0.1\n",
        "n_iters=100000\n",
        "attn_model=\"dot\"\n",
        "loss_list=[]\n",
        "learning_rate=0.001\n",
        "teacher_forcing_ratio=1\n",
        "decoder_learning_ratio = 1.0\n",
        "\n",
        "clip=5\n",
        "encoder = EncoderRNN(weights_matrix_zh, train_input_lang_new.n_words, hidden_size, n_layers = layers).to(device)\n",
        "decoder= LuongAttnDecoderRNN(attn_model, weights_matrix_eng, hidden_size, train_output_lang_new.n_words, drop_out, n_layers = layers).to(device)\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate*decoder_learning_ratio)\n",
        "#start_epoch, itertion, encoder, decoder, encoder_optimizer_, decoder_optimizer_, loss_list, Iter_loss_list = load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, 9069)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SttQ5PI9Lq_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8C5LhgK4K8jR",
        "colab_type": "code",
        "outputId": "97bca0c5-7a05-4906-929b-a092e8780e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6599
        }
      },
      "cell_type": "code",
      "source": [
        "criteron = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "trainIters(start_epoch, itertion, encoder, decoder, encoder_optimizer, decoder_optimizer, criteron, n_iters, train_loader, loss_list, print_every=100, plot_every=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:83: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration Loss: 1m 17s (- 1283m 23s) (100 0%) 5.7723\n",
            "Iteration Loss: 2m 33s (- 1278m 2s) (200 0%) 4.9995\n",
            "Iteration Loss: 3m 51s (- 1280m 17s) (300 0%) 4.7587\n",
            "Iteration Loss: 5m 8s (- 1279m 9s) (400 0%) 4.5606\n",
            "Iteration Loss: 6m 25s (- 1278m 29s) (500 0%) 4.4381\n",
            "Iteration Loss: 7m 42s (- 1278m 1s) (600 0%) 4.3277\n",
            "Iteration Loss: 9m 0s (- 1276m 54s) (700 0%) 4.2530\n",
            "Iteration Loss: 10m 17s (- 1275m 25s) (800 0%) 4.1867\n",
            "Iteration Loss: 11m 34s (- 1273m 40s) (900 0%) 4.0994\n",
            "Iteration Loss: 12m 51s (- 1272m 14s) (1000 1%) 4.0905\n",
            "Iteration Loss: 14m 8s (- 1271m 23s) (1100 1%) 4.0111\n",
            "Iteration Loss: 15m 25s (- 1269m 52s) (1200 1%) 3.9738\n",
            "Iteration Loss: 16m 43s (- 1269m 40s) (1300 1%) 3.9334\n",
            "Iteration Loss: 18m 0s (- 1267m 55s) (1400 1%) 3.8909\n",
            "Iteration Loss: 19m 17s (- 1266m 47s) (1500 1%) 3.8797\n",
            "Iteration Loss: 20m 34s (- 1265m 7s) (1600 1%) 3.8492\n",
            "Iteration Loss: 21m 51s (- 1264m 13s) (1700 1%) 3.7874\n",
            "Iteration Loss: 23m 9s (- 1263m 0s) (1800 1%) 3.7871\n",
            "Iteration Loss: 24m 26s (- 1261m 40s) (1900 1%) 3.7506\n",
            "Iteration Loss: 25m 43s (- 1260m 33s) (2000 2%) 3.7184\n",
            "Iteration Loss: 27m 1s (- 1259m 33s) (2100 2%) 3.7107\n",
            "Iteration Loss: 28m 18s (- 1258m 25s) (2200 2%) 3.6858\n",
            "Iteration Loss: 29m 36s (- 1257m 27s) (2300 2%) 3.6594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type LuongAttnDecoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type Attn. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.505611571392188, counts=[2945, 652, 236, 84], totals=[8275, 7522, 6773, 6031], precisions=[35.58912386706949, 8.667907471417177, 3.48442344603573, 1.3928038467915769], bp=0.8851002348548135, sys_len=8275, ref_len=9285)\n",
            "Epoch: 2\n",
            "Iteration Loss: 31m 44s (- 1290m 49s) (2400 2%) 3.4685\n",
            "Iteration Loss: 33m 1s (- 1288m 2s) (2500 2%) 3.3003\n",
            "Iteration Loss: 34m 19s (- 1285m 50s) (2600 2%) 3.3219\n",
            "Iteration Loss: 35m 36s (- 1283m 18s) (2700 2%) 3.3338\n",
            "Iteration Loss: 36m 53s (- 1280m 39s) (2800 2%) 3.3162\n",
            "Iteration Loss: 38m 10s (- 1278m 26s) (2900 2%) 3.3082\n",
            "Iteration Loss: 39m 28s (- 1276m 20s) (3000 3%) 3.3435\n",
            "Iteration Loss: 40m 45s (- 1274m 5s) (3100 3%) 3.3160\n",
            "Iteration Loss: 42m 3s (- 1272m 5s) (3200 3%) 3.3415\n",
            "Iteration Loss: 43m 20s (- 1270m 9s) (3300 3%) 3.3382\n",
            "Iteration Loss: 44m 38s (- 1268m 10s) (3400 3%) 3.3418\n",
            "Iteration Loss: 45m 55s (- 1266m 23s) (3500 3%) 3.3164\n",
            "Iteration Loss: 47m 13s (- 1264m 36s) (3600 3%) 3.3133\n",
            "Iteration Loss: 48m 31s (- 1262m 52s) (3700 3%) 3.3400\n",
            "Iteration Loss: 49m 48s (- 1260m 54s) (3800 3%) 3.3217\n",
            "Iteration Loss: 51m 6s (- 1259m 23s) (3900 3%) 3.3061\n",
            "Iteration Loss: 52m 23s (- 1257m 24s) (4000 4%) 3.3194\n",
            "Iteration Loss: 53m 40s (- 1255m 24s) (4100 4%) 3.2974\n",
            "Iteration Loss: 54m 57s (- 1253m 27s) (4200 4%) 3.3147\n",
            "Iteration Loss: 56m 14s (- 1251m 36s) (4300 4%) 3.3035\n",
            "Iteration Loss: 57m 31s (- 1249m 56s) (4400 4%) 3.3054\n",
            "Iteration Loss: 58m 49s (- 1248m 13s) (4500 4%) 3.3145\n",
            "Iteration Loss: 60m 6s (- 1246m 38s) (4600 4%) 3.3093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.174722594340055, counts=[3167, 716, 235, 80], totals=[9954, 9198, 8443, 7691], precisions=[31.816355234076752, 7.7843009349858665, 2.7833708397489043, 1.0401768300611103], bp=1.0, sys_len=9954, ref_len=9285)\n",
            "Epoch: 3\n",
            "Iteration Loss: 62m 4s (- 1258m 33s) (4700 4%) 3.2661\n",
            "Iteration Loss: 63m 20s (- 1256m 25s) (4800 4%) 2.7167\n",
            "Iteration Loss: 64m 38s (- 1254m 28s) (4900 4%) 2.7427\n",
            "Iteration Loss: 65m 55s (- 1252m 39s) (5000 5%) 2.7802\n",
            "Iteration Loss: 67m 13s (- 1250m 56s) (5100 5%) 2.7824\n",
            "Iteration Loss: 68m 30s (- 1248m 57s) (5200 5%) 2.8278\n",
            "Iteration Loss: 69m 48s (- 1247m 15s) (5300 5%) 2.8633\n",
            "Iteration Loss: 71m 5s (- 1245m 18s) (5400 5%) 2.8795\n",
            "Iteration Loss: 72m 22s (- 1243m 30s) (5500 5%) 2.8661\n",
            "Iteration Loss: 73m 39s (- 1241m 45s) (5600 5%) 2.8942\n",
            "Iteration Loss: 74m 56s (- 1239m 55s) (5700 5%) 2.9019\n",
            "Iteration Loss: 76m 14s (- 1238m 16s) (5800 5%) 2.9273\n",
            "Iteration Loss: 77m 32s (- 1236m 38s) (5900 5%) 2.9397\n",
            "Iteration Loss: 78m 49s (- 1235m 0s) (6000 6%) 2.9537\n",
            "Iteration Loss: 80m 7s (- 1233m 17s) (6100 6%) 2.9411\n",
            "Iteration Loss: 81m 24s (- 1231m 32s) (6200 6%) 2.9750\n",
            "Iteration Loss: 82m 41s (- 1229m 48s) (6300 6%) 2.9817\n",
            "Iteration Loss: 83m 58s (- 1228m 8s) (6400 6%) 2.9802\n",
            "Iteration Loss: 85m 15s (- 1226m 22s) (6500 6%) 2.9915\n",
            "Iteration Loss: 86m 32s (- 1224m 42s) (6600 6%) 2.9772\n",
            "Iteration Loss: 87m 48s (- 1222m 52s) (6700 6%) 3.0024\n",
            "Iteration Loss: 89m 5s (- 1221m 9s) (6800 6%) 2.9972\n",
            "Iteration Loss: 90m 22s (- 1219m 19s) (6900 6%) 3.0097\n",
            "Iteration Loss: 91m 39s (- 1217m 43s) (7000 7%) 3.0068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.593242324400979, counts=[3219, 742, 249, 84], totals=[9628, 8872, 8117, 7362], precisions=[33.433734939759034, 8.363390441839496, 3.067635826044105, 1.1409942950285248], bp=1.0, sys_len=9628, ref_len=9285)\n",
            "Epoch: 4\n",
            "Iteration Loss: 93m 35s (- 1224m 34s) (7100 7%) 2.7126\n",
            "Iteration Loss: 94m 52s (- 1222m 43s) (7200 7%) 2.4587\n",
            "Iteration Loss: 96m 9s (- 1221m 1s) (7300 7%) 2.4851\n",
            "Iteration Loss: 97m 25s (- 1219m 11s) (7400 7%) 2.4986\n",
            "Iteration Loss: 98m 42s (- 1217m 25s) (7500 7%) 2.5410\n",
            "Iteration Loss: 100m 0s (- 1215m 51s) (7600 7%) 2.5570\n",
            "Iteration Loss: 101m 17s (- 1214m 12s) (7700 7%) 2.5781\n",
            "Iteration Loss: 102m 34s (- 1212m 25s) (7800 7%) 2.5755\n",
            "Iteration Loss: 103m 51s (- 1210m 51s) (7900 7%) 2.6200\n",
            "Iteration Loss: 105m 8s (- 1209m 11s) (8000 8%) 2.6507\n",
            "Iteration Loss: 106m 26s (- 1207m 36s) (8100 8%) 2.6544\n",
            "Iteration Loss: 107m 43s (- 1205m 58s) (8200 8%) 2.6587\n",
            "Iteration Loss: 109m 0s (- 1204m 25s) (8300 8%) 2.6791\n",
            "Iteration Loss: 110m 17s (- 1202m 44s) (8400 8%) 2.6970\n",
            "Iteration Loss: 111m 35s (- 1201m 10s) (8500 8%) 2.7202\n",
            "Iteration Loss: 112m 52s (- 1199m 34s) (8600 8%) 2.7285\n",
            "Iteration Loss: 114m 9s (- 1197m 59s) (8700 8%) 2.7384\n",
            "Iteration Loss: 115m 26s (- 1196m 23s) (8800 8%) 2.7385\n",
            "Iteration Loss: 116m 43s (- 1194m 49s) (8900 8%) 2.7587\n",
            "Iteration Loss: 118m 0s (- 1193m 13s) (9000 9%) 2.7771\n",
            "Iteration Loss: 119m 17s (- 1191m 40s) (9100 9%) 2.8024\n",
            "Iteration Loss: 120m 35s (- 1190m 7s) (9200 9%) 2.7972\n",
            "Iteration Loss: 121m 52s (- 1188m 32s) (9300 9%) 2.7908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.636254252988955, counts=[3230, 759, 250, 79], totals=[9494, 8738, 7986, 7242], precisions=[34.02148725510849, 8.686198214694437, 3.130478337089907, 1.0908588787627727], bp=1.0, sys_len=9494, ref_len=9285)\n",
            "Epoch: 5\n",
            "Iteration Loss: 123m 48s (- 1193m 16s) (9400 9%) 2.7870\n",
            "Iteration Loss: 125m 5s (- 1191m 35s) (9500 9%) 2.2857\n",
            "Iteration Loss: 126m 21s (- 1189m 56s) (9600 9%) 2.3282\n",
            "Iteration Loss: 127m 39s (- 1188m 25s) (9700 9%) 2.3518\n",
            "Iteration Loss: 128m 57s (- 1186m 53s) (9800 9%) 2.3681\n",
            "Iteration Loss: 130m 13s (- 1185m 9s) (9900 9%) 2.4141\n",
            "Iteration Loss: 131m 31s (- 1183m 39s) (10000 10%) 2.4137\n",
            "Iteration Loss: 132m 48s (- 1182m 7s) (10100 10%) 2.4375\n",
            "Iteration Loss: 134m 5s (- 1180m 35s) (10200 10%) 2.4619\n",
            "Iteration Loss: 135m 22s (- 1178m 59s) (10300 10%) 2.4815\n",
            "Iteration Loss: 136m 40s (- 1177m 28s) (10400 10%) 2.4834\n",
            "Iteration Loss: 137m 57s (- 1175m 52s) (10500 10%) 2.5250\n",
            "Iteration Loss: 139m 13s (- 1174m 15s) (10600 10%) 2.5168\n",
            "Iteration Loss: 140m 30s (- 1172m 37s) (10700 10%) 2.5199\n",
            "Iteration Loss: 141m 47s (- 1171m 2s) (10800 10%) 2.5453\n",
            "Iteration Loss: 143m 4s (- 1169m 31s) (10900 10%) 2.5605\n",
            "Iteration Loss: 144m 21s (- 1167m 57s) (11000 11%) 2.6013\n",
            "Iteration Loss: 145m 38s (- 1166m 25s) (11100 11%) 2.5954\n",
            "Iteration Loss: 146m 56s (- 1165m 0s) (11200 11%) 2.6282\n",
            "Iteration Loss: 148m 12s (- 1163m 24s) (11300 11%) 2.5987\n",
            "Iteration Loss: 149m 30s (- 1161m 58s) (11400 11%) 2.6284\n",
            "Iteration Loss: 150m 47s (- 1160m 30s) (11500 11%) 2.6573\n",
            "Iteration Loss: 152m 5s (- 1159m 6s) (11600 11%) 2.6665\n",
            "Iteration Loss: 153m 22s (- 1157m 32s) (11700 11%) 2.6664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.620990273780996, counts=[3289, 778, 245, 77], totals=[9513, 8757, 8002, 7254], precisions=[34.57374119625775, 8.884321114536942, 3.0617345663584103, 1.0614833195478357], bp=1.0, sys_len=9513, ref_len=9285)\n",
            "Epoch: 6\n",
            "Iteration Loss: 155m 19s (- 1160m 56s) (11800 11%) 2.4247\n",
            "Iteration Loss: 156m 36s (- 1159m 23s) (11900 11%) 2.2000\n",
            "Iteration Loss: 157m 53s (- 1157m 55s) (12000 12%) 2.2322\n",
            "Iteration Loss: 159m 11s (- 1156m 24s) (12100 12%) 2.2559\n",
            "Iteration Loss: 160m 28s (- 1154m 55s) (12200 12%) 2.2920\n",
            "Iteration Loss: 161m 46s (- 1153m 27s) (12300 12%) 2.3222\n",
            "Iteration Loss: 163m 4s (- 1152m 3s) (12400 12%) 2.3462\n",
            "Iteration Loss: 164m 22s (- 1150m 34s) (12500 12%) 2.3455\n",
            "Iteration Loss: 165m 38s (- 1149m 1s) (12600 12%) 2.3687\n",
            "Iteration Loss: 166m 56s (- 1147m 30s) (12700 12%) 2.3947\n",
            "Iteration Loss: 168m 13s (- 1145m 59s) (12800 12%) 2.4200\n",
            "Iteration Loss: 169m 30s (- 1144m 28s) (12900 12%) 2.4163\n",
            "Iteration Loss: 170m 48s (- 1143m 3s) (13000 13%) 2.4689\n",
            "Iteration Loss: 172m 4s (- 1141m 28s) (13100 13%) 2.4376\n",
            "Iteration Loss: 173m 21s (- 1139m 57s) (13200 13%) 2.4836\n",
            "Iteration Loss: 174m 38s (- 1138m 29s) (13300 13%) 2.4896\n",
            "Iteration Loss: 175m 56s (- 1137m 5s) (13400 13%) 2.5025\n",
            "Iteration Loss: 177m 14s (- 1135m 37s) (13500 13%) 2.5128\n",
            "Iteration Loss: 178m 30s (- 1134m 5s) (13600 13%) 2.5301\n",
            "Iteration Loss: 179m 47s (- 1132m 34s) (13700 13%) 2.5373\n",
            "Iteration Loss: 181m 5s (- 1131m 9s) (13800 13%) 2.5490\n",
            "Iteration Loss: 182m 22s (- 1129m 38s) (13900 13%) 2.5481\n",
            "Iteration Loss: 183m 39s (- 1128m 8s) (14000 14%) 2.5546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.802394631985424, counts=[3301, 771, 251, 88], totals=[9565, 8809, 8053, 7309], precisions=[34.51123889179299, 8.752412305596549, 3.1168508630324103, 1.203995074565604], bp=1.0, sys_len=9565, ref_len=9285)\n",
            "Epoch: 7\n",
            "Iteration Loss: 185m 34s (- 1130m 32s) (14100 14%) 2.4981\n",
            "Iteration Loss: 186m 50s (- 1128m 58s) (14200 14%) 2.1311\n",
            "Iteration Loss: 188m 7s (- 1127m 26s) (14300 14%) 2.1526\n",
            "Iteration Loss: 189m 24s (- 1125m 56s) (14400 14%) 2.1765\n",
            "Iteration Loss: 190m 41s (- 1124m 25s) (14500 14%) 2.2330\n",
            "Iteration Loss: 191m 59s (- 1123m 0s) (14600 14%) 2.2305\n",
            "Iteration Loss: 193m 15s (- 1121m 28s) (14700 14%) 2.2724\n",
            "Iteration Loss: 194m 33s (- 1120m 0s) (14800 14%) 2.2814\n",
            "Iteration Loss: 195m 50s (- 1118m 32s) (14900 14%) 2.3065\n",
            "Iteration Loss: 197m 7s (- 1117m 5s) (15000 15%) 2.3170\n",
            "Iteration Loss: 198m 25s (- 1115m 37s) (15100 15%) 2.3610\n",
            "Iteration Loss: 199m 42s (- 1114m 9s) (15200 15%) 2.3496\n",
            "Iteration Loss: 200m 59s (- 1112m 39s) (15300 15%) 2.3454\n",
            "Iteration Loss: 202m 16s (- 1111m 9s) (15400 15%) 2.3840\n",
            "Iteration Loss: 203m 33s (- 1109m 41s) (15500 15%) 2.4015\n",
            "Iteration Loss: 204m 49s (- 1108m 10s) (15600 15%) 2.4110\n",
            "Iteration Loss: 206m 6s (- 1106m 41s) (15700 15%) 2.4240\n",
            "Iteration Loss: 207m 23s (- 1105m 14s) (15800 15%) 2.4209\n",
            "Iteration Loss: 208m 40s (- 1103m 45s) (15900 15%) 2.4561\n",
            "Iteration Loss: 209m 57s (- 1102m 17s) (16000 16%) 2.4672\n",
            "Iteration Loss: 211m 15s (- 1100m 51s) (16100 16%) 2.4772\n",
            "Iteration Loss: 212m 32s (- 1099m 25s) (16200 16%) 2.4788\n",
            "Iteration Loss: 213m 49s (- 1097m 59s) (16300 16%) 2.4891\n",
            "Iteration Loss: 215m 6s (- 1096m 30s) (16400 16%) 2.4906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.401833297886212, counts=[3325, 760, 244, 93], totals=[10231, 9475, 8719, 7968], precisions=[32.49926693382856, 8.021108179419524, 2.7984860649157013, 1.1671686746987953], bp=1.0, sys_len=10231, ref_len=9285)\n",
            "Epoch: 8\n",
            "Iteration Loss: 217m 4s (- 1098m 34s) (16500 16%) 2.2432\n",
            "Iteration Loss: 218m 22s (- 1097m 6s) (16600 16%) 2.1137\n",
            "Iteration Loss: 219m 39s (- 1095m 37s) (16700 16%) 2.1326\n",
            "Iteration Loss: 220m 56s (- 1094m 9s) (16800 16%) 2.1768\n",
            "Iteration Loss: 222m 13s (- 1092m 44s) (16900 16%) 2.1707\n",
            "Iteration Loss: 223m 30s (- 1091m 15s) (17000 17%) 2.1871\n",
            "Iteration Loss: 224m 48s (- 1089m 50s) (17100 17%) 2.2301\n",
            "Iteration Loss: 226m 5s (- 1088m 24s) (17200 17%) 2.2317\n",
            "Iteration Loss: 227m 22s (- 1086m 57s) (17300 17%) 2.2604\n",
            "Iteration Loss: 228m 39s (- 1085m 29s) (17400 17%) 2.2725\n",
            "Iteration Loss: 229m 57s (- 1084m 4s) (17500 17%) 2.2894\n",
            "Iteration Loss: 231m 13s (- 1082m 33s) (17600 17%) 2.3103\n",
            "Iteration Loss: 232m 30s (- 1081m 5s) (17700 17%) 2.2950\n",
            "Iteration Loss: 233m 47s (- 1079m 39s) (17800 17%) 2.3574\n",
            "Iteration Loss: 235m 4s (- 1078m 11s) (17900 17%) 2.3400\n",
            "Iteration Loss: 236m 21s (- 1076m 43s) (18000 18%) 2.3707\n",
            "Iteration Loss: 237m 38s (- 1075m 19s) (18100 18%) 2.3943\n",
            "Iteration Loss: 238m 55s (- 1073m 52s) (18200 18%) 2.3981\n",
            "Iteration Loss: 240m 12s (- 1072m 25s) (18300 18%) 2.4023\n",
            "Iteration Loss: 241m 29s (- 1070m 58s) (18400 18%) 2.4291\n",
            "Iteration Loss: 242m 46s (- 1069m 31s) (18500 18%) 2.4260\n",
            "Iteration Loss: 244m 3s (- 1068m 4s) (18600 18%) 2.4525\n",
            "Iteration Loss: 245m 19s (- 1066m 36s) (18700 18%) 2.4501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.299051935210646, counts=[3250, 731, 224, 64], totals=[9279, 8523, 7767, 7014], precisions=[35.02532600495743, 8.576792209315968, 2.883996395004506, 0.9124607927003137], bp=0.9993535876113976, sys_len=9279, ref_len=9285)\n",
            "Epoch: 9\n",
            "Iteration Loss: 247m 14s (- 1067m 53s) (18800 18%) 2.3887\n",
            "Iteration Loss: 248m 32s (- 1066m 27s) (18900 18%) 2.0734\n",
            "Iteration Loss: 249m 49s (- 1065m 1s) (19000 19%) 2.0995\n",
            "Iteration Loss: 251m 6s (- 1063m 36s) (19100 19%) 2.1288\n",
            "Iteration Loss: 252m 24s (- 1062m 11s) (19200 19%) 2.1360\n",
            "Iteration Loss: 253m 41s (- 1060m 46s) (19300 19%) 2.1448\n",
            "Iteration Loss: 254m 58s (- 1059m 18s) (19400 19%) 2.1769\n",
            "Iteration Loss: 256m 15s (- 1057m 51s) (19500 19%) 2.1896\n",
            "Iteration Loss: 257m 32s (- 1056m 25s) (19600 19%) 2.2057\n",
            "Iteration Loss: 258m 48s (- 1054m 57s) (19700 19%) 2.2160\n",
            "Iteration Loss: 260m 6s (- 1053m 32s) (19800 19%) 2.2616\n",
            "Iteration Loss: 261m 22s (- 1052m 5s) (19900 19%) 2.2650\n",
            "Iteration Loss: 262m 39s (- 1050m 38s) (20000 20%) 2.2623\n",
            "Iteration Loss: 263m 56s (- 1049m 12s) (20100 20%) 2.3012\n",
            "Iteration Loss: 265m 13s (- 1047m 46s) (20200 20%) 2.3106\n",
            "Iteration Loss: 266m 30s (- 1046m 19s) (20300 20%) 2.3188\n",
            "Iteration Loss: 267m 47s (- 1044m 56s) (20400 20%) 2.3348\n",
            "Iteration Loss: 269m 5s (- 1043m 32s) (20500 20%) 2.3493\n",
            "Iteration Loss: 270m 22s (- 1042m 7s) (20600 20%) 2.3629\n",
            "Iteration Loss: 271m 39s (- 1040m 42s) (20700 20%) 2.3914\n",
            "Iteration Loss: 272m 56s (- 1039m 17s) (20800 20%) 2.3952\n",
            "Iteration Loss: 274m 13s (- 1037m 52s) (20900 20%) 2.3913\n",
            "Iteration Loss: 275m 31s (- 1036m 29s) (21000 21%) 2.4033\n",
            "Iteration Loss: 276m 48s (- 1035m 4s) (21100 21%) 2.4111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.6526956148979135, counts=[3239, 770, 249, 82], totals=[9580, 8824, 8068, 7313], precisions=[33.81002087682672, 8.726201269265639, 3.086266732771443, 1.121290851907562], bp=1.0, sys_len=9580, ref_len=9285)\n",
            "Epoch: 10\n",
            "Iteration Loss: 278m 45s (- 1036m 8s) (21200 21%) 2.1717\n",
            "Iteration Loss: 280m 2s (- 1034m 42s) (21300 21%) 2.0681\n",
            "Iteration Loss: 281m 19s (- 1033m 17s) (21400 21%) 2.0858\n",
            "Iteration Loss: 282m 36s (- 1031m 50s) (21500 21%) 2.1104\n",
            "Iteration Loss: 283m 53s (- 1030m 23s) (21600 21%) 2.1260\n",
            "Iteration Loss: 285m 9s (- 1028m 57s) (21700 21%) 2.1339\n",
            "Iteration Loss: 286m 26s (- 1027m 30s) (21800 21%) 2.1621\n",
            "Iteration Loss: 287m 44s (- 1026m 7s) (21900 21%) 2.1787\n",
            "Iteration Loss: 289m 0s (- 1024m 40s) (22000 22%) 2.1902\n",
            "Iteration Loss: 290m 17s (- 1023m 14s) (22100 22%) 2.2066\n",
            "Iteration Loss: 291m 34s (- 1021m 51s) (22200 22%) 2.2212\n",
            "Iteration Loss: 292m 52s (- 1020m 28s) (22300 22%) 2.2613\n",
            "Iteration Loss: 294m 9s (- 1019m 2s) (22400 22%) 2.2447\n",
            "Iteration Loss: 295m 26s (- 1017m 38s) (22500 22%) 2.2684\n",
            "Iteration Loss: 296m 43s (- 1016m 13s) (22600 22%) 2.2958\n",
            "Iteration Loss: 298m 1s (- 1014m 50s) (22700 22%) 2.3001\n",
            "Iteration Loss: 299m 18s (- 1013m 26s) (22800 22%) 2.3211\n",
            "Iteration Loss: 300m 36s (- 1012m 4s) (22900 22%) 2.3414\n",
            "Iteration Loss: 301m 52s (- 1010m 38s) (23000 23%) 2.3385\n",
            "Iteration Loss: 303m 10s (- 1009m 15s) (23100 23%) 2.3712\n",
            "Iteration Loss: 304m 27s (- 1007m 51s) (23200 23%) 2.3525\n",
            "Iteration Loss: 305m 44s (- 1006m 28s) (23300 23%) 2.3926\n",
            "Iteration Loss: 307m 2s (- 1005m 5s) (23400 23%) 2.3603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.393580754702707, counts=[3268, 746, 246, 78], totals=[9798, 9042, 8286, 7530], precisions=[33.35374566238008, 8.25038708250387, 2.9688631426502536, 1.0358565737051793], bp=1.0, sys_len=9798, ref_len=9285)\n",
            "Epoch: 11\n",
            "Iteration Loss: 308m 59s (- 1005m 51s) (23500 23%) 2.3234\n",
            "Iteration Loss: 310m 16s (- 1004m 27s) (23600 23%) 2.0539\n",
            "Iteration Loss: 311m 33s (- 1003m 2s) (23700 23%) 2.0579\n",
            "Iteration Loss: 312m 51s (- 1001m 39s) (23800 23%) 2.0767\n",
            "Iteration Loss: 314m 8s (- 1000m 16s) (23900 23%) 2.0959\n",
            "Iteration Loss: 315m 26s (- 998m 52s) (24000 24%) 2.1298\n",
            "Iteration Loss: 316m 42s (- 997m 27s) (24100 24%) 2.1328\n",
            "Iteration Loss: 318m 0s (- 996m 5s) (24200 24%) 2.1540\n",
            "Iteration Loss: 319m 18s (- 994m 42s) (24300 24%) 2.1620\n",
            "Iteration Loss: 320m 35s (- 993m 19s) (24400 24%) 2.1780\n",
            "Iteration Loss: 321m 52s (- 991m 55s) (24500 24%) 2.2033\n",
            "Iteration Loss: 323m 10s (- 990m 31s) (24600 24%) 2.2291\n",
            "Iteration Loss: 324m 26s (- 989m 6s) (24700 24%) 2.2149\n",
            "Iteration Loss: 325m 43s (- 987m 41s) (24800 24%) 2.2426\n",
            "Iteration Loss: 327m 0s (- 986m 16s) (24900 24%) 2.2586\n",
            "Iteration Loss: 328m 17s (- 984m 53s) (25000 25%) 2.2738\n",
            "Iteration Loss: 329m 35s (- 983m 30s) (25100 25%) 2.2863\n",
            "Iteration Loss: 330m 52s (- 982m 6s) (25200 25%) 2.3050\n",
            "Iteration Loss: 332m 9s (- 980m 43s) (25300 25%) 2.3019\n",
            "Iteration Loss: 333m 26s (- 979m 20s) (25400 25%) 2.3112\n",
            "Iteration Loss: 334m 43s (- 977m 56s) (25500 25%) 2.3438\n",
            "Iteration Loss: 336m 1s (- 976m 33s) (25600 25%) 2.3484\n",
            "Iteration Loss: 337m 18s (- 975m 9s) (25700 25%) 2.3513\n",
            "Iteration Loss: 338m 35s (- 973m 46s) (25800 25%) 2.3668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=4.95462830138734, counts=[3209, 671, 200, 66], totals=[9462, 8706, 7950, 7202], precisions=[33.914605791587405, 7.707328279347577, 2.5157232704402515, 0.9164121077478479], bp=1.0, sys_len=9462, ref_len=9285)\n",
            "Epoch: 12\n",
            "Iteration Loss: 340m 30s (- 974m 11s) (25900 25%) 2.1121\n",
            "Iteration Loss: 341m 47s (- 972m 47s) (26000 26%) 2.0411\n",
            "Iteration Loss: 343m 4s (- 971m 24s) (26100 26%) 2.0615\n",
            "Iteration Loss: 344m 21s (- 969m 59s) (26200 26%) 2.0767\n",
            "Iteration Loss: 345m 38s (- 968m 34s) (26300 26%) 2.0939\n",
            "Iteration Loss: 346m 56s (- 967m 13s) (26400 26%) 2.1244\n",
            "Iteration Loss: 348m 13s (- 965m 50s) (26500 26%) 2.1248\n",
            "Iteration Loss: 349m 31s (- 964m 27s) (26600 26%) 2.1635\n",
            "Iteration Loss: 350m 48s (- 963m 3s) (26700 26%) 2.1709\n",
            "Iteration Loss: 352m 5s (- 961m 41s) (26800 26%) 2.1845\n",
            "Iteration Loss: 353m 23s (- 960m 19s) (26900 26%) 2.1948\n",
            "Iteration Loss: 354m 40s (- 958m 57s) (27000 27%) 2.2133\n",
            "Iteration Loss: 355m 58s (- 957m 35s) (27100 27%) 2.2214\n",
            "Iteration Loss: 357m 16s (- 956m 14s) (27200 27%) 2.2468\n",
            "Iteration Loss: 358m 33s (- 954m 51s) (27300 27%) 2.2491\n",
            "Iteration Loss: 359m 51s (- 953m 28s) (27400 27%) 2.2614\n",
            "Iteration Loss: 361m 8s (- 952m 7s) (27500 27%) 2.2757\n",
            "Iteration Loss: 362m 26s (- 950m 43s) (27600 27%) 2.2813\n",
            "Iteration Loss: 363m 43s (- 949m 21s) (27700 27%) 2.3108\n",
            "Iteration Loss: 365m 0s (- 947m 57s) (27800 27%) 2.3028\n",
            "Iteration Loss: 366m 17s (- 946m 34s) (27900 27%) 2.3069\n",
            "Iteration Loss: 367m 34s (- 945m 11s) (28000 28%) 2.3394\n",
            "Iteration Loss: 368m 51s (- 943m 47s) (28100 28%) 2.3328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.37912129368308, counts=[3252, 710, 234, 92], totals=[9943, 9193, 8443, 7693], precisions=[32.706426631801264, 7.723267703687588, 2.7715267085159305, 1.1958923696867283], bp=1.0, sys_len=9943, ref_len=9285)\n",
            "Epoch: 13\n",
            "Iteration Loss: 370m 48s (- 944m 5s) (28200 28%) 2.2549\n",
            "Iteration Loss: 372m 5s (- 942m 42s) (28300 28%) 1.9878\n",
            "Iteration Loss: 373m 22s (- 941m 18s) (28400 28%) 2.0340\n",
            "Iteration Loss: 374m 39s (- 939m 56s) (28500 28%) 2.0543\n",
            "Iteration Loss: 375m 56s (- 938m 31s) (28600 28%) 2.0691\n",
            "Iteration Loss: 377m 13s (- 937m 9s) (28700 28%) 2.0827\n",
            "Iteration Loss: 378m 29s (- 935m 43s) (28800 28%) 2.1062\n",
            "Iteration Loss: 379m 46s (- 934m 19s) (28900 28%) 2.1235\n",
            "Iteration Loss: 381m 4s (- 932m 57s) (29000 28%) 2.1341\n",
            "Iteration Loss: 382m 22s (- 931m 36s) (29100 29%) 2.1643\n",
            "Iteration Loss: 383m 40s (- 930m 15s) (29200 29%) 2.1694\n",
            "Iteration Loss: 384m 57s (- 928m 53s) (29300 29%) 2.1893\n",
            "Iteration Loss: 386m 14s (- 927m 30s) (29400 29%) 2.2018\n",
            "Iteration Loss: 387m 31s (- 926m 6s) (29500 29%) 2.2203\n",
            "Iteration Loss: 388m 48s (- 924m 44s) (29600 29%) 2.2228\n",
            "Iteration Loss: 390m 5s (- 923m 21s) (29700 29%) 2.2523\n",
            "Iteration Loss: 391m 23s (- 922m 0s) (29800 29%) 2.2635\n",
            "Iteration Loss: 392m 41s (- 920m 39s) (29900 29%) 2.2583\n",
            "Iteration Loss: 393m 59s (- 919m 18s) (30000 30%) 2.2891\n",
            "Iteration Loss: 395m 16s (- 917m 56s) (30100 30%) 2.2822\n",
            "Iteration Loss: 396m 33s (- 916m 33s) (30200 30%) 2.3002\n",
            "Iteration Loss: 397m 51s (- 915m 12s) (30300 30%) 2.3077\n",
            "Iteration Loss: 399m 8s (- 913m 50s) (30400 30%) 2.3352\n",
            "Iteration Loss: 400m 26s (- 912m 28s) (30500 30%) 2.3280\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bleu:  BLEU(score=5.011021739286931, counts=[3282, 693, 217, 75], totals=[9927, 9171, 8415, 7663], precisions=[33.06134783922635, 7.556427870461237, 2.5787284610814023, 0.9787289573274175], bp=1.0, sys_len=9927, ref_len=9285)\n",
            "Epoch: 14\n",
            "Iteration Loss: 402m 23s (- 912m 37s) (30600 30%) 2.0876\n",
            "Iteration Loss: 403m 41s (- 911m 14s) (30700 30%) 2.0181\n",
            "Iteration Loss: 404m 58s (- 909m 52s) (30800 30%) 2.0496\n",
            "Iteration Loss: 406m 15s (- 908m 30s) (30900 30%) 2.0449\n",
            "Iteration Loss: 407m 32s (- 907m 6s) (31000 31%) 2.0672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yIEnHFU4Yej5",
        "colab_type": "code",
        "outputId": "78471832-d781-4160-8dff-ee2f7744980d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "cell_type": "code",
      "source": [
        "decoded_sentences, actual_sentences= evaluate_batch(val_loader, encoder, decoder)\n",
        "bleu=corpus_bleu(decoded_sentences, [actual_sentences], use_effective_order=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}