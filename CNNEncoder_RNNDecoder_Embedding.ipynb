{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNEncoder_RNNDecoder_Embedding",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "sP4TOAqdM1ia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "cc3bb552-0542-4330-bf00-bf3fe1ee0a03"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x57d3c000 @  0x7fb6923f42a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0uWjT6lM7eL",
        "colab_type": "code",
        "outputId": "ae9f7b4b-a244-4611-9dc9-4bac84474c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AyXb-dQXM3dy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import functional\n",
        "from asyncio import PriorityQueue\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "MAX_LENGTH = 30 #temp\n",
        "\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "\n",
        "PAD_IDX = 0 \n",
        "SOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "UNK_IDX = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtj5cLvUN0We",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_loc = 'gdrive/My Drive/iwslt-vi-en'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-YGl6fUJ0y6",
        "colab_type": "code",
        "outputId": "0648870b-f35e-408e-815a-ef31cec97feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "p = PriorityQueue()\n",
        "p.put_nowait((1, 'a'))\n",
        "p.put_nowait((3, 'c'))\n",
        "p.put_nowait((2, 'b'))\n",
        "print(p.get_nowait())\n",
        "print(p.get_nowait())\n",
        "print(p.get_nowait())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 'a')\n",
            "(2, 'b')\n",
            "(3, 'c')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JA_3OsKzOvml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    batch_size = sequence_length.size(0)\n",
        "    seq_range = torch.range(0, max_len - 1).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
        "    seq_range_expand = Variable(seq_range_expand)\n",
        "    if sequence_length.is_cuda:\n",
        "        seq_range_expand = seq_range_expand.cuda()\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    return seq_range_expand < seq_length_expand\n",
        "\n",
        "\n",
        "def masked_cross_entropy(logits, target, length):\n",
        "    length = Variable(torch.LongTensor(length))\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "    \"\"\"\n",
        "\n",
        "    # logits_flat: (batch * max_len, num_classes)\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # log_probs_flat: (batch * max_len, num_classes)\n",
        "    log_probs_flat = functional.log_softmax(logits_flat)\n",
        "    # target_flat: (batch * max_len, 1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    # losses_flat: (batch * max_len, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    # losses: (batch, max_len)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    # mask: (batch, max_len)\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / length.float().sum()\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Remove punctuation\n",
        "def removePunctuation(s):\n",
        "\n",
        "    to_remove = ('&lt;', '&gt;', '&amp;', '&apos;', '&quot;')\n",
        "    table = str.maketrans(dict.fromkeys('.!?:,'))\n",
        "    s = s.translate(table)\n",
        "    for i in to_remove:\n",
        "        s=s.replace(i,'')   \n",
        "    s = s.strip()\n",
        "    \n",
        "    return s\n",
        "\n",
        "\n",
        "from typing import List\n",
        "from collections import Counter, namedtuple\n",
        "from itertools import zip_longest\n",
        "\n",
        "def tokenize_13a(line):\n",
        "    \"\"\"\n",
        "    Tokenizes an input line using a relatively minimal tokenization that is however equivalent to mteval-v13a, used by WMT.\n",
        "    :param line: a segment to tokenize\n",
        "    :return: the tokenized line\n",
        "    \"\"\"\n",
        "\n",
        "    norm = line\n",
        "\n",
        "    # language-independent part:\n",
        "    norm = norm.replace('<skipped>', '')\n",
        "    norm = norm.replace('-\\n', '')\n",
        "    norm = norm.replace('\\n', ' ')\n",
        "    norm = norm.replace('&quot;', '\"')\n",
        "    norm = norm.replace('&amp;', '&')\n",
        "    norm = norm.replace('&lt;', '<')\n",
        "    norm = norm.replace('&gt;', '>')\n",
        "\n",
        "    # language-dependent part (assuming Western languages):\n",
        "    norm = \" {} \".format(norm)\n",
        "    norm = re.sub(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', ' \\\\1 ', norm)\n",
        "    norm = re.sub(r'([^0-9])([\\.,])', '\\\\1 \\\\2 ', norm)  # tokenize period and comma unless preceded by a digit\n",
        "    norm = re.sub(r'([\\.,])([^0-9])', ' \\\\1 \\\\2', norm)  # tokenize period and comma unless followed by a digit\n",
        "    norm = re.sub(r'([0-9])(-)', '\\\\1 \\\\2 ', norm)  # tokenize dash when preceded by a digit\n",
        "    norm = re.sub(r'\\s+', ' ', norm)  # one space only between words\n",
        "    norm = re.sub(r'^\\s+', '', norm)  # no leading space\n",
        "    norm = re.sub(r'\\s+$', '', norm)  # no trailing space\n",
        "\n",
        "    return norm\n",
        "\n",
        "def corpus_bleu(sys_stream, ref_streams, smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n",
        "                 use_effective_order=False):\n",
        "    \"\"\"Produces BLEU scores along with its sufficient statistics from a source against one or more references.\n",
        "    :param sys_stream: The system stream (a sequence of segments)\n",
        "    :param ref_streams: A list of one or more reference streams (each a sequence of segments)\n",
        "    :param smooth: The smoothing method to use\n",
        "    :param smooth_floor: For 'floor' smoothing, the floor to use\n",
        "    :param force: Ignore data that looks already tokenized\n",
        "    :param lowercase: Lowercase the data\n",
        "    :param tokenize: The tokenizer to use\n",
        "    :return: a BLEU object containing everything you'd want\n",
        "    \"\"\"\n",
        "\n",
        "    # Add some robustness to the input arguments\n",
        "    if isinstance(sys_stream, str):\n",
        "        sys_stream = [sys_stream]\n",
        "    if isinstance(ref_streams, str):\n",
        "        ref_streams = [[ref_streams]]\n",
        "\n",
        "    sys_len = 0\n",
        "    ref_len = 0\n",
        "\n",
        "    correct = [0 for n in range(NGRAM_ORDER)]\n",
        "    total = [0 for n in range(NGRAM_ORDER)]\n",
        "    \n",
        "\n",
        "    # look for already-tokenized sentences\n",
        "    tokenized_count = 0\n",
        "\n",
        "    fhs = [sys_stream] + ref_streams\n",
        "    for lines in zip_longest(*fhs):\n",
        "        if None in lines:\n",
        "            raise EOFError(\"Source and reference streams have different lengths!\")\n",
        "\n",
        "        if lowercase:\n",
        "            lines = [x.lower() for x in lines]\n",
        "            \n",
        "        tokenize= 'tokenize_13a'    \n",
        "\n",
        "        if not (force or tokenize == 'none') and lines[0].rstrip().endswith(' .'):\n",
        "            tokenized_count += 1\n",
        "\n",
        "            if tokenized_count == 100:\n",
        "                logging.warning('That\\'s 100 lines that end in a tokenized period (\\'.\\')')\n",
        "                logging.warning('It looks like you forgot to detokenize your test data, which may hurt your score.')\n",
        "                logging.warning('If you insist your data is detokenized, or don\\'t care, you can suppress this message with \\'--force\\'.')\n",
        "\n",
        "        output, *refs = [tokenize_13a(x.rstrip()) for x in lines]\n",
        "        \n",
        "\n",
        "        ref_ngrams, closest_diff, closest_len = ref_stats(output, refs)\n",
        "        \n",
        "\n",
        "        sys_len += len(output.split())\n",
        "        ref_len += closest_len\n",
        "\n",
        "        sys_ngrams = extract_ngrams(output)\n",
        "        for ngram in sys_ngrams.keys():\n",
        "            n = len(ngram.split())\n",
        "            correct[n-1] += min(sys_ngrams[ngram], ref_ngrams.get(ngram, 0))\n",
        "            total[n-1] += sys_ngrams[ngram]\n",
        "            \n",
        "\n",
        "    return compute_bleu(correct, total, sys_len, ref_len, smooth, smooth_floor, use_effective_order)\n",
        "  \n",
        "  \n",
        "# n-gram order. Don't change this.\n",
        "NGRAM_ORDER = 4\n",
        "  \n",
        "def compute_bleu(correct: List[int], total: List[int], sys_len: int, ref_len: int, smooth = 'none', smooth_floor = 0.01,\n",
        "                 use_effective_order = False):\n",
        "    \"\"\"Computes BLEU score from its sufficient statistics. Adds smoothing.\n",
        "    :param correct: List of counts of correct ngrams, 1 <= n <= NGRAM_ORDER\n",
        "    :param total: List of counts of total ngrams, 1 <= n <= NGRAM_ORDER\n",
        "    :param sys_len: The cumulative system length\n",
        "    :param ref_len: The cumulative reference length\n",
        "    :param smooth: The smoothing method to use\n",
        "    :param smooth_floor: The smoothing value added, if smooth method 'floor' is used\n",
        "    :param use_effective_order: Use effective order.\n",
        "    :return: A BLEU object with the score (100-based) and other statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    precisions = [0 for x in range(NGRAM_ORDER)]\n",
        "\n",
        "    smooth_mteval = 1.\n",
        "    effective_order = NGRAM_ORDER\n",
        "    for n in range(NGRAM_ORDER):\n",
        "        if total[n] == 0:\n",
        "            break\n",
        "\n",
        "        if use_effective_order:\n",
        "            effective_order = n + 1\n",
        "\n",
        "        if correct[n] == 0:\n",
        "            if smooth == 'exp':\n",
        "                smooth_mteval *= 2\n",
        "                precisions[n] = 100. / (smooth_mteval * total[n])\n",
        "            elif smooth == 'floor':\n",
        "                precisions[n] = 100. * smooth_floor / total[n]\n",
        "        else:\n",
        "            precisions[n] = 100. * correct[n] / total[n]\n",
        "\n",
        "    # If the system guesses no i-grams, 1 <= i <= NGRAM_ORDER, the BLEU score is 0 (technically undefined).\n",
        "    # This is a problem for sentence-level BLEU or a corpus of short sentences, where systems will get no credit\n",
        "    # if sentence lengths fall under the NGRAM_ORDER threshold. This fix scales NGRAM_ORDER to the observed\n",
        "    # maximum order. It is only available through the API and off by default\n",
        "\n",
        "    brevity_penalty = 1.0\n",
        "    if sys_len < ref_len:\n",
        "        brevity_penalty = math.exp(1 - ref_len / sys_len) if sys_len > 0 else 0.0\n",
        "        \n",
        "\n",
        "    bleu = brevity_penalty * math.exp(sum(map(my_log, precisions[:effective_order])) / effective_order)\n",
        "\n",
        "    return bleu \n",
        "  \n",
        "  \n",
        "def ref_stats(output, refs):\n",
        "    ngrams = Counter()\n",
        "    closest_diff = None\n",
        "    closest_len = None\n",
        "    for ref in refs:\n",
        "        tokens = ref.split()\n",
        "        reflen = len(tokens)\n",
        "        diff = abs(len(output.split()) - reflen)\n",
        "        if closest_diff is None or diff < closest_diff:\n",
        "            closest_diff = diff\n",
        "            closest_len = reflen\n",
        "        elif diff == closest_diff:\n",
        "            if reflen < closest_len:\n",
        "                closest_len = reflen\n",
        "\n",
        "        ngrams_ref = extract_ngrams(ref)\n",
        "        for ngram in ngrams_ref.keys():\n",
        "            ngrams[ngram] = max(ngrams[ngram], ngrams_ref[ngram])\n",
        "\n",
        "    return ngrams, closest_diff, closest_len\n",
        "  \n",
        "  \n",
        "def extract_ngrams(line, min_order=1, max_order=NGRAM_ORDER) -> Counter:\n",
        "    \"\"\"Extracts all the ngrams (1 <= n <= NGRAM_ORDER) from a sequence of tokens.\n",
        "    :param line: a segment containing a sequence of words\n",
        "    :param max_order: collect n-grams from 1<=n<=max\n",
        "    :return: a dictionary containing ngrams and counts\n",
        "    \"\"\"\n",
        "\n",
        "    ngrams = Counter()\n",
        "    tokens = line.split()\n",
        "    for n in range(min_order, max_order + 1):\n",
        "        for i in range(0, len(tokens) - n + 1):\n",
        "            ngram = ' '.join(tokens[i: i + n])\n",
        "            ngrams[ngram] += 1\n",
        "\n",
        "    return ngrams  \n",
        "\n",
        "def my_log(num):\n",
        "    \"\"\"\n",
        "    Floors the log function\n",
        "    :param num: the number\n",
        "    :return: log(num) floored to a very low number\n",
        "    \"\"\"\n",
        "\n",
        "    if num == 0.0:\n",
        "        return -9999999999\n",
        "    return math.log(num)\n",
        "  \n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        #self.word2count = {}\n",
        "        self.index2word = {}\n",
        "        self.n_words = 4  # Count SOS and EOS and Pad\n",
        "        self.all_words = []\n",
        "        \n",
        "    def build_vocab(self, embedding, max_vocab_size = 10000):\n",
        "\n",
        "    # save index 1 for unk and 0 for pad\n",
        "    # Returns:\n",
        "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
        "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
        "    #all_tokens = [item for sublist in all_tokens for item in sublist]\n",
        "    #max_len = max([len(word) for word in all_tokens])\n",
        "    \n",
        "    \n",
        "        unique_words = list(embedding.keys())[0:max_vocab_size]\n",
        "\n",
        "        index2word =  unique_words #list of words available in embedding\n",
        "        index2word = ['<pad>', '<sos>', '<eos>','<unk>'] + index2word #add pad and unknown to the beginning\n",
        "\n",
        "        word2index = dict(zip(unique_words, range(4,4+len(unique_words)))) # dictionary of words and indices \n",
        "        word2index['<pad>'] = PAD_IDX  #add pad symbol to the dictionary\n",
        "        word2index['<unk>'] = UNK_IDX  #add unkown symbol to the dictionary\n",
        "        word2index['<eos>'] = EOS_IDX\n",
        "        word2index['<sos>'] = SOS_IDX\n",
        "        \n",
        "        self.word2index = word2index\n",
        "        self.index2word = index2word\n",
        "        self.n_words = len(self.word2index)\n",
        "\n",
        "        return word2index, index2word          \n",
        "\n",
        "            \n",
        "def remove_blanks(pair):\n",
        "    '''Remove empty lines'''\n",
        "    if len(pair[0]) == 0 and len(pair[1]) == 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def set_max_length(pair, max_length=MAX_LENGTH):\n",
        "    if len(pair[0].split(' ')) > max_length or len(pair[1].split(' '))>max_length:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def readLangs(filename1, filename2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    with open(filename1, encoding='utf-8') as f:\n",
        "        lines1 = f.read().strip().split('\\n')\n",
        "        \n",
        "    with open(filename2, encoding='utf-8') as f:\n",
        "        lines2 = f.read().strip().split('\\n')   \n",
        "        \n",
        "    # Remove punctuation\n",
        "    lines1 = [removePunctuation(l) for l in lines1]\n",
        "    lines2 = [removePunctuation(l) for l in lines2]\n",
        "              \n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse: #change from english->french to french->english for example\n",
        "        pairs =list(zip(lines2, lines1))\n",
        "        input_lang = Lang(filename2[-2:]) #take last two letters\n",
        "        output_lang = Lang(filename1[-2:])\n",
        "    else:\n",
        "        pairs =list(zip(lines1, lines2))\n",
        "        input_lang = Lang(filename1[-2:])\n",
        "        output_lang = Lang(filename2[-2:])\n",
        "            \n",
        "        \n",
        "\n",
        "    pairs = list(filter(remove_blanks, pairs))  \n",
        "    pairs = list(filter(set_max_length, pairs))\n",
        "\n",
        "    return input_lang, output_lang, pairs \n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, embedding_in, embedding_out, num_sent=None, reverse=False):\n",
        "    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    \n",
        "    pairs = pairs[:num_sent]\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    \n",
        "    print(\"Counting words...\")\n",
        "        \n",
        "    #print(type(embedding_in))    \n",
        "    input_lang.build_vocab(embedding_in)\n",
        "    output_lang.build_vocab(embedding_out)\n",
        "        \n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "\n",
        "class VocabDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_tuple, word2id_lang1, word2id_lang2):\n",
        "        \"\"\"\n",
        "        @param data_list: list of character\n",
        "        @param target_list: list of targets\n",
        "\n",
        "        \"\"\"\n",
        "        self.data_list1, self.data_list2 = zip(*data_tuple)\n",
        "        assert (len(self.data_list1) == len(self.data_list2))\n",
        "        self.word2id1 = word2id_lang1\n",
        "        self.word2id2 = word2id_lang2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list1)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        input_sentence = [self.word2id1[c] if c in self.word2id1.keys() \n",
        "                         else UNK_IDX for c in self.data_list1[key].split()][:MAX_LENGTH-1]\n",
        "        input_sentence.append(EOS_IDX)\n",
        "                                                                   \n",
        "        output_sentence = [self.word2id2[c] if c in self.word2id2.keys() \n",
        "                          else UNK_IDX for c in self.data_list2[key].split()][:MAX_LENGTH-1]\n",
        "        output_sentence.append(EOS_IDX)\n",
        "\n",
        "        return [input_sentence, output_sentence, len(input_sentence), len(output_sentence)]\n",
        "    \n",
        "\n",
        "def vocab_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all\n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    data_list1 = []\n",
        "    data_list2 = []\n",
        "    length_list1 = []\n",
        "    length_list2 = []\n",
        "     \n",
        "    # padding\n",
        "    for datum in batch:\n",
        "        x1 = datum[0]\n",
        "        x2 = datum[1]\n",
        "        len1 = datum[2]\n",
        "        len2 = datum[3]\n",
        "        \n",
        "        length_list1.append(len1)\n",
        "        length_list2.append(len2)\n",
        "        #Pad first sentences\n",
        "        padded_vec1 = np.pad(np.array(x1),\n",
        "                                pad_width=((0,MAX_LENGTH-len1)),\n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        data_list1.append(padded_vec1)\n",
        "        \n",
        "        #Pad second sentences\n",
        "        padded_vec2 = np.pad(np.array(x2),\n",
        "                        pad_width=((0,MAX_LENGTH-len2)),\n",
        "                        mode=\"constant\", constant_values=0)\n",
        "        data_list2.append(padded_vec2)\n",
        "        \n",
        "    data_list1 = np.array(data_list1)\n",
        "    data_list2 = np.array(data_list2)\n",
        "    length_list1 = np.array(length_list1)\n",
        "    lenth_list2 = np.array(length_list2)\n",
        "    \n",
        "    return [torch.from_numpy(np.array(data_list1)), \n",
        "            torch.from_numpy(np.array(data_list2)),\n",
        "            torch.LongTensor(length_list1), \n",
        "            torch.LongTensor(length_list2)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, hidden_size, emb_weights, num_layers, kernel_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "\n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        self.num_embeddings, self.embedding_dim = emb_weights.size()\n",
        "        self.embedding = nn.Embedding.from_pretrained(emb_weights,freeze=True)\n",
        "   \n",
        "        self.conv1 = nn.Conv1d(self.embedding_dim, hidden_size, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
        "        self.linear = nn.Linear(hidden_size,hidden_size)\n",
        "\n",
        "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
        "\n",
        "        embed = self.embedding(input_seqs.to(device))\n",
        "        batch_size,seq_len=input_seqs.size()\n",
        "        #First sentence\n",
        "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
        "\n",
        "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
        "\n",
        "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
        "\n",
        "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
        "        hidden=torch.sum(hidden, dim=0).view(1,seq_len,hidden.size(-1))\n",
        "\n",
        "        hidden=self.linear(hidden)\n",
        "\n",
        "        #logits = self.linear(hiddenfinal)\n",
        "        return 1,hidden\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, emb_weights):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        weight_size = emb_weights.size()[1]\n",
        "        self.embedding = nn.Embedding(vocab_size, weight_size, padding_idx=0).from_pretrained(emb_weights, \n",
        "                                                                freeze=True).to(device)\n",
        "        \n",
        "        self.gru = nn.GRU(weight_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inp, hidden):\n",
        "        embedded = self.embedding(inp.to(device)).unsqueeze(0) #so that we have 1 x batch x hidden\n",
        "        #print('embedded', embedded.size())\n",
        "        output = F.relu(embedded)\n",
        "        #print('after relu', output.size())\n",
        "        #print('hidden size', hidden.size())\n",
        "#         print('decoder output size', output.size())\n",
        "#         print('decoder hidden size', hidden.size())\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class BeamSearchNode(object):\n",
        "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
        "        '''\n",
        "        :param hiddenstate:\n",
        "        :param previousNode:\n",
        "        :param wordId:\n",
        "        :param logProb:\n",
        "        :param length:\n",
        "        '''\n",
        "        self.h = hiddenstate\n",
        "        self.prevNode = previousNode\n",
        "        self.wordid = wordId\n",
        "        self.logp = logProb\n",
        "        self.leng = length\n",
        "\n",
        "    def eval(self):\n",
        "        return self.logp / float(self.leng - 1 + 1e-6)\n",
        "\n",
        "    \n",
        "def train(inputs, input_lengths, targets, target_lengths, \n",
        "          encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH,\n",
        "         teacher_forcing_ratio=0.5):\n",
        "    \n",
        "    # Zero gradients of both optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss = 0 #\n",
        "    batch_size = inputs.size()[1]\n",
        "    #print('input size', inputs.size())\n",
        "    #print('batch size', batch_size)\n",
        "    max_targ_len = max_length\n",
        "\n",
        "    # Run words through encoder\n",
        "    _, encoder_hidden = encoder(inputs, input_lengths, None)\n",
        "\n",
        "    \n",
        "    # Prepare input and output variables\n",
        "    decoder_input = torch.LongTensor([SOS_IDX] * batch_size).to(device)\n",
        "    decoder_hidden = encoder_hidden#[:1] # Use last (forward) hidden state from encoder\n",
        "    \n",
        "    #print('time 1 size', decoder_input.size())\n",
        "    #print('time 1 hidden size', decoder_hidden.size())\n",
        "    \n",
        "    #randomly use teacher forcing or not\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
        "    all_decoder_outputs = Variable(torch.zeros(max_targ_len, batch_size, output_lang.n_words))\n",
        "    \n",
        "\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_targ_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            \n",
        "            all_decoder_outputs[t] = decoder_output\n",
        "            decoder_input = targets[t]\n",
        "            \n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input without beam\n",
        "        for di in range(max_targ_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            \n",
        "            all_decoder_outputs[di] = decoder_output\n",
        "\n",
        "                \n",
        "    loss = masked_cross_entropy(\n",
        "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
        "    targets.transpose(0, 1).contiguous(),\n",
        "    target_lengths)\n",
        "        \n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters with optimizers\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item()\n",
        "\n",
        "def trainIters(loader, encoder, decoder, n_iters, print_every=1000, plot_every=100, validate_every=1,\n",
        "               learning_rate=0.01,\n",
        "              teacher_forcing_ratio=1, beam_width=3, beam_search=False):\n",
        "    \n",
        "    start = time.time()\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "    plot_losses = []\n",
        "    val_bleu = []\n",
        "\n",
        "    counter = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < n_iters:\n",
        "        epoch += 1\n",
        "\n",
        "        # Get training data for this cycle\n",
        "        for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
        "\n",
        "            counter += 1\n",
        "\n",
        "            # Run the train function\n",
        "            loss = train(\n",
        "                source.long().transpose(0,1), lengths1, target.long().transpose(0,1), lengths2,\n",
        "                encoder, decoder,\n",
        "                encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio=teacher_forcing_ratio\n",
        "            )\n",
        "\n",
        "            # Keep track of loss\n",
        "            print_loss_total += loss\n",
        "            plot_loss_total += loss\n",
        "\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_iters), epoch, \n",
        "                                                       epoch / n_iters * 100, print_loss_avg)\n",
        "                print(print_summary)\n",
        "\n",
        "\n",
        "            if counter % plot_every == 0:\n",
        "                plot_loss_avg = plot_loss_total / plot_every\n",
        "                plot_losses.append(plot_loss_avg)\n",
        "                plot_loss_total = 0\n",
        "                \n",
        "                \n",
        "            if counter % validate_every == 0:\n",
        "                bleu = validate(encoder, decoder, val_loader, beam_width, beam_search)\n",
        "                val_bleu.append(bleu)\n",
        "                print('Validation BLEU', bleu)\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "    return plot_losses, val_bleu\n",
        "\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lengths, translated, beam_width=3, beam_search=True, \n",
        "             max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Function that generate translation.\n",
        "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
        "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
        "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
        "    And collect the attention for each output words.\n",
        "    @param encoder: the encoder network\n",
        "    @param decoder: the decoder network\n",
        "    @param sentence: string, a sentence in source language to be translated\n",
        "    @param max_length: the max # of words that the decoder can return\n",
        "    @output decoded_words: a list of words in target language\n",
        "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
        "    \"\"\"    \n",
        "    # process input sentence\n",
        "    with torch.no_grad():\n",
        "        input_tensor = sentence.transpose(0,1)\n",
        "        input_length = sentence.size()[0]\n",
        "        \n",
        "        # encode the source lanugage\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor, input_lengths, None)\n",
        "\n",
        "        decoder_input = torch.tensor([SOS_IDX], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden[:1] # Use last (forward) hidden state from encoder \n",
        "        # output of this function\n",
        "        decoded_words = ''\n",
        "\n",
        "        for di in range(max_length):\n",
        "            # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            \n",
        "            # hint: print out decoder_output and decoder_attention\n",
        "            # TODO: add your code here to populate decoded_words and decoder_attentions\n",
        "            # TODO: do this in 2 ways discussed in class: greedy & beam_search\n",
        "            \n",
        "            if beam_search:\n",
        "                topk = 1 #for beam search\n",
        "                # Number of sentence to generate\n",
        "                endnodes = []\n",
        "                number_required = min((topk + 1), topk - len(endnodes))\n",
        "\n",
        "                # starting node -  hidden vector, previous node, word id, logp, length\n",
        "                node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
        "                nodes = PriorityQueue()\n",
        "\n",
        "                # start the queue\n",
        "                nodes.put_nowait((-node.eval(), node))\n",
        "                qsize = 1\n",
        "\n",
        "                # start beam search\n",
        "                while True:\n",
        "                    # give up when decoding takes too long\n",
        "                    if qsize > 2000: break\n",
        "\n",
        "                    # fetch the best node\n",
        "                    score, n = nodes.get_nowait()\n",
        "                    decoder_input = n.wordid\n",
        "                    decoder_hidden = n.h\n",
        "\n",
        "                    if n.wordid.item() == EOS_IDX and n.prevNode != None:\n",
        "                        endnodes.append((score, n))\n",
        "                        # if we reached maximum # of sentences required\n",
        "                        if len(endnodes) >= number_required:\n",
        "                            break\n",
        "                        else:\n",
        "                            continue\n",
        "\n",
        "                    decoder_output, decoder_hidden = decoder(\n",
        "                        decoder_input, decoder_hidden)\n",
        "\n",
        "                    log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
        "                    nextnodes = []\n",
        "\n",
        "                    for new_k in range(beam_width):\n",
        "                        decoded_t = indexes[0][new_k].view(1, -1)\n",
        "                        log_p = log_prob[0][new_k].item()\n",
        "\n",
        "                        node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)\n",
        "                        score = -node.eval()\n",
        "                        nextnodes.append((score, node))\n",
        "\n",
        "                    # put them into queue\n",
        "                    for i in range(len(nextnodes)):\n",
        "                        score, nn = nextnodes[i]\n",
        "                        nodes.put_nowait((score, nn))\n",
        "                        # increase qsize\n",
        "                    qsize += len(nextnodes) - 1\n",
        "\n",
        "                # choose nbest paths, back trace them\n",
        "                if len(endnodes) == 0:\n",
        "                    endnodes = [nodes.get_nowait() for _ in range(topk)]\n",
        "\n",
        "                best_beam = []\n",
        "                for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
        "                    utterance = []\n",
        "                    utterance.append(n.wordid)\n",
        "                    # back trace\n",
        "                    while n.prevNode != None:\n",
        "                        n = n.prevNode\n",
        "                        utterance.append(n.wordid)\n",
        "\n",
        "                    utterance = utterance[::-1]\n",
        "                    best_beam.append(utterance)\n",
        "\n",
        "                all_decoder_outputs[t] = best_beam\n",
        "                decoder_input = targets[t]\n",
        "            \n",
        "            # GREEDY\n",
        "            else:\n",
        "                topv, topi = decoder_output.data.topk(1) \n",
        "\n",
        "                if topi.item() == EOS_IDX:\n",
        "                    #decoded_words.append('<EOS>')\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    if topi.item() not in [SOS_IDX, EOS_IDX, UNK_IDX, PAD_IDX]:\n",
        "                        decoded_words = decoded_words + ' ' + output_lang.index2word[topi.item()]\n",
        "\n",
        "                decoder_input = topi[0].detach()\n",
        "        \n",
        "        translation = ''\n",
        "        for i in translated: #expected translation\n",
        "            if i.item() not in [SOS_IDX, EOS_IDX, UNK_IDX, PAD_IDX]:\n",
        "                translation = translation + ' ' + output_lang_v.index2word[i.item()]\n",
        "\n",
        "        return decoded_words, translation\n",
        "    \n",
        "    \n",
        "def evaluate_batch(loader, encoder, decoder, beam_width=3, beam_search=True):\n",
        "    \n",
        "    decoded_sentences = []\n",
        "    actual_sentences = []\n",
        "    \n",
        "    for i, (source, target, lengths1, lengths2) in enumerate(loader):\n",
        "        #iterate over batch\n",
        "        \n",
        "        for n in range(len(source)):\n",
        "            # Go sentence by sentence\n",
        "            \n",
        "            decoded, actual = evaluate(encoder, decoder, source[n].unsqueeze(0), lengths1[n], target[n],\n",
        "                                       beam_width, beam_search)\n",
        "            decoded_sentences.append(decoded)\n",
        "            actual_sentences.append(actual)\n",
        "            \n",
        "    return decoded_sentences, actual_sentences\n",
        "\n",
        "\n",
        "def validate(encoder, decoder, val_loader, beam_width, beam_search):\n",
        "    decoded_sentences, actual_sentences = evaluate_batch(val_loader, encoder, decoder, beam_width, beam_search)\n",
        "    bleu = evaluate_bleu(decoded_sentences, actual_sentences)\n",
        "    \n",
        "    return bleu\n",
        "\n",
        "\n",
        "def evaluate_bleu(translation_list, reference_list):\n",
        "    \n",
        "    return corpus_bleu(translation_list, [reference_list])\n",
        "\n",
        "#Plot results\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    \n",
        "\n",
        "def as_minutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def time_since(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))    \n",
        "\n",
        "def load_embedding(fname, max_count=None):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    counter=0\n",
        "    for line in fin:\n",
        "        counter+=1\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = map(float, tokens[1:])\n",
        "        if counter==max_count:\n",
        "            break\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNi9-FA77GXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### VIETNAMESE\n",
        "#Read in pretrained embedding vectors - subset for now\n",
        "embeddings_map_vi = load_embedding(en_loc + '/cc.vi.300.vec', max_count=50000)\n",
        "\n",
        "#Convert embedding values to lists\n",
        "embeddings_vi = {}\n",
        "\n",
        "for key, value in embeddings_map_vi.items():\n",
        "    embeddings_vi[key] = list(value)\n",
        "    \n",
        "### ENGLISH    \n",
        "#Read in pretrained embedding vectors - subset for now\n",
        "embeddings_map_en = load_embedding(en_loc + '/wiki-news-300d-1M.vec', max_count=50000)\n",
        "\n",
        "#Convert embedding values to lists\n",
        "embeddings_en = {}\n",
        "\n",
        "for key, value in embeddings_map_en.items():\n",
        "    embeddings_en[key] = list(value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDmgVNMXYhoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def embed_to_tensor(embeddings):\n",
        "    y=np.array([np.array(list(xi)) for xi in embeddings.values()])\n",
        "    padding = np.zeros((1, y.shape[1]))\n",
        "    unknown = np.random.rand(1, y.shape[1]) # to account for Padding and Unknown\n",
        "    sos = np.random.rand(1, y.shape[1])\n",
        "    eos = np.random.rand(1, y.shape[1])\n",
        "    full_size = np.concatenate([padding, sos, eos, unknown, y], axis=0)\n",
        "    emb_weights = torch.from_numpy(full_size)\n",
        "    \n",
        "    return emb_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rYehXpCbjJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_vi_tensor = embed_to_tensor(embeddings_vi).float()\n",
        "embed_en_tensor = embed_to_tensor(embeddings_en).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pj6kkX6xbk4E",
        "colab_type": "code",
        "outputId": "41a1761d-fe53-44ec-d429-12a91f26f637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData(en_loc+'/train.tok.vi', en_loc+'/train.tok.en', \n",
        "                                             embedding_in = embeddings_vi,\n",
        "                                             embedding_out = embeddings_en, num_sent=100)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 100 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "vi 10004\n",
            "en 10004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j6zm0v10bm-N",
        "colab_type": "code",
        "outputId": "7f456d6b-af68-4cba-8054-eed6413601a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "input_lang_v, output_lang_v, pairs_v = prepareData(en_loc+'/dev.tok.vi', en_loc+'/dev.tok.en', \n",
        "                                                   embedding_in = embeddings_vi,\n",
        "                                                   embedding_out = embeddings_en,\n",
        "                                                   num_sent=100)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 100 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "vi 10004\n",
            "en 10004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qDRBGwtubpBo",
        "colab_type": "code",
        "outputId": "35b6fc49-1d50-45aa-cf47-7af0ed207a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "cell_type": "code",
      "source": [
        "pairs_v[0:5]"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Khi tôi còn nhỏ  Tôi nghĩ rằng BắcTriều Tiên là đất_nước tốt nhất trên thế_giới và tôi thường hát bài \" Chúng_ta chẳng có gì phải ghen_tị  \"',\n",
              "  'When I was little  I thought my country was the best on the planet  and I grew up singing a song called  Nothing To Envy'),\n",
              " ('Tôi đã rất tự_hào về đất_nước tôi', 'And I was very proud'),\n",
              " ('Khi tôi lên 7  tôi chứng_kiến cảnh người_ta xử_bắn công_khai lần đầu_tiên trong đời  nhưng tôi vẫn nghĩ cuộc_sống của mình ở đây là hoàn_toàn bình_thường',\n",
              "  'When I was seven years old  I saw my first public execution  but I thought my life in North Korea was normal'),\n",
              " ('Gia_đình của tôi không nghèo  và bản_thân tôi thì chưa_từng phải chịu đói',\n",
              "  'My family was not poor  and myself  I had never experienced hunger'),\n",
              " ('Nhưng vào một ngày của năm 1995  mẹ tôi mang về nhà một lá thư_từ một người chị_em cùng chỗ làm với mẹ',\n",
              "  'But one day  in 1995  my mom brought home a letter from a coworker s sister')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "metadata": {
        "id": "vxpOS9wKbsJq",
        "colab_type": "code",
        "outputId": "6ba5343c-2575-4154-c461-9486cf9302d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "hidden_size=256\n",
        "\n",
        "train_dataset = VocabDataset(pairs, input_lang.word2index, output_lang.word2index)\n",
        "# 1 batch input dimension: num_sentences x max sentence length\n",
        "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_dataset = VocabDataset(pairs_v, input_lang.word2index, output_lang.word2index)\n",
        "# 1 batch input dimension: num_sentences x max sentence length\n",
        "# 1 batch: source_sentences, target_sentences, source_lengths, target_lengths\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=vocab_collate_func,\n",
        "                                           shuffle=False)\n",
        "\n",
        "encoder = EncoderCNN(hidden_size = hidden_size,\n",
        "                    emb_weights = embed_vi_tensor,num_layers=1, kernel_size=2).to(device)\n",
        "\n",
        "decoder = DecoderRNN(hidden_size = hidden_size, vocab_size = output_lang.n_words,\n",
        "                    emb_weights = embed_en_tensor).to(device)\n",
        "\n",
        "plot_losses, validation_bleu = trainIters(train_loader, encoder, decoder, n_iters=2000, \n",
        "                         print_every=50, plot_every=100, validate_every = 50, learning_rate=0.01, \n",
        "                                          teacher_forcing_ratio=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0m 29s (- 74m 24s) (13 0%) 9.2057\n",
            "Validation BLEU 0.006207471707676466\n",
            "1m 1s (- 81m 32s) (25 1%) 9.1617\n",
            "Validation BLEU 0.030196857919202756\n",
            "1m 35s (- 81m 50s) (38 1%) 9.1070\n",
            "Validation BLEU 0.0\n",
            "2m 4s (- 80m 44s) (50 2%) 9.0167\n",
            "Validation BLEU 0.0\n",
            "2m 33s (- 78m 49s) (63 3%) 8.4508\n",
            "Validation BLEU 0.0\n",
            "3m 2s (- 78m 12s) (75 3%) 6.7530\n",
            "Validation BLEU 0.0\n",
            "3m 32s (- 76m 58s) (88 4%) 6.1122\n",
            "Validation BLEU 5.54063825288447e-07\n",
            "4m 1s (- 76m 33s) (100 5%) 5.8025\n",
            "Validation BLEU 0.02728725215554096\n",
            "4m 31s (- 75m 39s) (113 5%) 5.6298\n",
            "Validation BLEU 0.042887991932221516\n",
            "5m 1s (- 75m 17s) (125 6%) 5.4932\n",
            "Validation BLEU 0.0\n",
            "5m 31s (- 74m 30s) (138 6%) 5.2798\n",
            "Validation BLEU 0.020342174257477485\n",
            "6m 0s (- 74m 9s) (150 7%) 5.1662\n",
            "Validation BLEU 0.0\n",
            "6m 31s (- 73m 28s) (163 8%) 5.1371\n",
            "Validation BLEU 0.10774304576287853\n",
            "7m 0s (- 73m 6s) (175 8%) 5.0551\n",
            "Validation BLEU 0.07078445743389188\n",
            "7m 31s (- 72m 28s) (188 9%) 5.0602\n",
            "Validation BLEU 0.1543924013249309\n",
            "8m 0s (- 72m 4s) (200 10%) 4.9723\n",
            "Validation BLEU 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZCr7DM_OgNy4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_vi_tensor.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UJom4Q7pbutP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoded, actual = evaluate_batch(val_loader, encoder, decoder)\n",
        "\n",
        "for i in zip(decoded, actual):\n",
        "    if i == 10:\n",
        "        break\n",
        "    print('\\n')\n",
        "    print('Expected:', i[1])\n",
        "    print('Actual:' ,i[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9o7qvIAabuwG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluate_bleu(decoded, actual)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RUzGNQS_buya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u5bWnWMRbu1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}